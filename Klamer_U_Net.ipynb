{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Klamer_U_Net.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wtNUqHto5R65",
        "w6POq8z_-aSd",
        "HwBiOz6m39og",
        "XO6CrH4C4MqC",
        "8l35iRa74B86",
        "BZmj3OkL6brT",
        "TKDbz7DWWw_3",
        "WBd_G2JnW67_"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KlamJ/DeepLab_V3_KL/blob/master/Klamer_U_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ-vxFBI_msg",
        "colab_type": "text"
      },
      "source": [
        "![Bild](https://drive.google.com/uc?id=18KhcCqqeHGeL3Fmsliv2gV2vBcFIy2cP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uwQDXN_yiEb",
        "colab_type": "text"
      },
      "source": [
        "# Cityscapes Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MQ5IWWiqbtn",
        "colab_type": "text"
      },
      "source": [
        "Der Cityscapes Datensatz konzentriert sich auf die semantische Segmentierung von städtischen Straßenszenen. Im Folgenden wird mithilfe dieses Datensatzes das DCNN DeepLabV3 Trainiert und evaluiert.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WJm0rnDLUdE",
        "colab_type": "text"
      },
      "source": [
        "##TO DO \n",
        " \n",
        " - Beschreibung der AUfgabe und Motivation\n",
        " - Beschreibung des DeepLab\n",
        " - Beschreibung der Datensatzes \n",
        " - Beschreibung der einzelen Codeblöcke \n",
        "  - https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
        "  - beschreiben warum adam !!!\n",
        " - Code Blöcke entzerren und verkleinern\n",
        " - modele Trainieren mit verschiedenen Volumen \n",
        "  - mit verschiedenen Netzten ? (Zeit) \n",
        " - Präsentation Anlegen vor Weihnachten fertig machen \n",
        " - Präsentation warsch. am 15.01.20\n",
        " \n",
        " **NOTIZEN : **\n",
        "Resnet18\n",
        "\n",
        "model 2: \n",
        "  - batch 3 \n",
        "  - worker 4\n",
        "\n",
        "model 3: \n",
        "  - batch 30\n",
        "  - worker 4\n",
        "\n",
        "model 5\n",
        " - batch 3\n",
        " - worker 2 \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VRxG_c94loZ",
        "colab_type": "text"
      },
      "source": [
        "##Präsentation\n",
        "\n",
        "- 15min \n",
        "- 15 slides max \n",
        "- einführung hälfte technik deeplab \n",
        "- ergebnisse andere hälfte \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBxQIChQvWdc",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Training mehr als eine instanz lang \n",
        "https://medium.com/@prajwal.prashanth22/google-colab-drive-as-persistent-storage-for-long-training-runs-cb82bc1d5b71\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PnWRxO4sHWu",
        "colab_type": "text"
      },
      "source": [
        "###Mount GoogleDrive \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Icq80SfN7rYR",
        "colab_type": "code",
        "outputId": "b974a7ef-2bc0-43a8-ac33-f72ae716cf7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "## Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#https://github.com/KlamJ/deeplabv3\n",
        "#https://drive.google.com/open?id=17mKymLGO6mQxSnbHLsg7TwzGgoQK1MpN"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C78Gd98_sR4p",
        "colab_type": "text"
      },
      "source": [
        "###Check Gpu status"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feyLL9_FEAdP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "95a71ee0-1ac3-427a-8cc9-233705afd156"
      },
      "source": [
        "#' ' means CPU whereas '/device:G:0' means GPU\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rsm8T6EEEvK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "outputId": "b9f2cc8f-5eef-40f7-bca5-c3331b22b967"
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7410 sha256=90839127cead42cc6f1da5182ffbfc80e1afc746654b724f70a82f64b82223dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 24.7 GB  | Proc size: 2.8 GB\n",
            "GPU RAM Free: 15567MB | Used: 713MB | Util   4% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI97pt3KEFh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#if not the only user and the gpu Util is >0 do following \n",
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9mzp7ni0Cdp",
        "colab_type": "text"
      },
      "source": [
        "#DeepLab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d_Gwc6F2t3x",
        "colab_type": "text"
      },
      "source": [
        "##The DeepLab V3 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H992mJv6K9I",
        "colab_type": "text"
      },
      "source": [
        "Importieren der Benötigten Bibliotheken"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcPu5-b0iIrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import sys\n",
        "import cv2\n",
        "datasetPath = \"/content/drive/My Drive/U_Net_Datasets\"\n",
        "cityscapes_data_path=\"/content/drive/My Drive/U_Net_Datasets\"\n",
        "cityscapes_meta_path=\"/content/drive/My Drive/U_Net_Datasets/meta\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtNUqHto5R65",
        "colab_type": "text"
      },
      "source": [
        "####Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5WnI-Qz6OQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "from collections import namedtuple\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "\n",
        "train_dirs = [\"jena/\", \"zurich/\", \"weimar/\", \"ulm/\", \"tubingen/\", \"stuttgart/\",\n",
        "              \"strasbourg/\", \"monchengladbach/\", \"krefeld/\", \"hanover/\",\n",
        "             \"hamburg/\", \"erfurt/\", \"dusseldorf/\", \"darmstadt/\", \"cologne/\",\n",
        "             \"bremen/\", \"bochum/\", \"aachen/\"]\n",
        "val_dirs = [\"frankfurt/\", \"munster/\", \"lindau/\"]\n",
        "test_dirs = [\"berlin\", \"bielefeld\", \"bonn\", \"leverkusen\", \"mainz\", \"munich\"]\n",
        "\n",
        "class DatasetTrain(torch.utils.data.Dataset):\n",
        "    def __init__(self, cityscapes_data_path, cityscapes_meta_path):\n",
        "        self.img_dir = cityscapes_data_path + \"/leftImg8bit/train/\"\n",
        "        self.label_dir = cityscapes_meta_path + \"/label_imgs/\"\n",
        "\n",
        "        self.img_h = 1024\n",
        "        self.img_w = 2048\n",
        "\n",
        "        self.new_img_h = 512\n",
        "        self.new_img_w = 1024\n",
        "\n",
        "        self.examples = []\n",
        "        for train_dir in train_dirs:\n",
        "            train_img_dir_path = self.img_dir + train_dir\n",
        "\n",
        "            file_names = os.listdir(train_img_dir_path)\n",
        "            for file_name in file_names:\n",
        "                img_id = file_name.split(\"_leftImg8bit.png\")[0]\n",
        "\n",
        "                img_path = train_img_dir_path + file_name\n",
        "\n",
        "                label_img_path = self.label_dir + img_id + \".png\"\n",
        "\n",
        "                example = {}\n",
        "                example[\"img_path\"] = img_path\n",
        "                example[\"label_img_path\"] = label_img_path\n",
        "                example[\"img_id\"] = img_id\n",
        "                self.examples.append(example)\n",
        "\n",
        "        self.num_examples = len(self.examples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = self.examples[index]\n",
        "\n",
        "        img_path = example[\"img_path\"]\n",
        "        img = cv2.imread(img_path, -1) # (shape: (1024, 2048, 3))\n",
        "        # resize img without interpolation (want the image to still match\n",
        "        # label_img, which we resize below):\n",
        "        img = cv2.resize(img, (self.new_img_w, self.new_img_h),\n",
        "                         interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024, 3))\n",
        "\n",
        "        label_img_path = example[\"label_img_path\"]\n",
        "        label_img = cv2.imread(label_img_path, -1) # (shape: (1024, 2048))\n",
        "        # resize label_img without interpolation (want the resulting image to\n",
        "        # still only contain pixel values corresponding to an object class):\n",
        "        label_img = cv2.resize(label_img, (self.new_img_w, self.new_img_h),\n",
        "                               interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024))\n",
        "\n",
        "        # flip the img and the label with 0.5 probability:\n",
        "        flip = np.random.randint(low=0, high=2)\n",
        "        if flip == 1:\n",
        "            img = cv2.flip(img, 1)\n",
        "            label_img = cv2.flip(label_img, 1)\n",
        "\n",
        "        ########################################################################\n",
        "        # randomly scale the img and the label:\n",
        "        ########################################################################\n",
        "        scale = np.random.uniform(low=0.7, high=2.0)\n",
        "        new_img_h = int(scale*self.new_img_h)\n",
        "        new_img_w = int(scale*self.new_img_w)\n",
        "\n",
        "        # resize img without interpolation (want the image to still match\n",
        "        # label_img, which we resize below):\n",
        "        img = cv2.resize(img, (new_img_w, new_img_h),\n",
        "                         interpolation=cv2.INTER_NEAREST) # (shape: (new_img_h, new_img_w, 3))\n",
        "\n",
        "        # resize label_img without interpolation (want the resulting image to\n",
        "        # still only contain pixel values corresponding to an object class):\n",
        "        label_img = cv2.resize(label_img, (new_img_w, new_img_h),\n",
        "                               interpolation=cv2.INTER_NEAREST) # (shape: (new_img_h, new_img_w))\n",
        "        ########################################################################\n",
        "\n",
        "         # # # # # # # debug visualization START\n",
        "        #print (scale)\n",
        "        #print (new_img_h)\n",
        "        #print (new_img_w)\n",
        "        \n",
        "        #cv2.imshow(\"test\", img)\n",
        "        #cv2.waitKey(0)\n",
        "        \n",
        "        #cv2.imshow(\"test\", label_img)\n",
        "        #cv2.waitKey(0)\n",
        "         # # # # # # # debug visualization END\n",
        "\n",
        "        ########################################################################\n",
        "        # select a 256x256 random crop from the img and label:\n",
        "        ########################################################################\n",
        "        start_x = np.random.randint(low=0, high=(new_img_w - 256))\n",
        "        end_x = start_x + 256\n",
        "        start_y = np.random.randint(low=0, high=(new_img_h - 256))\n",
        "        end_y = start_y + 256\n",
        "\n",
        "        img = img[start_y:end_y, start_x:end_x] # (shape: (256, 256, 3))\n",
        "        label_img = label_img[start_y:end_y, start_x:end_x] # (shape: (256, 256))\n",
        "        ########################################################################\n",
        "\n",
        "        # # # # # # # # debug visualization START\n",
        "        # print (img.shape)\n",
        "        # print (label_img.shape)\n",
        "        #\n",
        "        # cv2.imshow(\"test\", img)\n",
        "        # cv2.waitKey(0)\n",
        "        #\n",
        "        # cv2.imshow(\"test\", label_img)\n",
        "        # cv2.waitKey(0)\n",
        "        # # # # # # # # debug visualization END\n",
        "\n",
        "        # normalize the img (with the mean and std for the pretrained ResNet):\n",
        "        img = img/255.0\n",
        "        img = img - np.array([0.485, 0.456, 0.406])\n",
        "        img = img/np.array([0.229, 0.224, 0.225]) # (shape: (256, 256, 3))\n",
        "        img = np.transpose(img, (2, 0, 1)) # (shape: (3, 256, 256))\n",
        "        img = img.astype(np.float32)\n",
        "\n",
        "        # convert numpy -> torch:\n",
        "        img = torch.from_numpy(img) # (shape: (3, 256, 256))\n",
        "        label_img = torch.from_numpy(label_img) # (shape: (256, 256))\n",
        "\n",
        "        return (img, label_img)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_examples\n",
        "\n",
        "class DatasetVal(torch.utils.data.Dataset):\n",
        "    def __init__(self, cityscapes_data_path, cityscapes_meta_path):\n",
        "        self.img_dir = cityscapes_data_path + \"/leftImg8bit/val/\"\n",
        "        self.label_dir = cityscapes_meta_path + \"/label_imgs/\"\n",
        "\n",
        "        self.img_h = 1024\n",
        "        self.img_w = 2048\n",
        "\n",
        "        self.new_img_h = 512\n",
        "        self.new_img_w = 1024\n",
        "\n",
        "        self.examples = []\n",
        "        for val_dir in val_dirs:\n",
        "            val_img_dir_path = self.img_dir + val_dir\n",
        "\n",
        "            file_names = os.listdir(val_img_dir_path)\n",
        "            for file_name in file_names:\n",
        "                img_id = file_name.split(\"_leftImg8bit.png\")[0]\n",
        "\n",
        "                img_path = val_img_dir_path + file_name\n",
        "\n",
        "                label_img_path = self.label_dir + img_id + \".png\"\n",
        "                label_img = cv2.imread(label_img_path, -1) # (shape: (1024, 2048))\n",
        "\n",
        "                example = {}\n",
        "                example[\"img_path\"] = img_path\n",
        "                example[\"label_img_path\"] = label_img_path\n",
        "                example[\"img_id\"] = img_id\n",
        "                self.examples.append(example)\n",
        "\n",
        "        self.num_examples = len(self.examples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = self.examples[index]\n",
        "\n",
        "        img_id = example[\"img_id\"]\n",
        "\n",
        "        img_path = example[\"img_path\"]\n",
        "        img = cv2.imread(img_path, -1) # (shape: (1024, 2048, 3))\n",
        "        # resize img without interpolation (want the image to still match\n",
        "        # label_img, which we resize below):\n",
        "        img = cv2.resize(img, (self.new_img_w, self.new_img_h),\n",
        "                         interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024, 3))\n",
        "\n",
        "        label_img_path = example[\"label_img_path\"]\n",
        "        label_img = cv2.imread(label_img_path, -1) # (shape: (1024, 2048))\n",
        "        # resize label_img without interpolation (want the resulting image to\n",
        "        # still only contain pixel values corresponding to an object class):\n",
        "        label_img = cv2.resize(label_img, (self.new_img_w, self.new_img_h),\n",
        "                               interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024))\n",
        "\n",
        "        # # # # # # # # debug visualization START\n",
        "        # cv2.imshow(\"test\", img)\n",
        "        # cv2.waitKey(0)\n",
        "        #\n",
        "        # cv2.imshow(\"test\", label_img)\n",
        "        # cv2.waitKey(0)\n",
        "        # # # # # # # # debug visualization END\n",
        "\n",
        "        # normalize the img (with the mean and std for the pretrained ResNet):\n",
        "        img = img/255.0\n",
        "        img = img - np.array([0.485, 0.456, 0.406])\n",
        "        img = img/np.array([0.229, 0.224, 0.225]) # (shape: (512, 1024, 3))\n",
        "        img = np.transpose(img, (2, 0, 1)) # (shape: (3, 512, 1024))\n",
        "        img = img.astype(np.float32)\n",
        "\n",
        "        # convert numpy -> torch:\n",
        "        img = torch.from_numpy(img) # (shape: (3, 512, 1024))\n",
        "        label_img = torch.from_numpy(label_img) # (shape: (512, 1024))\n",
        "\n",
        "        return (img, label_img, img_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_examples\n",
        "\n",
        "class DatasetSeq(torch.utils.data.Dataset):\n",
        "    def __init__(self, cityscapes_data_path, cityscapes_meta_path, sequence):\n",
        "        self.img_dir = cityscapes_data_path + \"/leftImg8bit/demoVideo/stuttgart_\" + sequence + \"/\"\n",
        "\n",
        "        self.img_h = 1024\n",
        "        self.img_w = 2048\n",
        "\n",
        "        self.new_img_h = 512\n",
        "        self.new_img_w = 1024\n",
        "\n",
        "        self.examples = []\n",
        "\n",
        "        file_names = os.listdir(self.img_dir)\n",
        "        for file_name in file_names:\n",
        "            img_id = file_name.split(\"_leftImg8bit.png\")[0]\n",
        "\n",
        "            img_path = self.img_dir + file_name\n",
        "\n",
        "            example = {}\n",
        "            example[\"img_path\"] = img_path\n",
        "            example[\"img_id\"] = img_id\n",
        "            self.examples.append(example)\n",
        "\n",
        "        self.num_examples = len(self.examples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = self.examples[index]\n",
        "\n",
        "        img_id = example[\"img_id\"]\n",
        "\n",
        "        img_path = example[\"img_path\"]\n",
        "        img = cv2.imread(img_path, -1) # (shape: (1024, 2048, 3))\n",
        "        # resize img without interpolation:\n",
        "        img = cv2.resize(img, (self.new_img_w, self.new_img_h),\n",
        "                         interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024, 3))\n",
        "\n",
        "        # normalize the img (with the mean and std for the pretrained ResNet):\n",
        "        img = img/255.0\n",
        "        img = img - np.array([0.485, 0.456, 0.406])\n",
        "        img = img/np.array([0.229, 0.224, 0.225]) # (shape: (512, 1024, 3))\n",
        "        img = np.transpose(img, (2, 0, 1)) # (shape: (3, 512, 1024))\n",
        "        img = img.astype(np.float32)\n",
        "\n",
        "        # convert numpy -> torch:\n",
        "        img = torch.from_numpy(img) # (shape: (3, 512, 1024))\n",
        "\n",
        "        return (img, img_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_examples\n",
        "\n",
        "class DatasetThnSeq(torch.utils.data.Dataset):\n",
        "    def __init__(self, thn_data_path):\n",
        "        self.img_dir = thn_data_path + \"/\"\n",
        "\n",
        "        self.examples = []\n",
        "\n",
        "        file_names = os.listdir(self.img_dir)\n",
        "        for file_name in file_names:\n",
        "            img_id = file_name.split(\".png\")[0]\n",
        "\n",
        "            img_path = self.img_dir + file_name\n",
        "\n",
        "            example = {}\n",
        "            example[\"img_path\"] = img_path\n",
        "            example[\"img_id\"] = img_id\n",
        "            self.examples.append(example)\n",
        "\n",
        "        self.num_examples = len(self.examples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = self.examples[index]\n",
        "\n",
        "        img_id = example[\"img_id\"]\n",
        "\n",
        "        img_path = example[\"img_path\"]\n",
        "        img = cv2.imread(img_path, -1) # (shape: (512, 1024, 3))\n",
        "\n",
        "        # normalize the img (with mean and std for the pretrained ResNet):\n",
        "        img = img/255.0\n",
        "        img = img - np.array([0.485, 0.456, 0.406])\n",
        "        img = img/np.array([0.229, 0.224, 0.225]) # (shape: (512, 1024, 3))\n",
        "        img = np.transpose(img, (2, 0, 1)) # (shape: (3, 512, 1024))\n",
        "        img = img.astype(np.float32)\n",
        "\n",
        "        # convert numpy -> torch:\n",
        "        img = torch.from_numpy(img) # (shape: (3, 512, 1024))\n",
        "\n",
        "        return (img, img_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_examples\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l14z8QR5Ovn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def add_weight_decay(net, l2_value, skip_list=()):\n",
        "    # https://raberrytv.wordpress.com/2017/10/29/pytorch-weight-decay-made-easy/\n",
        "\n",
        "    decay, no_decay = [], []\n",
        "    for name, param in net.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue # frozen weights\n",
        "        if len(param.shape) == 1 or name.endswith(\".bias\") or name in skip_list:\n",
        "            no_decay.append(param)\n",
        "        else:\n",
        "            decay.append(param)\n",
        "\n",
        "    return [{'params': no_decay, 'weight_decay': 0.0}, {'params': decay, 'weight_decay': l2_value}]\n",
        "\n",
        "# function for colorizing a label image:\n",
        "def label_img_to_color(img):\n",
        "    label_to_color = {\n",
        "        0: [128, 64,128],\n",
        "        1: [244, 35,232],\n",
        "        2: [ 70, 70, 70],\n",
        "        3: [102,102,156],\n",
        "        4: [190,153,153],\n",
        "        5: [153,153,153],\n",
        "        6: [250,170, 30],\n",
        "        7: [220,220,  0],\n",
        "        8: [107,142, 35],\n",
        "        9: [152,251,152],\n",
        "        10: [ 70,130,180],\n",
        "        11: [220, 20, 60],\n",
        "        12: [255,  0,  0],\n",
        "        13: [  0,  0,142],\n",
        "        14: [  0,  0, 70],\n",
        "        15: [  0, 60,100],\n",
        "        16: [  0, 80,100],\n",
        "        17: [  0,  0,230],\n",
        "        18: [119, 11, 32],\n",
        "        19: [81,  0, 81]\n",
        "        }\n",
        "\n",
        "    img_height, img_width = img.shape\n",
        "\n",
        "    img_color = np.zeros((img_height, img_width, 3))\n",
        "    for row in range(img_height):\n",
        "        for col in range(img_width):\n",
        "            label = img[row, col]\n",
        "\n",
        "            img_color[row, col] = np.array(label_to_color[label])\n",
        "\n",
        "    return img_color"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raYB_3GT5LP1",
        "colab_type": "text"
      },
      "source": [
        "###Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6POq8z_-aSd",
        "colab_type": "text"
      },
      "source": [
        "#### Einmalige ausführung zum erzeugen der Label und Klassengewichte\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-AVpbPI5W_m",
        "colab_type": "code",
        "outputId": "8bd10a22-baff-4882-ddf9-3fd94c637f98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "source": [
        "\n",
        "\n",
        "# (NOTE! this is taken from the official Cityscapes scripts:)\n",
        "Label = namedtuple( 'Label' , [\n",
        "\n",
        "    'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n",
        "                    # We use them to uniquely name a class\n",
        "\n",
        "    'id'          , # An integer ID that is associated with this label.\n",
        "                    # The IDs are used to represent the label in ground truth images\n",
        "                    # An ID of -1 means that this label does not have an ID and thus\n",
        "                    # is ignored when creating ground truth images (e.g. license plate).\n",
        "                    # Do not modify these IDs, since exactly these IDs are expected by the\n",
        "                    # evaluation server.\n",
        "\n",
        "    'trainId'     , # Feel free to modify these IDs as suitable for your method. Then create\n",
        "                    # ground truth images with train IDs, using the tools provided in the\n",
        "                    # 'preparation' folder. However, make sure to validate or submit results\n",
        "                    # to our evaluation server using the regular IDs above!\n",
        "                    # For trainIds, multiple labels might have the same ID. Then, these labels\n",
        "                    # are mapped to the same class in the ground truth images. For the inverse\n",
        "                    # mapping, we use the label that is defined first in the list below.\n",
        "                    # For example, mapping all void-type classes to the same ID in training,\n",
        "                    # might make sense for some approaches.\n",
        "                    # Max value is 255!\n",
        "\n",
        "    'category'    , # The name of the category that this label belongs to\n",
        "\n",
        "    'categoryId'  , # The ID of this category. Used to create ground truth images\n",
        "                    # on category level.\n",
        "\n",
        "    'hasInstances', # Whether this label distinguishes between single instances or not\n",
        "\n",
        "    'ignoreInEval', # Whether pixels having this class as ground truth label are ignored\n",
        "                    # during evaluations or not\n",
        "\n",
        "    'color'       , # The color of this label\n",
        "    ] )\n",
        "\n",
        "# (NOTE! this is taken from the official Cityscapes scripts:)\n",
        "labels = [\n",
        "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
        "    Label(  'unlabeled'            ,  0 ,      19 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'ego vehicle'          ,  1 ,      19 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'rectification border' ,  2 ,      19 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'out of roi'           ,  3 ,      19 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'static'               ,  4 ,      19 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'dynamic'              ,  5 ,      19 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
        "    Label(  'ground'               ,  6 ,      19 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
        "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
        "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
        "    Label(  'parking'              ,  9 ,      19 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
        "    Label(  'rail track'           , 10 ,      19 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
        "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
        "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
        "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
        "    Label(  'guard rail'           , 14 ,      19 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
        "    Label(  'bridge'               , 15 ,      19 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
        "    Label(  'tunnel'               , 16 ,      19 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
        "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
        "    Label(  'polegroup'            , 18 ,      19 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
        "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
        "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
        "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
        "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
        "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
        "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
        "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
        "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
        "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
        "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
        "    Label(  'caravan'              , 29 ,      19 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
        "    Label(  'trailer'              , 30 ,      19 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
        "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
        "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
        "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
        "    Label(  'license plate'        , -1 ,       19 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
        "]\n",
        "\n",
        "# create a function which maps id to trainId:\n",
        "id_to_trainId = {label.id: label.trainId for label in labels}\n",
        "id_to_trainId_map_func = np.vectorize(id_to_trainId.get)\n",
        "\n",
        "train_dirs = [\"jena/\", \"zurich/\", \"weimar/\", \"ulm/\", \"tubingen/\", \"stuttgart/\",\n",
        "              \"strasbourg/\", \"monchengladbach/\", \"krefeld/\", \"hanover/\",\n",
        "              \"hamburg/\", \"erfurt/\", \"dusseldorf/\", \"darmstadt/\", \"cologne/\",\n",
        "              \"bremen/\", \"bochum/\", \"aachen/\"]\n",
        "val_dirs = [\"frankfurt/\", \"munster/\", \"lindau/\"]\n",
        "test_dirs = [\"berlin\", \"bielefeld\", \"bonn\", \"leverkusen\", \"mainz\", \"munich\"]\n",
        "\n",
        "#cityscapes_data_path = \"/root/deeplabv3/data/cityscapes\"\n",
        "#cityscapes_meta_path = \"/root/deeplabv3/data/cityscapes/meta\"\n",
        "\n",
        "cityscapes_data_path = \"/content/drive/My Drive/U_Net_Datasets\"\n",
        "cityscapes_meta_path = \"/content/drive/My Drive/U_Net_Datasets/meta\"\n",
        "\n",
        "if not os.path.exists(cityscapes_meta_path):\n",
        "    os.makedirs(cityscapes_meta_path)\n",
        "if not os.path.exists(cityscapes_meta_path + \"/label_imgs\"):\n",
        "    os.makedirs(cityscapes_meta_path + \"/label_imgs\")\n",
        "\n",
        "################################################################################\n",
        "# convert all labels to label imgs with trainId pixel values (and save to disk):\n",
        "################################################################################\n",
        "train_label_img_paths = []\n",
        "\n",
        "img_dir = cityscapes_data_path + \"/leftImg8bit/train/\"\n",
        "label_dir = cityscapes_data_path + \"/gtFine/train/\"\n",
        "for train_dir in train_dirs:\n",
        "    print (train_dir)\n",
        "\n",
        "    train_img_dir_path = img_dir + train_dir\n",
        "    train_label_dir_path = label_dir + train_dir\n",
        "\n",
        "    file_names = os.listdir(train_img_dir_path)\n",
        "    for file_name in file_names:\n",
        "        img_id = file_name.split(\"_leftImg8bit.png\")[0]\n",
        "\n",
        "        gtFine_img_path = train_label_dir_path + img_id + \"_gtFine_labelIds.png\"\n",
        "        gtFine_img = cv2.imread(gtFine_img_path, -1) # (shape: (1024, 2048))\n",
        "\n",
        "        # convert gtFine_img from id to trainId pixel values:\n",
        "        label_img = id_to_trainId_map_func(gtFine_img)#(shape: (1024, 2048))\n",
        "        label_img = label_img.astype(np.uint8)\n",
        "\n",
        "        cv2.imwrite(cityscapes_meta_path + \"/label_imgs/\" + img_id + \".png\", label_img)\n",
        "        train_label_img_paths.append(cityscapes_meta_path + \"/label_imgs/\" + img_id + \".png\")\n",
        "\n",
        "img_dir = cityscapes_data_path + \"/leftImg8bit/val/\"\n",
        "label_dir = cityscapes_data_path + \"/gtFine/val/\"\n",
        "for val_dir in val_dirs:\n",
        "    print (val_dir)\n",
        "\n",
        "    val_img_dir_path = img_dir + val_dir\n",
        "    val_label_dir_path = label_dir + val_dir\n",
        "\n",
        "    file_names = os.listdir(val_img_dir_path)\n",
        "    for file_name in file_names:\n",
        "        img_id = file_name.split(\"_leftImg8bit.png\")[0]\n",
        "\n",
        "        gtFine_img_path = val_label_dir_path + img_id + \"_gtFine_labelIds.png\"\n",
        "        gtFine_img = cv2.imread(gtFine_img_path, -1) # (shape: (1024, 2048))\n",
        "\n",
        "        # convert gtFine_img from id to trainId pixel values:\n",
        "        label_img = id_to_trainId_map_func(gtFine_img) # (shape: (1024, 2048))\n",
        "        label_img = label_img.astype(np.uint8)\n",
        "\n",
        "        cv2.imwrite(cityscapes_meta_path + \"/label_imgs/\" + img_id + \".png\", label_img)\n",
        "\n",
        "################################################################################\n",
        "# compute the class weigths:\n",
        "################################################################################\n",
        "print (\"computing class weights\")\n",
        "\n",
        "num_classes = 20\n",
        "\n",
        "trainId_to_count = {}\n",
        "for trainId in range(num_classes):\n",
        "    trainId_to_count[trainId] = 0\n",
        "\n",
        "# get the total number of pixels in all train label_imgs that are of each object class:\n",
        "for step, label_img_path in enumerate(train_label_img_paths):\n",
        "    if step % 100 == 0:\n",
        "        print (step)\n",
        "\n",
        "    label_img = cv2.imread(label_img_path, -1)\n",
        "\n",
        "    for trainId in range(num_classes):\n",
        "        # count how many pixels in label_img which are of object class trainId:\n",
        "        trainId_mask = np.equal(label_img, trainId)\n",
        "        trainId_count = np.sum(trainId_mask)\n",
        "\n",
        "        # add to the total count:\n",
        "        trainId_to_count[trainId] += trainId_count\n",
        "\n",
        "# compute the class weights according to the ENet paper:\n",
        "class_weights = []\n",
        "total_count = sum(trainId_to_count.values())\n",
        "for trainId, count in trainId_to_count.items():\n",
        "    trainId_prob = float(count)/float(total_count)\n",
        "    trainId_weight = 1/np.log(1.02 + trainId_prob)\n",
        "    class_weights.append(trainId_weight)\n",
        "\n",
        "print (class_weights)\n",
        "\n",
        "with open(cityscapes_meta_path + \"/class_weights.pkl\", \"wb\") as file:\n",
        "    pickle.dump(class_weights, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jena/\n",
            "zurich/\n",
            "weimar/\n",
            "ulm/\n",
            "tubingen/\n",
            "stuttgart/\n",
            "strasbourg/\n",
            "monchengladbach/\n",
            "krefeld/\n",
            "hanover/\n",
            "hamburg/\n",
            "erfurt/\n",
            "dusseldorf/\n",
            "darmstadt/\n",
            "cologne/\n",
            "bremen/\n",
            "bochum/\n",
            "aachen/\n",
            "frankfurt/\n",
            "munster/\n",
            "lindau/\n",
            "computing class weights\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "[3.362088928135997, 14.031521298730318, 4.986657918172686, 39.254403222891234, 36.5125971773311, 32.89620795239199, 46.286660134462245, 40.69042748040039, 6.698241903441155, 33.55545414377673, 18.487832644189325, 32.97431249303082, 47.676506488107115, 12.70028597336979, 45.20542136324199, 45.78372411642551, 45.825290445040096, 48.40614734589367, 42.75592219573717, 7.912219457368151]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2INXlplU73ZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#einmalige ausführung\n",
        "#import zipfile\n",
        "#path_to_zip_file = #'/content/drive/My Drive/U_Net_Datasets/gtFine_trainvaltest.zip'\n",
        "#directory_to_extract_to = '/content/drive/My Drive/U_Net_Datasets' \n",
        "#with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "#    zip_ref.extractall(directory_to_extract_to)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJLURZEO5PaY",
        "colab_type": "text"
      },
      "source": [
        "####Random Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHSYi2oj5fxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this file contains code snippets which I have found (more or less) useful at\n",
        "# some point during the project. Probably nothing interesting to see here.\n",
        "\n",
        "import pickle\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "model_id = \"13_2_2_2\"\n",
        "\n",
        "with open(\"/home/fregu856/exjobb/training_logs/multitask/model_\" + model_id + \"/epoch_losses_train.pkl\", \"rb\") as file:\n",
        "    train_loss = pickle.load(file)\n",
        "\n",
        "with open(\"/home/fregu856/exjobb/training_logs/multitask/model_\" + model_id + \"/epoch_losses_val.pkl\", \"rb\") as file:\n",
        "    val_loss = pickle.load(file)\n",
        "\n",
        "print (\"train loss min:\", np.argmin(np.array(train_loss)), np.min(np.array(train_loss)))\n",
        "\n",
        "print (\"val loss min:\", np.argmin(np.array(val_loss)), np.min(np.array(val_loss)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfcowAUH32SX",
        "colab_type": "text"
      },
      "source": [
        "###Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwBiOz6m39og",
        "colab_type": "text"
      },
      "source": [
        "####ASPP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8lPxBUz2251",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ASPP, self).__init__()\n",
        "\n",
        "        self.conv_1x1_1 = nn.Conv2d(512, 256, kernel_size=1)\n",
        "        self.bn_conv_1x1_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_1 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=6, dilation=6)\n",
        "        self.bn_conv_3x3_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_2 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=12, dilation=12)\n",
        "        self.bn_conv_3x3_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_3 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=18, dilation=18)\n",
        "        self.bn_conv_3x3_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.conv_1x1_2 = nn.Conv2d(512, 256, kernel_size=1)\n",
        "        self.bn_conv_1x1_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_1x1_3 = nn.Conv2d(1280, 256, kernel_size=1) # (1280 = 5*256)\n",
        "        self.bn_conv_1x1_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_1x1_4 = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, feature_map):\n",
        "        # (feature_map has shape (batch_size, 512, h/16, w/16)) (assuming self.resnet is ResNet18_OS16 or ResNet34_OS16. If self.resnet instead is ResNet18_OS8 or ResNet34_OS8, it will be (batch_size, 512, h/8, w/8))\n",
        "\n",
        "        feature_map_h = feature_map.size()[2] # (== h/16)\n",
        "        feature_map_w = feature_map.size()[3] # (== w/16)\n",
        "\n",
        "        out_1x1 = F.relu(self.bn_conv_1x1_1(self.conv_1x1_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out_3x3_1 = F.relu(self.bn_conv_3x3_1(self.conv_3x3_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out_3x3_2 = F.relu(self.bn_conv_3x3_2(self.conv_3x3_2(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out_3x3_3 = F.relu(self.bn_conv_3x3_3(self.conv_3x3_3(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "\n",
        "        out_img = self.avg_pool(feature_map) # (shape: (batch_size, 512, 1, 1))\n",
        "        out_img = F.relu(self.bn_conv_1x1_2(self.conv_1x1_2(out_img))) # (shape: (batch_size, 256, 1, 1))\n",
        "        out_img = F.upsample(out_img, size=(feature_map_h, feature_map_w), mode=\"bilinear\") # (shape: (batch_size, 256, h/16, w/16))\n",
        "\n",
        "        out = torch.cat([out_1x1, out_3x3_1, out_3x3_2, out_3x3_3, out_img], 1) # (shape: (batch_size, 1280, h/16, w/16))\n",
        "        out = F.relu(self.bn_conv_1x1_3(self.conv_1x1_3(out))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out = self.conv_1x1_4(out) # (shape: (batch_size, num_classes, h/16, w/16))\n",
        "\n",
        "        return out\n",
        "\n",
        "class ASPP_Bottleneck(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ASPP_Bottleneck, self).__init__()\n",
        "\n",
        "        self.conv_1x1_1 = nn.Conv2d(4*512, 256, kernel_size=1)\n",
        "        self.bn_conv_1x1_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_1 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=6, dilation=6)\n",
        "        self.bn_conv_3x3_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_2 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=12, dilation=12)\n",
        "        self.bn_conv_3x3_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_3 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=18, dilation=18)\n",
        "        self.bn_conv_3x3_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.conv_1x1_2 = nn.Conv2d(4*512, 256, kernel_size=1)\n",
        "        self.bn_conv_1x1_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_1x1_3 = nn.Conv2d(1280, 256, kernel_size=1) # (1280 = 5*256)\n",
        "        self.bn_conv_1x1_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_1x1_4 = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, feature_map):\n",
        "        # (feature_map has shape (batch_size, 4*512, h/16, w/16))\n",
        "\n",
        "        feature_map_h = feature_map.size()[2] # (== h/16)\n",
        "        feature_map_w = feature_map.size()[3] # (== w/16)\n",
        "\n",
        "        out_1x1 = F.relu(self.bn_conv_1x1_1(self.conv_1x1_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out_3x3_1 = F.relu(self.bn_conv_3x3_1(self.conv_3x3_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out_3x3_2 = F.relu(self.bn_conv_3x3_2(self.conv_3x3_2(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out_3x3_3 = F.relu(self.bn_conv_3x3_3(self.conv_3x3_3(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "\n",
        "        out_img = self.avg_pool(feature_map) # (shape: (batch_size, 512, 1, 1))\n",
        "        out_img = F.relu(self.bn_conv_1x1_2(self.conv_1x1_2(out_img))) # (shape: (batch_size, 256, 1, 1))\n",
        "        out_img = F.upsample(out_img, size=(feature_map_h, feature_map_w), mode=\"bilinear\") # (shape: (batch_size, 256, h/16, w/16))\n",
        "\n",
        "        out = torch.cat([out_1x1, out_3x3_1, out_3x3_2, out_3x3_3, out_img], 1) # (shape: (batch_size, 1280, h/16, w/16))\n",
        "        out = F.relu(self.bn_conv_1x1_3(self.conv_1x1_3(out))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out = self.conv_1x1_4(out) # (shape: (batch_size, num_classes, h/16, w/16))\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO6CrH4C4MqC",
        "colab_type": "text"
      },
      "source": [
        "####ResNet\n",
        "\n",
        "Verschiedene vortrainierte ResNet modelle zum Trainieren\n",
        "Modellle sind auf ImageNet Datenbank vortrainiert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSFBJVdj4BJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NOTE! OS: output stride, the ratio of input image resolution to final output resolution (OS16: output size is (img_h/16, img_w/16)) (OS8: output size is (img_h/8, img_w/8))\n",
        "\n",
        "preTrainedModelsPath = datasetPath + \"/pretrained_models\"\n",
        "\n",
        "def make_layer(block, in_channels, channels, num_blocks, stride=1, dilation=1):\n",
        "    strides = [stride] + [1]*(num_blocks - 1) # (stride == 2, num_blocks == 4 --> strides == [2, 1, 1, 1])\n",
        "\n",
        "    blocks = []\n",
        "    for stride in strides:\n",
        "        blocks.append(block(in_channels=in_channels, channels=channels, stride=stride, dilation=dilation))\n",
        "        in_channels = block.expansion*channels\n",
        "\n",
        "    layer = nn.Sequential(*blocks) # (*blocks: call with unpacked list entires as arguments)\n",
        "\n",
        "    return layer\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, channels, stride=1, dilation=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        out_channels = self.expansion*channels\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=dilation, dilation=dilation, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=dilation, dilation=dilation, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "\n",
        "        if (stride != 1) or (in_channels != out_channels):\n",
        "            conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            bn = nn.BatchNorm2d(out_channels)\n",
        "            self.downsample = nn.Sequential(conv, bn)\n",
        "        else:\n",
        "            self.downsample = nn.Sequential()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (x has shape: (batch_size, in_channels, h, w))\n",
        "\n",
        "        out = F.relu(self.bn1(self.conv1(x))) # (shape: (batch_size, channels, h, w) if stride == 1, (batch_size, channels, h/2, w/2) if stride == 2)\n",
        "        out = self.bn2(self.conv2(out)) # (shape: (batch_size, channels, h, w) if stride == 1, (batch_size, channels, h/2, w/2) if stride == 2)\n",
        "\n",
        "        out = out + self.downsample(x) # (shape: (batch_size, channels, h, w) if stride == 1, (batch_size, channels, h/2, w/2) if stride == 2)\n",
        "\n",
        "        out = F.relu(out) # (shape: (batch_size, channels, h, w) if stride == 1, (batch_size, channels, h/2, w/2) if stride == 2)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channels, channels, stride=1, dilation=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "\n",
        "        out_channels = self.expansion*channels\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=dilation, dilation=dilation, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(channels, out_channels, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        if (stride != 1) or (in_channels != out_channels):\n",
        "            conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            bn = nn.BatchNorm2d(out_channels)\n",
        "            self.downsample = nn.Sequential(conv, bn)\n",
        "        else:\n",
        "            self.downsample = nn.Sequential()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (x has shape: (batch_size, in_channels, h, w))\n",
        "\n",
        "        out = F.relu(self.bn1(self.conv1(x))) # (shape: (batch_size, channels, h, w))\n",
        "        out = F.relu(self.bn2(self.conv2(out))) # (shape: (batch_size, channels, h, w) if stride == 1, (batch_size, channels, h/2, w/2) if stride == 2)\n",
        "        out = self.bn3(self.conv3(out)) # (shape: (batch_size, out_channels, h, w) if stride == 1, (batch_size, out_channels, h/2, w/2) if stride == 2)\n",
        "\n",
        "        out = out + self.downsample(x) # (shape: (batch_size, out_channels, h, w) if stride == 1, (batch_size, out_channels, h/2, w/2) if stride == 2)\n",
        "\n",
        "        out = F.relu(out) # (shape: (batch_size, out_channels, h, w) if stride == 1, (batch_size, out_channels, h/2, w/2) if stride == 2)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet_Bottleneck_OS16(nn.Module):\n",
        "    def __init__(self, num_layers):\n",
        "        super(ResNet_Bottleneck_OS16, self).__init__()\n",
        "\n",
        "        if num_layers == 50:\n",
        "            resnet = models.resnet50()\n",
        "            # load pretrained model:\n",
        "            resnet.load_state_dict(torch.load(preTrainedModelsPath+\"/resnet/resnet50-19c8e357.pth\"))\n",
        "            # remove fully connected layer, avg pool and layer5:\n",
        "            self.resnet = nn.Sequential(*list(resnet.children())[:-3])\n",
        "\n",
        "            print (\"pretrained resnet, 50\")\n",
        "        elif num_layers == 101:\n",
        "            resnet = models.resnet101()\n",
        "            # load pretrained model:\n",
        "            resnet.load_state_dict(torch.load(preTrainedModelsPath+\"/resnet/resnet101-5d3b4d8f.pth\"))\n",
        "            # remove fully connected layer, avg pool and layer5:\n",
        "            self.resnet = nn.Sequential(*list(resnet.children())[:-3])\n",
        "\n",
        "            print (\"pretrained resnet, 101\")\n",
        "        elif num_layers == 152:\n",
        "            resnet = models.resnet152()\n",
        "            # load pretrained model:\n",
        "            resnet.load_state_dict(torch.load(preTrainedModelsPath+\"/resnet/resnet152-b121ed2d.pth\"))\n",
        "            # remove fully connected layer, avg pool and layer5:\n",
        "            self.resnet = nn.Sequential(*list(resnet.children())[:-3])\n",
        "\n",
        "            print (\"pretrained resnet, 152\")\n",
        "        else:\n",
        "            raise Exception(\"num_layers must be in {50, 101, 152}!\")\n",
        "\n",
        "        self.layer5 = make_layer(Bottleneck, in_channels=4*256, channels=512, num_blocks=3, stride=1, dilation=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (x has shape (batch_size, 3, h, w))\n",
        "\n",
        "        # pass x through (parts of) the pretrained ResNet:\n",
        "        c4 = self.resnet(x) # (shape: (batch_size, 4*256, h/16, w/16)) (it's called c4 since 16 == 2^4)\n",
        "\n",
        "        output = self.layer5(c4) # (shape: (batch_size, 4*512, h/16, w/16))\n",
        "\n",
        "        return output\n",
        "\n",
        "class ResNet_BasicBlock_OS16(nn.Module):\n",
        "    def __init__(self, num_layers):\n",
        "        super(ResNet_BasicBlock_OS16, self).__init__()\n",
        "\n",
        "        if num_layers == 18:\n",
        "            resnet = models.resnet18()\n",
        "            # load pretrained model:\n",
        "            \n",
        "            \n",
        "            resnet.load_state_dict(torch.load(preTrainedModelsPath+\"/resnet/resnet18-5c106cde.pth\"))\n",
        "            # remove fully connected layer, avg pool and layer5:\n",
        "            \n",
        "            self.resnet = nn.Sequential(*list(resnet.children())[:-3])\n",
        "\n",
        "            num_blocks = 2\n",
        "            print (\"pretrained resnet, 18\")\n",
        "        elif num_layers == 34:\n",
        "            resnet = models.resnet34()\n",
        "            # load pretrained model:\n",
        "            resnet.load_state_dict(torch.load(preTrainedModelsPath+\"/resnet/resnet34-333f7ec4.pth\"))\n",
        "            # remove fully connected layer, avg pool and layer5:\n",
        "            self.resnet = nn.Sequential(*list(resnet.children())[:-3])\n",
        "\n",
        "            num_blocks = 3\n",
        "            print (\"pretrained resnet, 34\")\n",
        "        else:\n",
        "            raise Exception(\"num_layers must be in {18, 34}!\")\n",
        "\n",
        "        self.layer5 = make_layer(BasicBlock, in_channels=256, channels=512, num_blocks=num_blocks, stride=1, dilation=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (x has shape (batch_size, 3, h, w))\n",
        "\n",
        "        # pass x through (parts of) the pretrained ResNet:\n",
        "        c4 = self.resnet(x) # (shape: (batch_size, 256, h/16, w/16)) (it's called c4 since 16 == 2^4)\n",
        "\n",
        "        output = self.layer5(c4) # (shape: (batch_size, 512, h/16, w/16))\n",
        "\n",
        "        return output\n",
        "\n",
        "class ResNet_BasicBlock_OS8(nn.Module):\n",
        "    def __init__(self, num_layers):\n",
        "        super(ResNet_BasicBlock_OS8, self).__init__()\n",
        "\n",
        "        if num_layers == 18:\n",
        "            resnet = models.resnet18()\n",
        "            # load pretrained model:            \n",
        "            resnet.load_state_dict(torch.load(preTrainedModelsPath+\"/resnet/resnet18-5c106cde.pth\"))       \n",
        "\n",
        "            # remove fully connected layer, avg pool, layer4 and layer5:\n",
        "\n",
        "            \n",
        "            self.resnet = nn.Sequential(*list(resnet.children())[:-4])\n",
        "\n",
        "            num_blocks_layer_4 = 2\n",
        "            num_blocks_layer_5 = 2\n",
        "            print (\"pretrained resnet, 18\")\n",
        "        elif num_layers == 34:\n",
        "            resnet = models.resnet34()\n",
        "            # load pretrained model:\n",
        "            resnet.load_state_dict(torch.load(preTrainedModelsPath+\"/resnet/resnet34-333f7ec4.pth\"))\n",
        "            # remove fully connected layer, avg pool, layer4 and layer5:\n",
        "            self.resnet = nn.Sequential(*list(resnet.children())[:-4])\n",
        "\n",
        "            num_blocks_layer_4 = 6\n",
        "            num_blocks_layer_5 = 3\n",
        "            print (\"pretrained resnet, 34\")\n",
        "        else:\n",
        "            raise Exception(\"num_layers must be in {18, 34}!\")\n",
        "\n",
        "        self.layer4 = make_layer(BasicBlock, in_channels=128, channels=256, num_blocks=num_blocks_layer_4, stride=1, dilation=2)\n",
        "\n",
        "        self.layer5 = make_layer(BasicBlock, in_channels=256, channels=512, num_blocks=num_blocks_layer_5, stride=1, dilation=4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (x has shape (batch_size, 3, h, w))\n",
        "\n",
        "        # pass x through (parts of) the pretrained ResNet:\n",
        "        c3 = self.resnet(x) # (shape: (batch_size, 128, h/8, w/8)) (it's called c3 since 8 == 2^3)\n",
        "\n",
        "        output = self.layer4(c3) # (shape: (batch_size, 256, h/8, w/8))\n",
        "        output = self.layer5(output) # (shape: (batch_size, 512, h/8, w/8))\n",
        "\n",
        "        return output\n",
        "\n",
        "def ResNet18_OS16():\n",
        "    return ResNet_BasicBlock_OS16(num_layers=18)\n",
        "\n",
        "def ResNet34_OS16():\n",
        "    return ResNet_BasicBlock_OS16(num_layers=34)\n",
        "\n",
        "def ResNet50_OS16():\n",
        "    return ResNet_Bottleneck_OS16(num_layers=50)\n",
        "\n",
        "def ResNet101_OS16():\n",
        "    return ResNet_Bottleneck_OS16(num_layers=101)\n",
        "\n",
        "def ResNet152_OS16():\n",
        "    return ResNet_Bottleneck_OS16(num_layers=152)\n",
        "\n",
        "def ResNet18_OS8():\n",
        "    return ResNet_BasicBlock_OS8(num_layers=18)\n",
        "\n",
        "def ResNet34_OS8():\n",
        "    return ResNet_BasicBlock_OS8(num_layers=34)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l35iRa74B86",
        "colab_type": "text"
      },
      "source": [
        "####DeepLab V3\n",
        "\n",
        "Erstellen des DeepLab V3 \n",
        "  - hier kann man einstellen welches ResNet verwendet wird\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjlFHBL20GZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from resnet import ResNet18_OS16, ResNet34_OS16, ResNet50_OS16, ResNet101_OS16, ResNet152_OS16, ResNet18_OS8, ResNet34_OS8\n",
        "#from aspp import ASPP, ASPP_Bottleneck\n",
        "\n",
        "class DeepLabV3(nn.Module):\n",
        "    def __init__(self, model_id, project_dir):\n",
        "        super(DeepLabV3, self).__init__()\n",
        "\n",
        "        self.num_classes = 20\n",
        "\n",
        "        self.model_id = model_id\n",
        "        self.project_dir = project_dir\n",
        "        self.create_model_dirs()\n",
        "\n",
        "        self.resnet = ResNet18_OS8() # NOTE! specify the type of ResNet here\n",
        "        self.resnet = ResNet50_OS16() # NOTE! specify the type of ResNet here\n",
        "        #self.aspp = ASPP(num_classes=self.num_classes) # NOTE! if you use ResNet50-152,\n",
        "        #set self.aspp = ASPP_Bottleneck(num_classes=self.num_classes) #instead\n",
        "        self.aspp = ASPP_Bottleneck(num_classes=self.num_classes)\n",
        "    def forward(self, x):\n",
        "        # (x has shape (batch_size, 3, h, w))\n",
        "\n",
        "        h = x.size()[2]\n",
        "        w = x.size()[3]\n",
        "\n",
        "        feature_map = self.resnet(x) # (shape: (batch_size, 512, h/16, w/16)) (assuming self.resnet \n",
        "        #is ResNet18_OS16 or ResNet34_OS16. If self.resnet is ResNet18_OS8 or ResNet34_OS8,\n",
        "        # it will be (batch_size, 512, h/8, w/8). If self.resnet is ResNet50-152, it will be (batch_size, 4*512, h/16, w/16))\n",
        "\n",
        "        output = self.aspp(feature_map) # (shape: (batch_size, num_classes, h/16, w/16))\n",
        "\n",
        "        output = F.upsample(output, size=(h, w), mode=\"bilinear\") # (shape: (batch_size, num_classes, h, w))\n",
        "\n",
        "        return output\n",
        "\n",
        "    def create_model_dirs(self):\n",
        "        self.logs_dir = self.project_dir + \"/training_logs\"\n",
        "        self.model_dir = self.logs_dir + \"/model_%s\" % self.model_id\n",
        "        self.checkpoints_dir = self.model_dir + \"/checkpoints\"\n",
        "        if not os.path.exists(self.logs_dir):\n",
        "            os.makedirs(self.logs_dir)\n",
        "        if not os.path.exists(self.model_dir):\n",
        "            os.makedirs(self.model_dir)\n",
        "            os.makedirs(self.checkpoints_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZmj3OkL6brT",
        "colab_type": "text"
      },
      "source": [
        "###Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDSBaVoj6dpp",
        "colab_type": "code",
        "outputId": "3d8a7e2d-0f21-459f-89fb-da00c2865ff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "# NOTE! NOTE! change this to not overwrite all log data when you train the model:\n",
        "model_id = \"50_b3_w2\"\n",
        "\n",
        "num_epochs = 1000\n",
        "batch_size = 3\n",
        "learning_rate = 0.0001\n",
        "\n",
        "#network = DeepLabV3(model_id, project_dir=\"/root/deeplabv3\").cuda()\n",
        "network = DeepLabV3(model_id, project_dir=datasetPath).cuda()\n",
        "train_dataset = DatasetTrain(cityscapes_data_path=\"/content/drive/My Drive/U_Net_Datasets\",\n",
        "                             cityscapes_meta_path=\"/content/drive/My Drive/U_Net_Datasets/meta\")\n",
        "val_dataset = DatasetVal(cityscapes_data_path=\"/content/drive/My Drive/U_Net_Datasets\",\n",
        "                         cityscapes_meta_path=\"/content/drive/My Drive/U_Net_Datasets/meta\")\n",
        "\n",
        "num_train_batches = int(len(train_dataset)/batch_size)\n",
        "num_val_batches = int(len(val_dataset)/batch_size)\n",
        "print (\"num_train_batches:\", num_train_batches)\n",
        "print (\"num_val_batches:\", num_val_batches)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, shuffle=True,\n",
        "                                           num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                         batch_size=batch_size, shuffle=False,\n",
        "                                         num_workers=2)\n",
        "\n",
        "params = add_weight_decay(network, l2_value=0.0001)\n",
        "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
        "\n",
        "with open(\"/content/drive/My Drive/U_Net_Datasets/meta/class_weights.pkl\", \"rb\") as file: # (needed for python3)\n",
        "    class_weights = np.array(pickle.load(file))\n",
        "class_weights = torch.from_numpy(class_weights)\n",
        "class_weights = Variable(class_weights.type(torch.FloatTensor)).cuda()\n",
        "\n",
        "# loss function\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "epoch_losses_train = []\n",
        "epoch_losses_val = []\n",
        "for epoch in range(num_epochs):\n",
        "    print (\"###########################\")\n",
        "    print (\"######## NEW EPOCH ########\")\n",
        "    print (\"###########################\")\n",
        "    print (\"epoch: %d/%d\" % (epoch+1, num_epochs))\n",
        "\n",
        "    ############################################################################\n",
        "    # train:\n",
        "    ############################################################################\n",
        "    network.train() # (set in training mode, this affects BatchNorm and dropout)\n",
        "    batch_losses = []\n",
        "    for step, (imgs, label_imgs) in enumerate(train_loader):\n",
        "        #current_time = time.time()\n",
        "\n",
        "        imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n",
        "        label_imgs = Variable(label_imgs.type(torch.LongTensor)).cuda() # (shape: (batch_size, img_h, img_w))\n",
        "\n",
        "        outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n",
        "\n",
        "        # compute the loss:\n",
        "        loss = loss_fn(outputs, label_imgs)\n",
        "        loss_value = loss.data.cpu().numpy()\n",
        "        batch_losses.append(loss_value)\n",
        "\n",
        "        # optimization step:\n",
        "        optimizer.zero_grad() # (reset gradients)\n",
        "        loss.backward() # (compute gradients)\n",
        "        optimizer.step() # (perform optimization step)\n",
        "\n",
        "        #print (time.time() - current_time)\n",
        "\n",
        "    epoch_loss = np.mean(batch_losses)\n",
        "    epoch_losses_train.append(epoch_loss)\n",
        "    with open(\"%s/epoch_losses_train.pkl\" % network.model_dir, \"wb\") as file:\n",
        "        pickle.dump(epoch_losses_train, file)\n",
        "    print (\"train loss: %g\" % epoch_loss)\n",
        "    plt.figure(1)\n",
        "    plt.plot(epoch_losses_train, \"k^\")\n",
        "    plt.plot(epoch_losses_train, \"k\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.title(\"train loss per epoch\")\n",
        "    plt.savefig(\"%s/epoch_losses_train.png\" % network.model_dir)\n",
        "    plt.close(1)\n",
        "\n",
        "    print (\"####\")\n",
        "\n",
        "    ############################################################################\n",
        "    # val:\n",
        "    ############################################################################\n",
        "    network.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n",
        "    batch_losses = []\n",
        "    for step, (imgs, label_imgs, img_ids) in enumerate(val_loader):\n",
        "        with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n",
        "            imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n",
        "            label_imgs = Variable(label_imgs.type(torch.LongTensor)).cuda() # (shape: (batch_size, img_h, img_w))\n",
        "\n",
        "            outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n",
        "\n",
        "            # compute the loss:\n",
        "            loss = loss_fn(outputs, label_imgs)\n",
        "            loss_value = loss.data.cpu().numpy()\n",
        "            batch_losses.append(loss_value)\n",
        "\n",
        "    epoch_loss = np.mean(batch_losses)\n",
        "    epoch_losses_val.append(epoch_loss)\n",
        "    with open(\"%s/epoch_losses_val.pkl\" % network.model_dir, \"wb\") as file:\n",
        "        pickle.dump(epoch_losses_val, file)\n",
        "    print (\"val loss: %g\" % epoch_loss)\n",
        "    plt.figure(1)\n",
        "    plt.plot(epoch_losses_val, \"k^\")\n",
        "    plt.plot(epoch_losses_val, \"k\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.title(\"val loss per epoch\")\n",
        "    plt.savefig(\"%s/epoch_losses_val.png\" % network.model_dir)\n",
        "    plt.close(1)\n",
        "\n",
        "    # save the model weights to disk:\n",
        "    checkpoint_path = network.checkpoints_dir + \"/model_\" + model_id +\"_epoch_\" + str(epoch+1) + \".pth\"\n",
        "    torch.save(network.state_dict(), checkpoint_path)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pretrained resnet, 18\n",
            "pretrained resnet, 50\n",
            "num_train_batches: 991\n",
            "num_val_batches: 166\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 1/1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 1.51803\n",
            "####\n",
            "val loss: 1.16656\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 2/1000\n",
            "train loss: 1.1556\n",
            "####\n",
            "val loss: 0.945586\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 3/1000\n",
            "train loss: 1.07551\n",
            "####\n",
            "val loss: 0.840524\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 4/1000\n",
            "train loss: 1.00252\n",
            "####\n",
            "val loss: 0.903908\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 5/1000\n",
            "train loss: 0.960568\n",
            "####\n",
            "val loss: 0.838962\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 6/1000\n",
            "train loss: 0.913451\n",
            "####\n",
            "val loss: 0.800789\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 7/1000\n",
            "train loss: 0.869571\n",
            "####\n",
            "val loss: 0.750536\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 8/1000\n",
            "train loss: 0.855639\n",
            "####\n",
            "val loss: 0.759786\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 9/1000\n",
            "train loss: 0.883332\n",
            "####\n",
            "val loss: 0.770378\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 10/1000\n",
            "train loss: 0.84064\n",
            "####\n",
            "val loss: 0.740509\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 11/1000\n",
            "train loss: 0.829322\n",
            "####\n",
            "val loss: 0.696\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 12/1000\n",
            "train loss: 0.793913\n",
            "####\n",
            "val loss: 0.716837\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 13/1000\n",
            "train loss: 0.797343\n",
            "####\n",
            "val loss: 0.767752\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 14/1000\n",
            "train loss: 0.826244\n",
            "####\n",
            "val loss: 0.731913\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 15/1000\n",
            "train loss: 0.800223\n",
            "####\n",
            "val loss: 0.724955\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 16/1000\n",
            "train loss: 0.760507\n",
            "####\n",
            "val loss: 0.718527\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 17/1000\n",
            "train loss: 0.763173\n",
            "####\n",
            "val loss: 0.665768\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 18/1000\n",
            "train loss: 0.776733\n",
            "####\n",
            "val loss: 0.746024\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 19/1000\n",
            "train loss: 0.762621\n",
            "####\n",
            "val loss: 0.634022\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 20/1000\n",
            "train loss: 0.751064\n",
            "####\n",
            "val loss: 0.663833\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 21/1000\n",
            "train loss: 0.747389\n",
            "####\n",
            "val loss: 0.777798\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 22/1000\n",
            "train loss: 0.734546\n",
            "####\n",
            "val loss: 0.696415\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 23/1000\n",
            "train loss: 0.748716\n",
            "####\n",
            "val loss: 0.662045\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 24/1000\n",
            "train loss: 0.725263\n",
            "####\n",
            "val loss: 0.645919\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 25/1000\n",
            "train loss: 0.731122\n",
            "####\n",
            "val loss: 0.672301\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 26/1000\n",
            "train loss: 0.70671\n",
            "####\n",
            "val loss: 0.684043\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 27/1000\n",
            "train loss: 0.743706\n",
            "####\n",
            "val loss: 0.697983\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 28/1000\n",
            "train loss: 0.739183\n",
            "####\n",
            "val loss: 0.680925\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 29/1000\n",
            "train loss: 0.692993\n",
            "####\n",
            "val loss: 0.727175\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 30/1000\n",
            "train loss: 0.708791\n",
            "####\n",
            "val loss: 0.648628\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 31/1000\n",
            "train loss: 0.695084\n",
            "####\n",
            "val loss: 0.689336\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 32/1000\n",
            "train loss: 0.719278\n",
            "####\n",
            "val loss: 0.660839\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 33/1000\n",
            "train loss: 0.69462\n",
            "####\n",
            "val loss: 0.679052\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 34/1000\n",
            "train loss: 0.702778\n",
            "####\n",
            "val loss: 0.621311\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 35/1000\n",
            "train loss: 0.680608\n",
            "####\n",
            "val loss: 0.605729\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 36/1000\n",
            "train loss: 0.700531\n",
            "####\n",
            "val loss: 0.643788\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 37/1000\n",
            "train loss: 0.688532\n",
            "####\n",
            "val loss: 0.677574\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 38/1000\n",
            "train loss: 0.688385\n",
            "####\n",
            "val loss: 0.641541\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 39/1000\n",
            "train loss: 0.671893\n",
            "####\n",
            "val loss: 0.626105\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 40/1000\n",
            "train loss: 0.675276\n",
            "####\n",
            "val loss: 0.617244\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 41/1000\n",
            "train loss: 0.680801\n",
            "####\n",
            "val loss: 0.702338\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 42/1000\n",
            "train loss: 0.685227\n",
            "####\n",
            "val loss: 0.634196\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 43/1000\n",
            "train loss: 0.680796\n",
            "####\n",
            "val loss: 0.633074\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 44/1000\n",
            "train loss: 0.67107\n",
            "####\n",
            "val loss: 0.658617\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 45/1000\n",
            "train loss: 0.665659\n",
            "####\n",
            "val loss: 0.610004\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 46/1000\n",
            "train loss: 0.705645\n",
            "####\n",
            "val loss: 0.637455\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 47/1000\n",
            "train loss: 0.683903\n",
            "####\n",
            "val loss: 0.673631\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 48/1000\n",
            "train loss: 0.672878\n",
            "####\n",
            "val loss: 0.607815\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 49/1000\n",
            "train loss: 0.673559\n",
            "####\n",
            "val loss: 0.666474\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 50/1000\n",
            "train loss: 0.686\n",
            "####\n",
            "val loss: 0.636616\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 51/1000\n",
            "train loss: 0.650829\n",
            "####\n",
            "val loss: 0.630852\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 52/1000\n",
            "train loss: 0.658606\n",
            "####\n",
            "val loss: 0.622106\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 53/1000\n",
            "train loss: 0.646564\n",
            "####\n",
            "val loss: 0.596178\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 54/1000\n",
            "train loss: 0.676462\n",
            "####\n",
            "val loss: 0.622836\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 55/1000\n",
            "train loss: 0.668617\n",
            "####\n",
            "val loss: 0.66878\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 56/1000\n",
            "train loss: 0.652098\n",
            "####\n",
            "val loss: 0.637374\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 57/1000\n",
            "train loss: 0.644848\n",
            "####\n",
            "val loss: 0.646954\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 58/1000\n",
            "train loss: 0.661222\n",
            "####\n",
            "val loss: 0.645661\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 59/1000\n",
            "train loss: 0.665644\n",
            "####\n",
            "val loss: 0.605698\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 60/1000\n",
            "train loss: 0.650469\n",
            "####\n",
            "val loss: 0.629879\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 61/1000\n",
            "train loss: 0.642274\n",
            "####\n",
            "val loss: 0.644443\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 62/1000\n",
            "train loss: 0.640579\n",
            "####\n",
            "val loss: 0.632654\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 63/1000\n",
            "train loss: 0.643944\n",
            "####\n",
            "val loss: 0.605938\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 64/1000\n",
            "train loss: 0.654281\n",
            "####\n",
            "val loss: 0.625766\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 65/1000\n",
            "train loss: 0.649447\n",
            "####\n",
            "val loss: 0.628633\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 66/1000\n",
            "train loss: 0.651391\n",
            "####\n",
            "val loss: 0.611329\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 67/1000\n",
            "train loss: 0.663381\n",
            "####\n",
            "val loss: 0.650325\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 68/1000\n",
            "train loss: 0.643156\n",
            "####\n",
            "val loss: 0.600205\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 69/1000\n",
            "train loss: 0.628835\n",
            "####\n",
            "val loss: 0.641461\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 70/1000\n",
            "train loss: 0.650165\n",
            "####\n",
            "val loss: 0.616382\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 71/1000\n",
            "train loss: 0.635905\n",
            "####\n",
            "val loss: 1.01004\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 72/1000\n",
            "train loss: 0.644867\n",
            "####\n",
            "val loss: 0.594414\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 73/1000\n",
            "train loss: 0.6298\n",
            "####\n",
            "val loss: 0.630373\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 74/1000\n",
            "train loss: 0.641137\n",
            "####\n",
            "val loss: 0.609237\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 75/1000\n",
            "train loss: 0.649861\n",
            "####\n",
            "val loss: 0.62046\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 76/1000\n",
            "train loss: 0.633664\n",
            "####\n",
            "val loss: 0.614921\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 77/1000\n",
            "train loss: 0.639088\n",
            "####\n",
            "val loss: 0.638291\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 78/1000\n",
            "train loss: 0.620855\n",
            "####\n",
            "val loss: 0.636286\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 79/1000\n",
            "train loss: 0.640649\n",
            "####\n",
            "val loss: 0.639452\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 80/1000\n",
            "train loss: 0.625377\n",
            "####\n",
            "val loss: 0.639091\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 81/1000\n",
            "train loss: 0.613066\n",
            "####\n",
            "val loss: 0.605221\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 82/1000\n",
            "train loss: 0.629444\n",
            "####\n",
            "val loss: 0.655847\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 83/1000\n",
            "train loss: 0.651447\n",
            "####\n",
            "val loss: 0.626215\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 84/1000\n",
            "train loss: 0.631541\n",
            "####\n",
            "val loss: 0.679945\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 85/1000\n",
            "train loss: 0.627246\n",
            "####\n",
            "val loss: 0.637755\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 86/1000\n",
            "train loss: 0.632519\n",
            "####\n",
            "val loss: 0.636502\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 87/1000\n",
            "train loss: 0.625635\n",
            "####\n",
            "val loss: 0.587901\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 88/1000\n",
            "train loss: 0.62589\n",
            "####\n",
            "val loss: 0.628299\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 89/1000\n",
            "train loss: 0.633383\n",
            "####\n",
            "val loss: 0.595024\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 90/1000\n",
            "train loss: 0.6319\n",
            "####\n",
            "val loss: 0.566599\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 91/1000\n",
            "train loss: 0.61693\n",
            "####\n",
            "val loss: 0.58637\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 92/1000\n",
            "train loss: 0.633434\n",
            "####\n",
            "val loss: 0.62191\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 93/1000\n",
            "train loss: 0.62393\n",
            "####\n",
            "val loss: 0.603315\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 94/1000\n",
            "train loss: 0.604091\n",
            "####\n",
            "val loss: 0.563309\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 95/1000\n",
            "train loss: 0.61613\n",
            "####\n",
            "val loss: 0.633579\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 96/1000\n",
            "train loss: 0.604695\n",
            "####\n",
            "val loss: 0.590256\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 97/1000\n",
            "train loss: 0.623726\n",
            "####\n",
            "val loss: 0.598502\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 98/1000\n",
            "train loss: 0.610664\n",
            "####\n",
            "val loss: 0.692681\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 99/1000\n",
            "train loss: 0.651973\n",
            "####\n",
            "val loss: 0.607992\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 100/1000\n",
            "train loss: 0.615222\n",
            "####\n",
            "val loss: 0.605655\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 101/1000\n",
            "train loss: 0.626492\n",
            "####\n",
            "val loss: 0.581542\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 102/1000\n",
            "train loss: 0.606395\n",
            "####\n",
            "val loss: 0.618394\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 103/1000\n",
            "train loss: 0.634453\n",
            "####\n",
            "val loss: 0.637914\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 104/1000\n",
            "train loss: 0.608214\n",
            "####\n",
            "val loss: 0.629247\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 105/1000\n",
            "train loss: 0.610647\n",
            "####\n",
            "val loss: 0.602144\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 106/1000\n",
            "train loss: 0.615173\n",
            "####\n",
            "val loss: 0.577639\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 107/1000\n",
            "train loss: 0.646034\n",
            "####\n",
            "val loss: 0.587716\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 108/1000\n",
            "train loss: 0.597612\n",
            "####\n",
            "val loss: 0.582836\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 109/1000\n",
            "train loss: 0.616399\n",
            "####\n",
            "val loss: 0.580828\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 110/1000\n",
            "train loss: 0.590974\n",
            "####\n",
            "val loss: 0.595797\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 111/1000\n",
            "train loss: 0.602328\n",
            "####\n",
            "val loss: 0.600306\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 112/1000\n",
            "train loss: 0.612171\n",
            "####\n",
            "val loss: 0.609785\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 113/1000\n",
            "train loss: 0.604976\n",
            "####\n",
            "val loss: 0.598732\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 114/1000\n",
            "train loss: 0.605219\n",
            "####\n",
            "val loss: 0.591662\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 115/1000\n",
            "train loss: 0.609512\n",
            "####\n",
            "val loss: 0.60805\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 116/1000\n",
            "train loss: 0.603757\n",
            "####\n",
            "val loss: 0.59645\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 117/1000\n",
            "train loss: 0.595508\n",
            "####\n",
            "val loss: 0.581385\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 118/1000\n",
            "train loss: 0.594157\n",
            "####\n",
            "val loss: 0.607103\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 119/1000\n",
            "train loss: 0.60149\n",
            "####\n",
            "val loss: 0.58518\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 120/1000\n",
            "train loss: 0.624814\n",
            "####\n",
            "val loss: 0.603198\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 121/1000\n",
            "train loss: 0.606661\n",
            "####\n",
            "val loss: 0.742254\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 122/1000\n",
            "train loss: 0.617892\n",
            "####\n",
            "val loss: 0.703569\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 123/1000\n",
            "train loss: 0.616193\n",
            "####\n",
            "val loss: 0.580368\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 124/1000\n",
            "train loss: 0.609082\n",
            "####\n",
            "val loss: 0.592938\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 125/1000\n",
            "train loss: 0.620825\n",
            "####\n",
            "val loss: 0.631729\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 126/1000\n",
            "train loss: 0.606982\n",
            "####\n",
            "val loss: 0.592576\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 127/1000\n",
            "train loss: 0.598917\n",
            "####\n",
            "val loss: 0.571721\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 128/1000\n",
            "train loss: 0.606632\n",
            "####\n",
            "val loss: 0.60863\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 129/1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-dba82c0b0dfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (set in training mode, this affects BatchNorm and dropout)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mbatch_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_imgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;31m#current_time = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LnKitgx4dCi",
        "colab_type": "text"
      },
      "source": [
        "###Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIIqzmN-4jqR",
        "colab_type": "text"
      },
      "source": [
        "####Eval on Validation Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKDbz7DWWw_3",
        "colab_type": "text"
      },
      "source": [
        "#####Code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPKGInk04i9Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "network = DeepLabV3(\"eval_val\", project_dir=datasetPath).cuda()\n",
        "network.load_state_dict(torch.load(datasetPath+\"/pretrained_models/model_13_2_2_2_epoch_580.pth\"))\n",
        "\n",
        "val_dataset = DatasetVal(cityscapes_data_path, cityscapes_meta_path)\n",
        "\n",
        "num_val_batches = int(len(val_dataset)/batch_size)\n",
        "print (\"num_val_batches:\", num_val_batches)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                         batch_size=batch_size, shuffle=False,\n",
        "                                         num_workers=1)\n",
        "\n",
        "with open(cityscapes_meta_path+\"/class_weights.pkl\", \"rb\") as file: # (needed for python3)\n",
        "    class_weights = np.array(pickle.load(file))\n",
        "class_weights = torch.from_numpy(class_weights)\n",
        "class_weights = Variable(class_weights.type(torch.FloatTensor)).cuda()\n",
        "\n",
        "# loss function\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "network.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n",
        "batch_losses = []\n",
        "for step, (imgs, label_imgs, img_ids) in enumerate(val_loader):\n",
        "    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n",
        "        imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n",
        "        label_imgs = Variable(label_imgs.type(torch.LongTensor)).cuda() # (shape: (batch_size, img_h, img_w))\n",
        "\n",
        "        outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n",
        "\n",
        "        # compute the loss:\n",
        "        loss = loss_fn(outputs, label_imgs)\n",
        "        loss_value = loss.data.cpu().numpy()\n",
        "        batch_losses.append(loss_value)\n",
        "\n",
        "        ########################################################################\n",
        "        # save data for visualization:\n",
        "        ########################################################################\n",
        "        outputs = outputs.data.cpu().numpy() # (shape: (batch_size, num_classes, img_h, img_w))\n",
        "        pred_label_imgs = np.argmax(outputs, axis=1) # (shape: (batch_size, img_h, img_w))\n",
        "        pred_label_imgs = pred_label_imgs.astype(np.uint8)\n",
        "\n",
        "        for i in range(pred_label_imgs.shape[0]):\n",
        "            if i == 0:\n",
        "                pred_label_img = pred_label_imgs[i] # (shape: (img_h, img_w))\n",
        "                img_id = img_ids[i]\n",
        "                img = imgs[i] # (shape: (3, img_h, img_w))\n",
        "\n",
        "                img = img.data.cpu().numpy()\n",
        "                img = np.transpose(img, (1, 2, 0)) # (shape: (img_h, img_w, 3))\n",
        "                img = img*np.array([0.229, 0.224, 0.225])\n",
        "                img = img + np.array([0.485, 0.456, 0.406])\n",
        "                img = img*255.0\n",
        "                img = img.astype(np.uint8)\n",
        "\n",
        "                pred_label_img_color = label_img_to_color(pred_label_img)\n",
        "                overlayed_img = 0.35*img + 0.65*pred_label_img_color\n",
        "                overlayed_img = overlayed_img.astype(np.uint8)\n",
        "\n",
        "                cv2.imwrite(network.model_dir + \"/\" + img_id + \"_overlayed.png\", overlayed_img)\n",
        "\n",
        "val_loss = np.mean(batch_losses)\n",
        "print (\"val loss: %g\" % val_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWkMA7Qv4uww",
        "colab_type": "text"
      },
      "source": [
        "####Eval for Metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBd_G2JnW67_",
        "colab_type": "text"
      },
      "source": [
        "#####Code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "512GO4Vh4yl9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "outputId": "3ca71468-22e4-454a-ed19-dafc4907aae0"
      },
      "source": [
        "\n",
        "\n",
        "trainId_to_id = {\n",
        "    0: 7,\n",
        "    1: 8,\n",
        "    2: 11,\n",
        "    3: 12,\n",
        "    4: 13,\n",
        "    5: 17,\n",
        "    6: 19,\n",
        "    7: 20,\n",
        "    8: 21,\n",
        "    9: 22,\n",
        "    10: 23,\n",
        "    11: 24,\n",
        "    12: 25,\n",
        "    13: 26,\n",
        "    14: 27,\n",
        "    15: 28,\n",
        "    16: 31,\n",
        "    17: 32,\n",
        "    18: 33,\n",
        "    19: 0\n",
        "}\n",
        "trainId_to_id_map_func = np.vectorize(trainId_to_id.get)\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "network = DeepLabV3(\"eval_val_for_metrics\", project_dir=datasetPath).cuda()\n",
        "network.load_state_dict(torch.load(datasetPath+\"/pretrained_models/model_13_2_2_2_epoch_580.pth\"))\n",
        "\n",
        "val_dataset = DatasetVal(cityscapes_data_path,cityscapes_meta_path)\n",
        "\n",
        "num_val_batches = int(len(val_dataset)/batch_size)\n",
        "print (\"num_val_batches:\", num_val_batches)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                         batch_size=batch_size, shuffle=False,\n",
        "                                         num_workers=1)\n",
        "\n",
        "with open(cityscapes_meta_path+\"/class_weights.pkl\", \"rb\") as file: # (needed for python3)\n",
        "    class_weights = np.array(pickle.load(file))\n",
        "class_weights = torch.from_numpy(class_weights)\n",
        "class_weights = Variable(class_weights.type(torch.FloatTensor)).cuda()\n",
        "\n",
        "# loss function\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "network.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n",
        "batch_losses = []\n",
        "for step, (imgs, label_imgs, img_ids) in enumerate(val_loader):\n",
        "    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n",
        "        imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n",
        "        label_imgs = Variable(label_imgs.type(torch.LongTensor)).cuda() # (shape: (batch_size, img_h, img_w))\n",
        "\n",
        "        outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n",
        "\n",
        "        # compute the loss:\n",
        "        loss = loss_fn(outputs, label_imgs)\n",
        "        loss_value = loss.data.cpu().numpy()\n",
        "        batch_losses.append(loss_value)\n",
        "\n",
        "        ########################################################################\n",
        "        # save data for visualization:\n",
        "        ########################################################################\n",
        "        outputs = F.upsample(outputs, size=(1024, 2048), mode=\"bilinear\") # (shape: (batch_size, num_classes, 1024, 2048))\n",
        "\n",
        "        outputs = outputs.data.cpu().numpy() # (shape: (batch_size, num_classes, 1024, 2048))\n",
        "        pred_label_imgs = np.argmax(outputs, axis=1) # (shape: (batch_size, 1024, 2048))\n",
        "        pred_label_imgs = pred_label_imgs.astype(np.uint8)\n",
        "\n",
        "        for i in range(pred_label_imgs.shape[0]):\n",
        "            pred_label_img = pred_label_imgs[i] # (shape: (1024, 2048))\n",
        "            img_id = img_ids[i]\n",
        "\n",
        "            # convert pred_label_img from trainId to id pixel values:\n",
        "            pred_label_img = trainId_to_id_map_func(pred_label_img) # (shape: (1024, 2048))\n",
        "            pred_label_img = pred_label_img.astype(np.uint8)\n",
        "\n",
        "            cv2.imwrite(network.model_dir + \"/\" + img_id + \"_pred_label_img.png\", pred_label_img)\n",
        "\n",
        "val_loss = np.mean(batch_losses)\n",
        "print (\"val loss: %g\" % val_loss)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pretrained resnet, 50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-6a5daee58da3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepLabV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eval_val_for_metrics\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasetPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetPath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/pretrained_models/model_13_2_2_2_epoch_580.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetVal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcityscapes_data_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcityscapes_meta_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 839\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DeepLabV3:\n\tMissing key(s) in state_dict: \"resnet.resnet.4.0.conv3.weight\", \"resnet.resnet.4.0.bn3.weight\", \"resnet.resnet.4.0.bn3.bias\", \"resnet.resnet.4.0.bn3.running_mean\", \"resnet.resnet.4.0.bn3.running_var\", \"resnet.resnet.4.0.downsample.0.weight\", \"resnet.resnet.4.0.downsample.1.weight\", \"resnet.resnet.4.0.downsample.1.bias\", \"resnet.resnet.4.0.downsample.1.running_mean\", \"resnet.resnet.4.0.downsample.1.running_var\", \"resnet.resnet.4.1.conv3.weight\", \"resnet.resnet.4.1.bn3.weight\", \"resnet.resnet.4.1.bn3.bias\", \"resnet.resnet.4.1.bn3.running_mean\", \"resnet.resnet.4.1.bn3.running_var\", \"resnet.resnet.4.2.conv1.weight\", \"resnet.resnet.4.2.bn1.weight\", \"resnet.resnet.4.2.bn1.bias\", \"resnet.resnet.4.2.bn1.running_mean\", \"resnet.resnet.4.2.bn1.running_var\", \"resnet.resnet.4.2.conv2.weight\", \"resnet.resnet.4.2.bn2.weight\", \"resnet.resnet.4.2.bn2.bias\", \"resnet.resnet.4.2.bn2.running_mean\", \"resnet.resnet.4.2.bn2.running_var\", \"resnet.resnet.4.2.conv3.weight\", \"resnet.resnet.4.2.bn3.weight\", \"resnet.resnet.4.2.bn3.bias\", \"resnet.resnet.4.2.bn3.running_mean\", \"resnet.resnet.4.2.bn3.running_var\", \"resnet.resnet.5.0.conv3.weight\", \"resnet.resnet.5.0.bn3.weight\", \"resnet.resnet.5.0.bn3.bias\", \"resnet.resnet.5.0.bn3.running_mean\", \"resnet.resnet.5.0.bn3.running_var\", \"resnet.resnet.5.1.conv3.weight\", \"resnet.resnet.5.1.bn3.weight\", \"resnet.resnet.5.1.bn3.bias\", \"resnet.resnet.5.1.bn3.running_mean\", \"resnet.resnet.5.1.bn3.running_var\", \"resnet.resnet.5.2.conv1.weight\", \"resnet.resnet.5.2.bn...\n\tUnexpected key(s) in state_dict: \"resnet.layer4.0.conv1.weight\", \"resnet.layer4.0.bn1.weight\", \"resnet.layer4.0.bn1.bias\", \"resnet.layer4.0.bn1.running_mean\", \"resnet.layer4.0.bn1.running_var\", \"resnet.layer4.0.conv2.weight\", \"resnet.layer4.0.bn2.weight\", \"resnet.layer4.0.bn2.bias\", \"resnet.layer4.0.bn2.running_mean\", \"resnet.layer4.0.bn2.running_var\", \"resnet.layer4.0.downsample.0.weight\", \"resnet.layer4.0.downsample.1.weight\", \"resnet.layer4.0.downsample.1.bias\", \"resnet.layer4.0.downsample.1.running_mean\", \"resnet.layer4.0.downsample.1.running_var\", \"resnet.layer4.1.conv1.weight\", \"resnet.layer4.1.bn1.weight\", \"resnet.layer4.1.bn1.bias\", \"resnet.layer4.1.bn1.running_mean\", \"resnet.layer4.1.bn1.running_var\", \"resnet.layer4.1.conv2.weight\", \"resnet.layer4.1.bn2.weight\", \"resnet.layer4.1.bn2.bias\", \"resnet.layer4.1.bn2.running_mean\", \"resnet.layer4.1.bn2.running_var\". \n\tsize mismatch for resnet.resnet.4.0.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n\tsize mismatch for resnet.resnet.4.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for resnet.resnet.5.0.conv1.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for resnet.resnet.5.0.downsample.0.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for resnet.resnet.5.0.downsample.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.resnet.5.0.downsample.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.resnet.5.0.downsample.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.resnet.5.0.downsample.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.resnet.5.1.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for resnet.layer5.0.conv1.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 1, 1]).\n\tsize mismatch for resnet.layer5.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for resnet.layer5.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer5.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer5.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer5.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer5.1.conv1.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 2048, 1, 1]).\n\tsize mismatch for aspp.conv_1x1_1.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 2048, 1, 1]).\n\tsize mismatch for aspp.conv_3x3_1.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 2048, 3, 3]).\n\tsize mismatch for aspp.conv_3x3_2.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 2048, 3, 3]).\n\tsize mismatch for aspp.conv_3x3_3.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 2048, 3, 3]).\n\tsize mismatch for aspp.conv_1x1_2.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 2048, 1, 1])."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM3GWpVd5o5w",
        "colab_type": "text"
      },
      "source": [
        "###Visualisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx4KIJoKYOJe",
        "colab_type": "text"
      },
      "source": [
        "###### Visualisazion ResNet18 --> Epoch 91"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY3hJ7m2G0EO",
        "colab_type": "text"
      },
      "source": [
        "Erster versuch mit dem ResNet 18\n",
        "- Batch 3 \n",
        "- Worker 2\n",
        "\n",
        "https://drive.google.com/file/d/1J51SAo6sth0fvglgUgjNribSPnNhX9l1/view"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPeEKcx2W-ag",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "d71b09e3-488d-451c-c326-7f5ec51797fe"
      },
      "source": [
        "\n",
        "plt.plot(ResNet18_Train_losses)\n",
        "plt.plot(ResNet18_val_losses)\n",
        "plt.title('ResNet18 Training to Epoch 92')\n",
        "plt.xlabel('Epoch')\n",
        "plt.yticks(np.arange(0.5, 1.7, 0.1))\n",
        "plt.ylabel('')\n",
        "plt.show()"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEWCAYAAAB2X2wCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRZD899HYaBe",
        "colab_type": "text"
      },
      "source": [
        "###### Visualisazion ResNet50 --> Epoch 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WmIDFLhhZJpi"
      },
      "source": [
        "Zweiter versuch mit dem ResNet 50\n",
        "- Batch 3 \n",
        "- Worker 2\n",
        "\n",
        "TODO: Link für Video ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASjsFn1XW5Kj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "ba27012b-8a86-4abd-b0df-3ceb5613092d"
      },
      "source": [
        "ResNet50_Train_losses = pickle.load(\n",
        "    open('/content/drive/My Drive/U_Net_Datasets/training_logs/model_50_b3_w2/epoch_losses_train.pkl','rb'))\n",
        "ResNet50_val_losses = pickle.load(\n",
        "    open('/content/drive/My Drive/U_Net_Datasets/training_logs/model_50_b3_w2/epoch_losses_val.pkl','rb'))\n",
        "\n",
        "ResNet18_Train_losses = pickle.load(\n",
        "    open('/content/drive/My Drive/U_Net_Datasets/training_logs/model_5/epoch_losses_train.pkl','rb'))\n",
        "ResNet18_val_losses = pickle.load(\n",
        "    open('/content/drive/My Drive/U_Net_Datasets/training_logs/model_5/epoch_losses_val.pkl','rb'))\n",
        "\n",
        "plt.plot(ResNet50_Train_losses )\n",
        "plt.plot(ResNet50_val_losses)\n",
        "plt.title('ResNet50 Training to Epoch 128')\n",
        "plt.yticks(np.arange(0.5, 1.7, 0.1))\n",
        "plt.ylabel('')\n",
        "plt.show()\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOydd3iUVfbHPyeZFAhJgBRChwChSS/S\nEUEEBOzYy9rLz767trXsrrq66tqxK3ZUVCyoqPSOoXcCodcUCOlt7u+PO5NMGglkkpBwPs+TZzLv\ne9/3vfMm833PPffcc8QYg6IoilL78anpDiiKoijeQQVdURSljqCCriiKUkdQQVcURakjqKAriqLU\nEVTQFUVR6ggq6EqdRESiRSTN221PB0Rkr4icVdP9UE4cFfRTEBHZKSKZIpImIgdFZIqINPDSeQ+L\nSJDHtptEZG4Fj58iIk8V2zZXRLJcfU0TkS3F9l8pIrtEJF1EpotI41LO28rj+DQRMa727vdDT/Sz\nGmPijTEVumcn0vZEEZFPReTJShy/sNj9TROR77zYxUohIs1F5EcROeD6u7Uotv8lEdkmIqkisklE\nriq2/xwRWeXav11EbqzeT1C3UEE/dZngEpmeQC/gYS+d1xe4x0vncvN/xpgGrp+O7o0i0hV4G7gG\naAJkAJOLH2yM2e1xvFtYe3hsW1D8GBHx9fJnOJW5zfP+GGMurOkOeeAEfgYuKWN/GnAeEArcALwh\nIv0BRCQA+BZ4HQgBrgReFZEzqrrTdRUV9FMcY8xBYCZW2AH7RRCRF0Rkt4gcEpG3RKSea1+4iPwk\nIkdFJFlEFoiI59/5eeCvItKwtOuJSCcR+d117BYRmeTafgtwFfB3l5X4YwW6fxXwozFmvjEmDXgM\nuEhEgk/0Prgs3TdE5FcRSQeGishEEVktIsdc9+Ixj/btRcR4vF8oIv8UkcUua/BX92jhRNq69v/F\ndb1EEXmkLBeFiNwBXAY84mlZi0hXEZnn+hutE5HzTvR+uM4zyjXqelxEkkRkh4hc7rG/oeu+Jbja\nPSwi4rH/VhHZ7PqM60Wkh8fpe7v6liIiX7jEtwTGmAPGmDeBFWXsf8wYs8UY4zTGLAEWAwNdu8OB\nBsAnxrIM2Ap0Ppn7oaign/K4hrBjgW0em58FYrAi3x5oDjzu2vcAsBeIwFrFjwCe+R1igbnAX0u5\nVhDwO/A5EAlcDkwWkS7GmHeAz4D/uqzECR6H/sclbouKCVtXYI37jTFmO5Dj6vvJcCXwTyAYWIK1\n/q4CGgITgHtEZHw5x1+HvS9BwP0n2lZEugGvYu9Nc+x9jirtBMaYycCXwDNuy1pE/IGfgBmuY+8D\nvhSR9uV9+DJogb0fzYAbgQ88zjUZqA9EA2e79l/r+hxXAP/A3r8Q4CIg2eO8k4BzXMf2wY6yKoWI\n1Af6AhsAjDH7gK+Bv4iIr4gMxt7TRZW91umKCvqpy3QRSQX2AIeBJwBcFtYtwH3GmGRjTCrwDFZg\nAHKBpkBrY0yuMWaBKZmw53HgLhGJKLZ9PLDTGPOhMSbPGLMK+Aa49Dj9fBD7pW8OvAP8KCLtXPsa\nACnF2qdgBehk+M4Ys8Rl7WUbY2YbYza43q8BpgLDj3P8+8aYOGNMBlZIep5E20uB6caYxcaYbKwo\nngiDAX/gedff5w/gFwr/fqUx2WXNu3+e8NjnBJ5w3w/gV+BSEfHDivJDxphUY0w88BKFwnwT8Kwx\nZoXLOt5qjNnjcd6XjTEHjTFJ2AfQ8e5Vubj+b98Blrs+s5vPgKeAbKyh8ZAxZn9lrnU6o4J+6nKB\nMSYYOAvohB2egrXq6gMr3F9w7JfYLc7PY63530QkXkQeKn5iY8x67Je0+L7WwJme4oG14Eq1QF3n\nWuYSjGxjzEdY62qca3ca1vrzJARILf/jl4qn4CAiA8VOyiaISApWpMJLPxSAgx6/Z2AfOCfatpln\nP4wx6cCRCvTdTTNgd7GH7C7sA7Es7jDGNPT4+afHviTXQ8fzXM2wIyxf1/vSrtMS2H6ca57IvaoI\n/8OOzK5wbxA7x/K5a5s/0A14VETGVPJapy0q6Kc4xph5wBTgBdemRCAT6OrxBQ91Tya6xPUBY0w0\nMBG4X0RGlnLqJ4CbKSoke4B5xcSjgTHmdnd3KtJlwO2n3QAU+GVFJBoIwPpJT4bi15+KHUG0NMaE\nAu95XLuqOIB1cwAFbqpGx2lfvM/7gZaevmygFbDvJPsT5p4/8TjXfuyoLh/7kC7tOnuAdlQDIvI0\nMBIY4xpRuukGbDTG/OEaZW3GjlZU0E8SFfTawcvAOSLSwxjjBN4FXhKRSCgIHTvX9ft41ySfYN0b\n+dhheRGMMduw/t27PTb/BMSIyDUi4uf66Sci7kmqQ1j3Cq5rNRSRc0UkUEQcYkPShmFHDGCH0xNE\nZKhL+P4FfFvsS10ZgoFkY0yWiAzg+G4Lb/E1cIGIDHD5w/9VTvsi9ww7KZgHPOC6v2djRzRfnmR/\nfIAnRcTfNX8xFphmjMkFpgHPiEgDEWmL9dd/6jruPewEdy+xdBCRlifTAREJxD6oAQI8J1DFTlRf\nApxjjEkudugqoJOInOXuA/ZerD2Zfigq6LUCY0wC8DGFE58PYt0qS0XkGPAH4A4X7OB6n4adOJxs\njJlTxqn/hZ3wc18nFRiNFcb92GH3cxR+Wd8HurjcMdMBP6z/MwE7crgL6yra6jrfBuA2rLAfxgrw\nHSd9I0pyO3ZCNhU7+fuVF89dKsaYtVhh/Bp7j5JcP9llHPIe0ENEjojINJfffQJwPvaevQpcaYyJ\nO85l35KicejLPfbtBdKxI4ePgJs8znUHdhJ6JzDPtf9j1+f4Avu3/RI4hg0fPN5Io1RExIEdMR51\nbdrm6o87tPRfQBtgu0f//+7qwxbsKPENVx9mY0ddU060H4pFSs6XKYpSUUQkBCtmrYtNKlbHtUcB\n7xlj2lTndZVTF7XQFeUEERv/Xl/s6t0XgZXVLeaKUhrlCrqIfCB2ufj647Q5S+wCjw0iMs+7XVSU\nU44Lse6WvVh3whXHba0o1US5LhcRGYb1x35sjCmxJFfsisPF2Bns3SISaYw5XCW9VRRFUcqkXAvd\nGDOfoivIinMlNnJht6u9irmiKEoN4PDCOWIAP7EZ+4KBV4wxH5fWUGw+kFsAgoKC+nTq1MkLl1cU\nRTl9WLFiRaIxpvgqb8A7gu7A5noYCdQDlojIUnfomieufCDvAPTt29fExsZ64fKKoiinDyKyq6x9\n3hD0vdjlx+lAuojMx64OPNnVgIqiKMpJ4I2wxe+BIa6VgvWBM4FNXjivoiiKcgKUa6GLyBfYBFHh\nIrIXmwPED8AY85YxZpOI/IpdruvELnQoM8RRURRFqRrKFXRjTLkxtsaY57FZ/hRFUZQaQleKKoqi\n1BFU0BVFUeoIKuiKoih1BBV0RVGUOoIKuqIoSh1BBV1RFKWOoIKuKIpSR1BBVxRFqSOooCuKotQR\nap2gp2Tmsn5fClm5+TXdFUVRlFOKWifoC+ISGP/aQnYnZ9R0VxRFUU4pKl1T1FVPNMVVU3S1iDzu\n/W4WEuDwBSA711mVl1EURal1VCQf+hTgdaDUKkQuFhhjxnulR+UQ6GefQVl56nJRFEXxxBs1RauV\nQD9roasPXVEUpSje8qEPFJE1IvKLiHQtq5GI3CIisSISm5CQcFIXCnDYLqvLRVEUpSjeEPSVQGtj\nTA/gNWB6WQ2NMe8YY/oaY/pGRJRa47RcCix0dbkoiqIUodKCbow5ZoxJc/3+M+AnIuGV7lkZqIWu\nKIpSOpUWdBGJEhFx/d7fdc6kyp63LNRCVxRFKZ1K1xQFLgFuF5E8IBO43BhjqqrDaqEriqKUTqVr\nihpjXseGNVYLaqEriqKUTq1bKaoWuqIoSunUOkEXEfwdPmqhK4qiFKPWCTpAoMNHLXRFUZRi1EpB\nD/DzJVstdEVRlCLUSkEP9PMhSy10RVGUItRKQQ9wqIWuKIpSnFop6GqhK4qilKRWCrpa6IqiKCWp\nlYKuFrqiKEpJaqegq4WuKIpSglop6AFqoSuKopSg0jVFPdr1E5E8EbnEe90rnUCHr1YsUhRFKUZF\nLPQpwJjjNRARX+A54Dcv9KlcAvx8yM5TC11RFMUTb9UUvQv4BjjsjU6VR4Ba6IqiKCXwRoGL5sCF\nwJsVaFvpmqKgFrqiKEppeGNS9GXgQWNMuQrrjZqiYH3oOXlOnM4qq6OhKIpS6yi3wEUF6AtMdVWh\nCwfGiUieMabMYtGVxV3kIiffSaCPb1VdRlEUpVZRaUE3xrR1/y4iU4CfqlLMobDIRVZufoG4K4qi\nnO54o6ZoteMWcfWjK4qiFFLpmqLF2l5fqd5UEE8LXVEURbHUypWiBYWidbWooihKAbVS0AsKRWs+\nF0VRlAJqpaCrha4oilKSWiroaqEriqIUp1YKeoBDLXRFUZTi1EpBVwtdURSlJLVS0NVCVxRFKUmt\nFHS10BVFUUpSKwVdLXRFUZSS1E5B99OVooqiKMWpnYJesLBILXRFURQ3la4pKiLni8haEVntKl4x\nxPvdLHFNAhw+ZKuFriiKUoA3aorOAnoYY3oCNwDveaFf5RLo56sWuqIoigeVrilqjEkzxrhLBwUB\n1VJGKMDhoz50RVEUD7ziQxeRC0VkMzADa6WX1c4rNUVBLXRFUZTieEXQjTHfGWM6ARcA/z5OO6/U\nFAW10BVFUYrj1SgXl3smWkTCvXne0lALXVEUpSiVFnQRaS+uCtEi0hsIAJIqe97yUAtdURSlKN6o\nKXoxcK2I5AKZwGUek6RVRqCfLxk5eVV9GUVRlFpDpWuKGmOeA57zWo8qSKCfD0cy1OWiKIriplau\nFAWbz0VdLoqiKIXUXkH389FJUUVRFA9qr6A7fDXboqIoige1VtAD/Xw0H7qiKIoHtVbQAxy+ZKuF\nriiKUkCtFfRAPx9y8p04ndWSOkZRFOWUp9YKurtqkU6MKoqiWGqtoAdq1SJFUZQi1GJBVwtdURTF\nk1or6O4ydGqhK4qiWLxRgu4qVwm6dSKyWER6eL+bJVELXVEUpSjeKEG3AxhujOmGzYX+jhf6VS5q\noSuKohSlIsm55otIm+PsX+zxdinQovLdKh+10BVFUYribR/6jcAvZe30Zgk6tdAVRVGK4jVBF5ER\nWEF/sKw23ixBpxa6oihKUcp1uVQEEekOvAeMNcZUebUi0Dh0RVGU4nijBF0r4FvgGmPM1sp3qWK4\nV4qqoCuKoli8UYLucSAMmOwqLZpnjOlbVR12E+Cy0NXloiiKYvFGCbqbgJu81qMKoha6oihKUWrf\nStGdi+CTiwjMPACoha4oiuKm9gl61lHYPgv/rGREIFstdEVRFKA2CnpAMACSk0aAQ+uKKoqiuKm1\ngk52KqH1/EhKz6nZ/iiKopwi1EJBD7Gv2am0CQtiR2J6zfZHURTlFKEWCrrbQj9GdEQD4hPSarY/\niqIopwi1WNBTiQ4P4khGLkfU7aIoilILBd0RCD4OK+gRQQDEq9tFURSlFgq6iLXSs1OJjmgAoG4X\nRVEUaqOggxX0rGO0aFQPh4+oha4oikKtFfQQyE7Fz9eHVmH12ZGggq4oiuKNmqKdRGSJiGSLyF+9\n38VSCAiG7GMARIc3ID5RXS6KoijeqCmaDNwNvOCNDlUIlw8doF1EEDuTMsh3mmq7vKIoyqlIuYJu\njJmPFe2y9h82xvwJ5HqzY8fFQ9DbhgeRk+dk35HMaru8oijKqUi1+tC9VlPUQ9DdkS7b1e2iKMpp\nTrUKutdqihYRdBuLrhOjiqKc7tTeKJe8TMjPJSzIn+BAh06MKopy2lNLBb1w+b+IuHK6qIWuKMrp\nTaVriopIFBALhABOEbkX6GKMOVZlvfbIuEj9xrQLD2Lx9qQqu5yiKEptwBs1RQ8CLbzWo4rgYaED\ntItswLer9pGalUtwoF+1dkVRFOVUoda7XADaR9pIl22H1Y+uKMrpSy0VdA+XCxDTxAp8nAq6oiin\nMbVU0AuLXAC0alwff4cPcYdSa7BTiqIoNUstF3Qr4L4+QruIBmqhK4pyWlMnBB0gpkkD4g6poCuK\ncvpSOwXdPwiQIoLeIbIB+45mkpadV3P9UhRFqUFqp6CLFOREd9PBNTGqkS6Kopyu1E5BhyL5XMBa\n6IBOjCqKctpSywW9cDFqQaSLWuiKopym1BlBd/j6EB0epBa6oiinLbVc0IuKd0yTYLZqpIuiKKcp\n3qgpKiLyqohsE5G1ItLb+90shVIE3R3pkq6RLkpNsX0OzH66pnuhnKZ4o6boWKCD6+cW4M3Kd6sC\nlCboGumi1DSbfoBlb9V0L5TTlErXFAXOBz42lqVAQxFp6q0OlkmxsEWATlFW0NfvT6nyyytKqeRl\n2x9FqQG84UNvDuzxeL/Xta0EXqspCtZCz0kDZ37BptZh9WnesB7ztlTy3IpysuRlQX42GFPTPVFO\nQ2pnTVEoXP6fU+heERFGdIpg0bZEsvPyyzhQUaoQt3Wen1Oz/VBOS7wh6PuAlh7vW7i2VS2l5HMB\nGNExkvScfGJ3HqnyLihKCdyCnpdVs/1QTku8Ieg/ANe6ol0GACnGmANeOO/xKUPQB7YLw9/hw5zN\nh6u8C4pSAreQ56mFrlQ/FQlb/AJYAnQUkb0icqOI3CYit7ma/AzEA9uAd4E7qqy3nhQrcuGmvr+D\nAdFhzNmigq7UAGqhKzWIN2qKGuBOr/WoohQrcuHJiI4R/PPHjexOyqBVWP1q7phyWuMWcvWhKzVA\n7V4pCiUsdLB+dIC5W9VKV6oZtdCVGqROCnqb8CDahgepH12pfgp86BqLrlQ/dVLQAYa0D2f5jmRy\n853V2CnltKfAQldBV6qfOivog9qFkZ6Tz9q9umpUqUYKfOgq6Er1U3sF3ccX/IIgq3TBHhAdBsCS\n7YnV2SvldEctdKUGqb2CDhDcBFJLD3lvFORPl6YhLN6eVM2dUk5bjCm0zFXQlRqgdgt6aAtIKXtR\n6qB2YcTuOkJWrqYBUKoBZx4Y15yNhi0qNUDtFvSQFpCyt8zdA9uFkZPnZOVuTQOgVAOeoYoatqjU\nALVb0ENbQNpByM8tdXf/to3x9RGWqNtFqQ483SzqclFqgNov6MZZph89ONCPbs1D1Y+uVA9FLHQV\ndKX6qeWC7kq7Xo4ffc2eo6RpWTqlqvEUcQ1bVGqACgm6iIwRkS2uuqEPlbK/tYjMctUUnSsiLbzf\n1VIIdWXtPY4ffXD7cPKchmXxaqUrVYxa6EoNU5Fsi77AG9jaoV2AK0SkS7FmL2DL0HUH/gX8x9sd\nLZUQl4V+zCXoORnw1XWQtL2gSZ/WjQj082H+Vq1ipFQxKuhKDVMRC70/sM0YE2+MyQGmYuuIetIF\nmO36fU4p+6uGgAYQ2LDQQt+7HDZOh43fFzQJ9PNlQHQYC+J0gZFSxeikqFLDVETQK1IzdA1wkev3\nC4FgEQkrfiKv1hR14xmLfmCtfT24rkiTYR0iiE9MZ09yhneuqSiloT50pYbx1qToX4HhIrIKGI4t\nQVdiNY9Xa4q6CfWIRT9YhqDH2GvNj1O3i1KFFLHQNQ5dqX4qIujl1gw1xuw3xlxkjOkFPOradtRr\nvTweIc0hxTWAcFvoSdsgJ72gSbuIIJqFBpbwo6dl5/H3aWvYe0Qtd8ULFPGh60pRpfqpiKD/CXQQ\nkbYi4g9cjq0jWoCIhIuI+1wPAx94t5vHIbQFZB2FtARIioOoboCBw5s8+8ewmAgWb0sqkk73yz/3\n8FXsXr6KLTtKRlEqjNtC9/VXC12pEcoVdGNMHvB/wExgE/CVMWaDiPxLRCa6mp0FbBGRrUAT4Okq\n6m9JQl0RknG/2UVGPa+y793uFxfDYiJIzc5jzR47cMh3GqYs3gHAPK0/qngDt4gHhGguF6VGKLem\nKIAx5mdsMWjPbY97/D4NmObdrlUQt6BvcXWv4ziY858SfvTB7cLxEfhi+R76tG7E7xsPsSc5k+4t\nQlm7L4WktGzCGgRUc+eVOoXbQg8MVQtdqRFq90pRKIxF3z7bhjA2bGXdLsUEPbS+HzcPi+ablXt5\nf+EOPli0g+YN6/HkxK4Yg4Y1KpXHLeKBIepDV2qEOiDozQCB3Axo2h1ErKAf2gDOooE2D57biXHd\nonhqxiaW70jm+kFt6NmiIWFB/sxVt4tSWdRCV2qY2i/ovn4QHGV/j+ruej3DCnxyfJGmPj7C/yb1\npE/rRgQHOpjUryU+PnbCdH5cIk6nqebOK3WKvCwQVyUt9aErNUDtF3Qo9KM37WFfo7rZ12JuF7Ar\nR7+4eQCz7h9OaD0/AM7qGEFyeg5r92n9UaUS5GeDIxAcAWqhKzVC3RJ0t4Ue0Ql8HKUKOoC/w4fI\nkMCC90M7RCAC87bowiOlEuRlg8PfJehqoSvVT90Q9IhOUK8xhLW37x0BdtuB1RU6vHGQP31aNeLD\nxTsKwhoV5YTJy1ILXalR6oagD74X7lgKvh5RmG2Gws5FkJ1aoVO8OKkHwYEOrnx3qVY4Uk6OvGwr\n5r4BmstFqRHqhqD7BUJwk6LbOo+3X6ptf1ToFK3Dgvj61kE0a1iP6z9czoGUzBJtnE7DT2v3a9Fp\npXSKWOgq6Er1UzcEvTRaDYT64bDpxwofEhUayDvX9iU7z8lPa0qWtftt40H+7/NVvDM/vpSjldMe\nt4XuFnSjUVNK9VJ3Bd3HFzqOha2/nZC11DY8iG7NQ/lx7f4S+z5avAuAKYt3kpmjVrpSDE8LHQNO\nLXuoVC/eKkHXSkTmiMgqVxm6cd7v6knQeSLkpMKO+Sd02IQeTVm7N4VdSYUZG+MOpbIkPolRnSNJ\nTs/hq9g9xzmDclri6UMHnRhVqh1vlaD7BzZpVy9sNsbJ3u7oSRE9HPyDT8jtAnBe92YA/LS20O3y\n8ZJd+Dt8eO7i7vRp3Yh3F8ST55G5UVEKLXRXSKyGLirVjLdK0BkgxPV7KFDSX1ETOAKgwzmweUaJ\nNADHo3nDevRp3Ygf19iPkZqVy7cr9zK+e1PCGgRw2/B27D2SyYx1Jf3symlMXrZNnevwd71XC12p\nXrxVgu5J4GoR2YvNynhXaSeqkhJ05dHpPMhIhP0Vi0l3M6F7UzYfTOWL5bt56Jt1pOfkc93ANgCM\n7BRJu4ggPlmyqwo6rNRa8lwrRd0uFw1dVKoZb02KXgFMMca0AMYBn3gUvCigSkrQlUebofZ116IT\nOmxc96b4CDz87Tr+2HSIqwe0okfLhoDNCTOhRzNW7D5CYpp+aRUXeR5L/93vFaUaqUg+9HJL0AE3\nAmMAjDFLRCQQCAdqPoVhcBNo3A52L4HBd1f4sMjgQN6/vh++IvRv25hAP98i+0d1bsLLf8Qxe/Nh\nJvVtWcZZlNOKvKzCsEVQQVeqHa+UoAN2AyMBRKQzEAicOolRWg+CXYvBeWKTmCM6RjIsJqKEmAN0\nbRZC09BA/th4yFu9VGo7aqErNYy3StA9ANwsImuAL4DrjTmFVlW0HmzrjiZsKr9tBRERRnVuwoK4\nxBIrRxfEJfDWvO2Uegv2r4ZPL4ZcnTCrc7gtdPWhKzWEt0rQbQQGe7drXqT1QPu6azE06Vq43ZkP\n8XOg7XCbV/0EGdWlCZ8s3cXi7Ymc3cmmHlgWn8SNH8WSk+ckO9fJPaM6FD1o+yybjiBhMzTrebKf\nSDnVcOaDM7dY2KIKulK91N2Vop40bG1L1e1aXLgtJwO+utZay0vLCZtf+xVMu6HEUu4B0Y1pEODg\nd5fbZeuhVG7+OJaWjeoxsUczXvpjK9+t2lv0XMdcoY5J2yr7qZRTCbd4OwI8whZV0JXqpUIWeq1H\nxPrRdyywopyeCF9cDvtWQHBTiP0QBt4FPmU831Z+DDsXwJD7bTUkFwEOX4bHRPDL+oPsP5pF7M5k\nggIcfHRDfyKDAzmcmsWD09bROCiA4TGuqJ5Ut6Bvr+IPrVQr7pjzIha6utWU6uX0sNDBCnraQVgz\nFd4eZmuOXvYpjH4KjuyAHXNLPy4vB/b+aX/f8G2J3Rf0ak5qVh4HU7KY0KMZn988gBaN6uPv8OHt\nq/vSPrIBN38Uy8wNB+0Bx1xrrpLivP8ZlZqjwEL3t4uLQMvQKdXO6WGhg50YBZh+GzRqCzfOtCXr\n8rKhfhjEfgDtzi553IHV1tLyC4IN38HZj1mL38U5XZqw9amx+PpIiUND6/vxxc0DuH7Kcu74bCX3\njOzAHUf32Zvu4XIxxpCT7yTAUTKaRqkluCdA1UJXapDTx0IPj4GmPaHLBXDrvML6o44A6HU1bP65\n0L/tidvvPvR+W3T6wJoSTUqI+cF1sP4bwIr6JzeeydAO4bzy+yYk3YbmOxO3gTHk5Tu5+eNYxr68\noNw86xk5eVrI+lSliA/dHbaoFrpSvZw+gi5ihXzSRxAYWnRfn+vB5FtfeXF2L4GwDtD3BluntBS3\nSwkWvgQ/3F0widogwMGUv/Rnwe1d8BVDHC3xyUkl4dAenvxxA39sOkx8YjqfL9td5ilTs3IZ+J/Z\nDHluNi/M3MK+oyULcCg1SBEfumZbrLMsfg3mv1DTvSiT00fQj0fjaJsioLhYO52we6kNe6zfGKLP\nsm6X8kLsE+MgJw2yjxXZ3MzX1isN7TwSgIff/Y5Pl+7m1uHRDGoXxuS528jIKT2H9uzNh0nJzCUq\nNJDJc7dxxTtLS49zV2oGTwu9IA5dLfQ6x7qvrcF2iq4jUUF302G0jQ0/5pEoMmGTXZDUyhXH3vUi\nOLoblr1ddvZGYwojWIq7cI7ZjAmR3UfZ15y9jD0jigfP7cQDo2NITMthyuKdJKfn8PrsOGZtKlyF\n+su6g0QGBzDttkH8+4Iz2J2cwfaENK98dMULeFrovg4QX7XQ6yJpCdZYO8EaC9WFCrob94To9jmF\n29z+c7egdzkfWg2CXx+ENwfbakjFreTUA5DrKoxxrFjKG7fAtzwTfP15fGAAb1zZGx8foU/rxozo\nGMEbs7cx+NnZvPDbVv769RrSs/NIz85jzpbDjD0jCh8fYWh7GwK5csNmmDIeUorFuivVT57HpCho\nXdG6iDGQ7sposvmnmu1LGaigu2nSFYIi7cpRN7uX2jj1Rm3s+4AG8JefYdLHNqrh80vh44lFJ0oT\nPcIRjxVLC5+63w7HgyKgcZtwXGQAACAASURBVDSBKfH4eEyo/vXcjvg5fDi3axNevLQHRzJy+XjJ\nLuZuSSA7z8mYM5oC0CqsPi0a1SN3/Q82Pr4Ma2HLwVQtaF1dFFjoAYWvKuh1i6wUuxpYfGDLzydU\nY6G6UEF3IwLtRlgL3emE/DybcrfVwCJhiohYS/2OZTDmOTi4Ht47x2MF6HEE/dgBCI6y5whrX2K1\naNdmoax+fDQvX96Li/u0YGiHcN5dEM83K/cSFuRP/7aNC9oObhdOk6Sl9k3i1hIfJ+5QKmNfmc9z\nv26u1G1RKohbvN3+c98AzeVS10hPtK/tz7GW+t7Ymu1PKXirpuhLIrLa9bNVRI56v6vVQLuzbTGM\nQ+tgxYfWfXLGxaW3dfjDgNvg6mn2i+vOt560Hfzq29j21OKCvh9CbHk7wtrbMMjjPOXvHdWB5PQc\nZm8+zLlnRBUJjxzcvhH9zHr7JjGOoxk5RQpXvzp7G04Dny/bzeFj6sutckpY6P4atljXcIUc0+sq\n8PE7Jd0uXqkpaoy5zxjT0xjTE3gNqEBs3ylI9Fn2dd00mP0UtB1mKx4dj6geVsD3LLfvE+MgrJ3N\nHVOayyXYuk0Ia2+Hb0fLDlXs07oxQzuEAzDO5W5xM6zBfkIlgzyfAPIOb2XU/+Yz8fWFpGTksvVQ\nKj+t3c+EHs3IcxremR9fcFx1R8ZsO5zGewvi635ETgkfeqBOitY13P7zxu2sNmz+qfyIt2rGWzVF\nPbkCm0K39hEcBZFdYfGrkJ1qXSpScgVoEXwd0LwP7HG5P5LibNx6cUE3xrpcPC10KDeny+Pju3DD\n4LYMiG5s3UAuGh60E7YLA4ZBcjxZ2dnsSsrg5k9iefG3LdT38+WfE7tyfs9mfLpsFwdSMnlh5hZ6\n/ut3YncmF5xnV1I678zfTk5eBXPFb/kV3h8N+bkVaj557jaemrGJmRvqeN54z7BFcLlc1EKvU7gF\nPSgCOo61I+wjO2u0S8XxVk1RAESkNdAWmF3G/uqvKXqitBthX/vdCE26HL+tm1YDrC89I9la3GHt\nIaRpUUHPOgp5mSUFvRT/tycdmgTz+IQuOBI2wgsdYM4zdkf8PA4HRvPTsWgc5PHyuQ15/tLuLN+R\nzMwNh7huUBsaB/lz54j25OQ5GfXiPF6fs43svHz+/dNGjDE4nYa7p67mmZ83c+fnKysm6tv+gD3L\nKpRcLN9pmLPZDlOf/WVTxR8atRHPsEVwTYqqhV6nSEsAxLpTw11psYtHstUw3p4UvRyYZowp1TFc\nIzVFT5SeV0HHcXDWwxU/puUAu9J03TQwTvvHDmkGmcmQ61rR6RZ3t8slKNxGz8T9Vv75U/bBZ5fa\nh8K8/0LcH7B7KdmthrDdaR8QoyKOcX7P5vxrbFsuDtvJzUOjAWgX0YBJfVvSINDB+9f15d/nn8Ga\nvSnMWHeAqX/uYc2eo4zpGsXvGw9xx2crWL3nKOv2ppCaVYYFnuwS8kPry+32yt1HOJKRyyV9WrAz\nKYPPltXhotrFLXRHwOnrQ//kIpjzn5ruhfdJT7ALDH0d0CDKbks9WLN9Koa3aoq6uRy4s7KdqlGa\ndIErTtBj1LIfILDqE/s+rD04Xe6RY/utT90dBeO20EWg26Ww4EVIPWRrn5ZG1jH4fJJ1Ad0wE6bf\nDl9eDXmZNO81hru7dIDpT1jffcexXOv8HtKfhWNDIag7AM9c2A2wxa3znYb3F+7g2V82k5qVx5lt\nG/Pm1b35dOkuHvvepiEAiAoJZMbdQwhrEFC0P8nWH5+4fSWfH+5BTJNgRnSKKDWx2B8bD+HnKzwx\noQsHU7J4ZVYcF/VqQWj9osVE8vKdvPxHHAOiwxjimjOodeRlWTeL20XnCLB/u9MNp9OG0uZmwogT\nMIpqA+kJ1t0Chd/XU0zQvVVTFBHpBDQClni3i7WAwFCI7AIH19r3Ye0Lhdud/zy1mIUOVtCN06YT\nKI2sFFuA4/AmmDQFWvaHi98riIX1aTuEET1j7D+Z23WzxVVYavk7BafxST+MT5Ld7+sjPDS2E3uP\nZJKencdTF5yBiHDNwDb8fPdQPri+L/+b1IPkjBwe+HpNkWRgmZmZOI/YSdzVKxbzv9+3ctunKzjz\nmVm8MHMLeflFXSq/bzrEgOgwggP9eGRcZ45l5nLjR3+SklFo/ec7Dfd/tYbX52zj2V+9VyKw2nHX\nE3VTE2GL6Umw6tOanahLO2jnDpLjy29b2/AU9MCG9m+cVssEvYI1RcEK/dRTqpZoddKyv31t0AQC\nQ+ykKBS6WtwWuqegR3SEqG6w7quS58tIho8mwv5VcOkUaG/TBdCsF4x/GQbdVZhkLKyDjWk/tt8+\nVAJCbc6JjGQrNB9PtMNgF8NjIrh+UBsen9CFDk2C7cbMo3T5rA9nZ8/lot4t+Md5nZm7JYF3FsSz\nLD6J//yyiSue/wofnOTgx4Cgg/z56Cg+/Es/BkaH8fqcbVz7wXKS062bIT4hjfiEdEZ2igSgS7MQ\nPpzQiI17k7j07cWs3XuU1XuO8rev1/DDmv30aNmQ9fuOEX+cdAbbE9K4+r1lBX75U4q8rMJKRVAz\nC4v+fBe+v7PQsKgJjrjcamkHIbuOpaZIT7CuUrAjseAoO7o+hfBKTVHX+ye9161aSKsBNnY9zDVZ\n4hZu96TJsX326e75pQdrpf/+uJ1kDGtntxlj3SqHN9oiHB3HFD2m9zVF34e3h80zYOtM+37CS7Zk\n3sqPrZWf4Fpc5IqDFxGenNi16Dk2fm/jbFd+BD0u45oBrVkYl8izv9hjfX2Eu1seg0PgFzMS/62/\n0sAvixEdIxnRMZJvVuzl4e/WMeG1hdwzqgMJqVbMRnZ2DU2TtjP89/H8OOSfnL80homvLyq49H2j\nYrisX0sGPjuLH9ccKFmH1cU78+JZuC2RhdsSublLPneeN5CGYafIXExxC70mBH3PMvsa93theujq\n5qjHPElyPDTtXjP9qArSE+xqcjfBUbXPQlcqSMsz7atblAMaWEvZbZmnHihqnbs542JACvKnAzaH\nzK5FMPrpkmJeGuExkJEEa76w9VO7XmSzRy5+DRa9Yq16gH0ryz7H2q8Kr33sACLC85f04O6RHXjr\n6j6sevwc7ulln//S2TUwO7Sx4PCL+7Rg2m0DCQ508Pdpa3l+5hY6RQXTsnF922DDd2DyaZezhRl3\nD+GVy3vy+cWRbGj7Knf3DyIqNJD+bRrzw5p9pcasp2Tm8sOa/VzcuwX3nt2WO7ffxrp3biQ3v/TI\nmaMZOXyydBcPTlvL0QzvTE7uSc4oO54+L6twQhSqX9Cd+YUrF+N+r77rFudIMUGvK+RlW+MoyMOA\naNCkVvrQlYrQqA30uwm6TyrcFtLMWubOfBvW2Kh1yeNCW9hqSqs+KRyiLnrFhkYVt8TLwj0q2LMM\nYsbY4WD/W+yq1wZN4MqvbC73fStKP/7oHti10I4WMLDJTpGE1vfj/nNiGHNGFCGBfnYUERBiF1UA\nHN5Q5DTdm4fyy6UN+OymMxnXLYrbz2pXuHPDdPt6cB2tw4I4v2dzBuUtJ+jAUiT2QwAm9GjG9oR0\nNh9MxRjDmj1Hyc6zAVPTV+0jMzef6we14d7OaTSUdPpnLeKtX/4s8XFemLmFfk//wWPT1/Nl7B7u\n+mIV+ZUsDPLGnG0M/e+cIou0ilDTPvSEzTZdc+No2LscMo9U37U9Obqr0BWYXIfq5rqX/Qd5TNqf\ngi4XFXRvIQLnvQhthhRuC2lm3Rxxv9lJ0W6TSj/2rAetqM54wE6Axs2E/reCX72KXTvcw0URc659\n7TjOivqlH0KDSJt8rCxBX/e1fR3xqF1Ytb6Mhb7J8VYwQlvY0cehooLOqk+Rd0cw2LGFyVf14fye\nrnmExG02nUJgQ/v53Auk3COGlR9Dfi7jujXF10d4f+EO/jLlT85/YxE3ToklMyefz5btokeLULq1\nCIX4uQAESB7JSz9hYVxiQRd+XLOf1+ds49yuUcy4ewjPXtSNBXGJPD9zS7m38WhGDl/F7uFwatH4\n8VdnxfH8zC2EBDp4ZVYc+0srLpKfXb0WeuYR61ZzZ9p0u1uGP2Qn2reXuhSk6jmyCyI6WUOiLlno\n7kVFDTxcLg2aQHZKYWjyKYAKelUS0tS6WmI/sHGrHceW3q7tMDjrIVg7FaZeZVMJ9L+54tdp2Nrm\nlvBvUPhA8XXAuOetbx/satb9q21YmSfGwNovbSx947bQ9UK76jWllMjU5O1W0EXsA8LD5YIz3yb+\nB9gxr+hxG11RPIPvtsLnTkq2fyXUD7d+yC2/0DjInyHtw5m2Yi/LdyRzRf+WLNqeyAVvLGLroTSu\nPLOVPS5+LjTtQX7TXlzjP587P1vBV3/uYWdiOg9/u45erRry0mU96doslMv7t+KqM1vx1rztfLR4\nZ5kukx2J6Vw4eTF/n7aWQf+ZzW2frOBvX6/h/NcX8r/ftzKpZzgLes6isTOZp2cUjcYxxrA/8Sip\n+R6hm8cR9KzcfN5bEM/WQ6ml7q8QW3+zbrolb/DY9PVsWj7LugPOuBjqNbJrFWqCo7vs/2PjaJJ2\nb+Kc/82rGxk/Cyx0D5dL8KkXi66CXpWENLd/7Ljfoc914OtXdtthf4O2w61o9rrGLmCoKL4O6yfv\ndF5RK9GTZr2tNVF8GHxwnR2uu11FXS+wrxu/L9ouL8e1CtblRmnSxVroboHc+L09t6Me7FxY9NgN\n0+0cQwfX6OHQemthJsfDmbdBSAv70APuHtmey/u15Lf7hvGfi7rzwiU92Ho4leAABxN6NIOcdJs3\nJ/osfHtfQ7RzF2MaH2DWd+8R/Fon5stNfGn+jt/ytwou/8SErgyPieCJHzbw4LvTObL8i4J+p2fn\nMWPtAS6cvIijGTm8fmUv/jK4DX/uTGbOlgTq+zt44JwY/tPtAKGr3+bZ9huZse5AkVHBrE2HOZh8\nlLUHs9l22Ip0htMBJp/DKUUjPbYdTuXCyYt5asYm/vLhnyfv399tUz/krviUr5duJfBgLEfDetr/\nhXYjYdvvJR/eVU1+rnUxNmoNjdvhe2QHcYfTmLe18qvCdyWlc/2HyzmSXkOLtdyJuYq7XADSTh23\nS4WiXJSTJKQZYGz+5N7XHr+tj6+NMV/wIgy5/8Svde10WyWnLJr3sa/7VhR10az/xvrXu15o34d3\ngCbdbDxzv5sKo3KO7rZD+cZ2BSpNukJOqt3esBUs+J+dnO0w2sbA52Zal1HCVivgY561+3387EPE\n/cBq0RcwMOdpSI6nT+to+rQufJhd3KcF4cEBOJ2G+v4OiJtr4/DbDrfHznyUZ3kF8Y9ni297glr3\noXHKSpj7rB3l+Prh7/Dhw+v78fXcWIbNm0Sj/cl8+uvPfBF6M1sPp5Gbb4iOCOLD6/vROiyI8d2b\n8eh5xdI+fPs8AAMdW2gdNpK/T1vDTxcF0HDuo0xOuY/nHPmkGj9umBLLI+M6Ebd0P3cBd32yjI9v\nO4sAhy8/rzvA/V+tpr6/g0fGdeL5mVv469dreffaPiSn57B22SwGdm1PYGQ7yEph98qZbFmzlKGd\nmhJYL8jOcbhFZNcSTIMo/NIOcnfwHNrmHuSDhDFc5zT4djgH1k+DnfOh9RAr8tVByl77P9KwNTkS\nQENnMvXJ4ud1Bzi3a1TZx6UngX/947oYP1i4g7lbEpi9+TAX92lRBZ0vh4I8Lp4uF7eFXkpx+RpC\nBb0qcceix4yxfufyaBAJY587uWv5Bx1/f0RH8Auygt7jcrvNGNj0o42I8RwRDP8bfHUtzHwEznMV\nxHX7Qxu7LPRIV9jjsrchKMz6yM+fbC2YJa/D3j+tK8ntn+880T4cIjpZgQ9wxb8362m3zX0Wvr0F\nRj5u++ORFG14jMcwN34O+PrbPPX+9aHrBciaL6DPX+g49jk7Qtk8A6ZeaSN2oocD4JOfzWXbH8Lp\nl8nWRudydcIPROYKsYMeZFhME/q1bVTqalfAWp6ukFDfPct4/aq3mfTucv787l3OzVrN6LxpNGss\nhDduwqFtWdz26UoeCAmAfNiyN4HHpq+nd6tGPPLdOnq1asSbV/UmMiQQXx8f/v3TRq56bxkNd//O\nZN8XYAHgG4DJz6EVhlYAbgM3YTOc/4Yd/iduYV2ne6i3cRq3Om2E0s9HW+G7dBfX9RhlH+4fn49B\ncHYYg+9VU0t8rLTsPFbuOkJEcAAtGtUjOLBwBOl0GlbtOUKPFg1x+JYykN+7wo7SPEXYHbLYqDWb\nDufRAxjSOIVZmw6TlZtPoF8p9zc9iexX+yJ+9fGf9H6hi9CDrNx8vltlXYBL45NqTtAd9Yp+zwpc\nLqeOha4ul6oksrONChl4CmRD8PG1bhnP0MWEzdZN0nlC0bZdzoeB/2cXqqxxCYHbVeNpodcPh6Vv\nwKx/QWgr67ZpNcCOSHYutIV0Yz9wPdBcD7eoM2zEz/5V9lz1Gtm5hgmv2Am1jybAh2PLTv61Y551\n3/i7wiHH/AeunwETXi50N0WfZaNMtv5aeNxP98G+WHwueoeYO76Egf/H6LTveSQqliEdwouKeeoh\n+1Bwf1F3LbZ5dDpPhOwUuvnt5aVJPeiWaScir/P7g6DcozQKCeata/pw6/Bobjm7MwC3DWmB/6oP\nWfn9qwztEMGnN55JZIiNhrlhcBvGdI1i5a4kHg/6lgT/Fvwt9xb2xVzDira3cnH2E9zV4XfaZ33M\nxohxsPEHe09328XYr2+LZG7weHzzszA+fgRH9+PZXzbz6G8HWD16Kr9HP8QCeuMb9wvvzfyzwJdt\njuzkj9gNjHxxLtd+sJyxryyg25O/8cacwoIrb8zZxsVvLuHaD5aTmFZsLiA9Ed4fBYteLbrdHbLY\nsDXzkkIAuKunD2nZeSzwcFF5kvnzI/hkpZCckYP5cCzMf75Em5/XHeBYVh7NQgNZuiOp1PNUOWmu\nVaKe2VfrNbaj2+Kx6McOwEvdbMWzakYFvSoJbQEP7S4a+VKTNO9lVxG6k0Zt+hGQ0nO+j/qntZR/\nvMe2S463Dye3DzGgATywBe7fDLfMg5t+t3MEgaF2UcvOhXYFbEYiDLij8LxR3ewXYMcC69d30/sa\nuHcdjHvBRsK8PQxWf1F0GXt6onXXuKxuwD4Qit9f/yAr6ptn2OP3/AlrPoehf4UuE+2XcvRT0LSn\njdV3+5oPrIHX+sKLMdbC/+Jya51v+dmGJJ79mG23azFjIhJpJsl8zHjqmUybiM0RwIiOkTw8tjMB\nAdZyvaVfGP8ImMoz/lN4d3wY9fwLHxwiwhtX9WbNxWlEZcUTOvYJYhudx6Qd53HNthE06jSUV6/s\nx/m92/D0vh42LHHrr7BrCbniz7z0FvSZeAf41Uea9uCZSf0Y1aUJ36zcywXf53LLpu4sampdfX/O\n+4mh/53DuJfnc/DVcwj4/hbCggJ479q+vH5lL0Z3acLzM7cwc8NBlu9I5qU/ttKrZSixu44w4bWF\nrN+XUnh/D22wrhWPxHK5+U52bt+EEV9MSDO+3WlddV0CEgmt58cv6w6QlZvPvVNXcesnsTalxM5F\n1NswlffyxzEq4xkSWo6xdQj2ryry55y6fA9twupz87Bo9iRnsvdIRsn/1yokOy+fXXt2kVMvrOgO\nHx9XLHoxC33j95CyG1Z/Vn2ddHep2q94ulFePvXqpHkfm2djp6sG6aYfbMqC4FL8m74OuORDm6Pm\ny6utuDZuW/Tz+Dqsdd2sZ9FztBliXS6LX4MmZxTGrYN9D3aCtrmHoAP4BVq/9+2L7ENh+m2w+vPC\n/Ztn2Nfos8v/rB3HWhdAwmaY96yN6x9yX+F+EZs+ISnOhonm58H0O61ojn4Kzn3GRuHMf95et93Z\nEBFjRyK7FhW4YK687wXoNN6es/hKUcBnywwCnZk4TB7+s12Lq7NS4NtbYc4z+KYfImDBcxDZFf8e\nl/DfS7qzPyUTg+GJCV0REZ6+8Ayymg/ikGnI4UUfc2TzPFbkt+OmszrRu2MbuOBNGPUETUPr8doV\nvYj9xzm8fU0f5jxwFg/feCX41efJ7kfp27oRZ9bfR1NzmCG+G/jhmtaM6tKE8d2b8eoVvejRIpT7\nv1zNXV+sZEToQb5NmcTMifkIcNunKworYh12RfnsW2HTSwCvzd7GmnVrSHJEsv5ABjtTfcgMCMf3\nyA5Gd2nC7xsPccOUP5m+ej8zNxziuxU7cP50L/uIZEWbWwgObcxDOTeBfzAseaPgNm5PSGP5zmQu\n69eKge2soC6LL8znXx18tnQ3qUkHWHfEvyC3Ub7TsH5fCjn1IjDFolwy1tmAgtyNM6q97qgK+ulE\n+1HQqC18/3/Waj24rlCMSqNBhM3wOOguOwEa0ali12k9xD44Erda69zzIRDVrfD3Zr1LHgt2ZHPd\nj9aCXvCC/VI4nfaLHtWt5IOgNGJcK2xnP2VzuA+6244qPOlyAYS2tA+e5e/YeYBxz9vPO/BO6HEF\nzHsOUvYUjmJaD7IumK0zoVkvHKFNYdhf7T6/koLOmqnWNXXWw7bCzbppMOU8O7cw7zn4X2frzjr7\nUfDxoV+bxjx/SQ9evbxXwSrbQD9fPrxxIEvqjaDhvrkEH9nI3uCe3Dcqxl6j6wVFHpoNAhyc2zWK\nNuFBdt6i5Zk0PRLLm1f34YmOtrSBYHBsmFZwTKCfL29f05f6AQ6S03N4tt16JCedtose5n8Xtmfv\nkUwmz7UumdTda3EigIHts9l6KJU3526jvV8SW7Iac9undr2Db3h7SN7OuG5NSc3OY9mOZP43qQd9\nWjdi1c8f4pO4lcdzruH64V24cUhbZu/MIqHDJNjwHeboHtbvS+HpGZtw+AgX92lOTGQwjer7sTTe\nul0+X7abS95cXGZYZFp2XqnbT4SMnDwmz91GpM8xtqXX46MlO8nKzef2T1cw/rWFzN3vS9z2OB74\nag2Hj2WxJX4X/vuWst3ZFL+sJHavK72Ae1XhlZqirjaTRGSjiGwQkc9La6PUMAHBNjdM5lH4xBWe\n2Pk4gg5WEEY/BbcthHP+XbHruP3oQZHQ7ZKi++o3huBmdv/x8nz4+FqLOjnejiS2/Q6JW6wwV2TU\nE9LUzhls/sla5/1uKtnG12EfOLsWwax/2gdeZ498c2P/ayN4xKfwAdF6kHUj7V1eGIbZrBdc9C70\nvq7wWLe1nhRn3TyD77bW/Tc32vmBq76CO5dDr6sLc/C7uKRPC0YXiwoJCfRj5GV34S/5OMTJiNHn\nlz5ZWRptBtuJ6Ixk67Jp3sfOQ6z9sohLKyo0kGm3DWTqzf2J2P2LfYCn7GFA/Otc2Ks5b83bzuzN\nh9i56U9inR05aoLYuHA6D3+7jgYBDjoHJhMUFc2+o5l0bxGKf0R7SIxjcFO4vF9L3r22Dxf1bsG/\nzu/K0LzFHDCN2RM2lMHtw7i8fyuCAx08dmgoTqeTryY/xvjXFrIwLpE7R7QnMjgQHx/hzLZhLN2R\nxI7EdP754wZidx3h69g9RT5ubr6T1774gS//fTX//GJOkTkAp9OwZHsSU779gUWbS08zQU5GgWX9\n4aKdJKZlEyHHCGocxXO/bubq95bx28ZD3DuqA23aRNPccYwf1+xnxAtz+fjjt3HgJHPUs+Thy/wf\npnCoWE3fjAVvkr37OGk4KoFXaoqKSAfgYWCwMaYrcG8V9FXxBlFn2AnEnDTr/nBPcpZ7XLeyc7YX\np15DK5Sj/116XHzL/lYEy4vM6TzBRtUsfNlOwIU0LwyvrAhukSzNOnfT+xq76tUYa517PiwCQ+CK\nL+HCtwvnDloPLtwfM7rw9+6TCmP0wUbiuOl6oY0GOe9FG7p57ff24RHRESa+BhdMrtBDKrhNb0x4\nR4z4EtbpBOZl2gy1rxu+tW6SmLE20ilhs5038KB1WBB92GJD8Yb9za42Xv4Oj3dPIdDPlxum/Em0\n2U27bgPYEdKfsIMLWLErmSfHtMUnI4EeZ/TgH+d15qExnezEeUYi/i914NndV3F2PTvR3TXMh5GO\ntfya34/rhkQjIjQIcPCXwW35da8/v3Mm43N/4z/j27L80ZHcd05MQf8GRDdmT3Imt3+6An+HD52b\nhvDWvPiCnD5JadlMfu1Zbtx8Ezc6fuH6zbdyzQtfct0Hy7ny3aUMfm42U99/gevXXoPzs0lc9vqs\nonHyORnwWh/45UFSMnN5e952xneoj5g8hvXqgp+vD6v2HOWly3pw76gYYtp1ICg/hd/uHsDAduGM\ndawgL6gpZwyZSFbzQQzKW87Fby5myfYkjDH8vHQdAbMeYcXMjyv+9zsBKhK2WFBTFEBE3DVFPZYJ\ncjPwhjHmCIAx5hTMb6oU0ONy68f1FCBvc+7TZe+b+FphAZDj4eNrLdsf77HvRz91/MVZxel9nf2c\nx1t1GxAMF75pLbLSHm5NuhQtRRjWrjAWuWmvss/rttCDIgsfAjGjiz4EThQRZNQTcGBtYdhnRWjW\n24bczXWFxHZ0hdH+8qC10pv1LNp+/Te2fcwY+7P5JxrFvsw/J77G17MWE5SeRVDbHjRq1xefH+Zw\nX7dcJra2Oe6lURtu6u66j9G3WgPiwBpY+hb88ne4dQHE/YbD5BA18HLO9ghBvGdkB64+sxURKZHI\n+6O4wmcW1C+6HmCAy4+++WAqz17UjYjgAG78KJYfVu1lqN9m1v3wCvfkLyQxrDf1R/+NFt/dztS8\nx3nq2GPsCOjMOZFpPJ4/hfzgaAYf2UjokX9w1QcPMLZvDI+N70Lwmk8gdT8m9n2e3t2XY1nBPNRu\nD+yBkKh2fHbTmeQ5Db1bNbIdchk5bQLSeO+KLvDfNdDlKvDxoUGP82mw769Es48r3s2kU1QwPRO+\nZ5yfk6YDL6/43+8EqIigl1ZT9MxibWIARGQR4As8aYz5tVgbROQW4BaAVq1anUx/FW9x5q01d+3A\nkIq37XGFLWeWm1HUpVERgpsc/8HiprQon7IQsZV4fBw2yqEs3AuyupxvH0zeotN5J9Zfd19anWlT\nJoS0sCMzEZv3Z93XXUohvAAACktJREFUcM6/Ch+U+Xk2SqPjmMJRzRkXwdK3uOjSEC5qEAqfA5Fd\n8Wlov8P3BM6AqU+63GgeaXt9fK1vv+0w6/aafrudYN4wHRo0YezYovfG10dsSGdIPzuqWPSqdZV5\nxLrHRAYTGRxA+8gGXNbPFlIbGplJ/xnnEmn20596HOx2G1EX2Ie/702/EfrpxTx/9H7oebWN8vIL\ngOt/gr3L6fbNTfwS8SqjVjzAvE0HmMELZATE0DBrL+MPvsWAsf+lxeLbbWqMjmPpXvxv6V5clHbI\nzknlZRb+fTqOhZ//yvsDEnjmWG9+WL2fu6I2YEw0bbsWl1Dv4K1JUQfQATgLuAJ4V0QaFm9UK2qK\nKqcWjgCbYOzSD0/sQVCV9L2h/JW/jaOtCPS9oXr6VB7u0M6YcwvdO72usQtmNv1Y2G7nfDtH0LWw\nIAoxY+3q3O2zbY5+gMhOdp6iyRmFi8eu/9lGApVGt0nWfTbnaZsKo/PE4z/oznrILrdfMcW+d+bD\nwXX4OHOZfudg3r22LyKC5GbymrxAqPMI//C9l/03rSHq4ucKH1ARHeH2xdbttvZLK+gXvGnXRXS9\nELnoXVqkrmVht1+5I2wl4fmHeSnvEuZEXsswWcVF6++yE/wXTC69v2435MG18MeTNja9teteh7aA\nZr3wW/s5T4yLYcUDfWievBzpcn6VRb95q6boXmCZMSYX2CEiW7ECXzK3qaKcKK0H1XQPTpzAULhx\nZk33opD258Dsp4vOQbQfZdM+L3/HWuEAKz+xSd46nFPYruWZNt5/yy82/jykRWGK3BGPWr/8kPvK\nnqcAOwE9/O/wnWtk2OX84/e3zRArjAtftgnHpt9uo5WCImjW62rrCoroCDMeIDRlM7N6v8otQy6h\nVVj9kucKDLHzOX2uhyM7Cqt/gf3ch9YTvuBFrgsIgSZn8NJtf7eJ1V7/2c4zjP1v2e5Jd42Dn/9u\nR21Xf1O0iM3wB+16huXv2n6Y/PI/eyWQ8irGiYgD2AqMxAr5n8CVxpgNHm3GAFcYY64TkXBgFdDT\nGFPmsq6+ffua2NhYL3wERVEqRHqSTdPgyeLX4bdH4db5dr7howl2AdbIx4q2+/YWa1kHR9nJ6aun\nccLk58HkM+11HthSvitqx3zbn4AQ63Ib+oB1a2z91T5Y3Ix8AoaeRP4jN858K7pxv8HF7xdGZu1a\nbB8iI/5RtnvNmQ//Drc5iq6catcreGKMrQu8N9Y+gNIOwT1rKmWhi8gKY0zf0vaVa6EbY/JExF1T\n1Bf4wF1TFIg1xvzg2jdaRDYC+cDfjifmiqLUAMXFHGzY5JynrbAfWG1T37rj6j3pONa6LDKTi1q4\nJ4KvA66YahdvVWReoc1Q638/uA6umQ5tXdE6qYdsXxM228nhPn85uf648fGFSz6A7XOKrstoPaj8\n0aGPr11B3KwXtBtRcr+ITUz35kAb6lrRsNuTpFwLvapQC11RThF+uq8gfTFXfl16JE7WMfhvtPWl\nX/AW9LyievqWk26t4FNl/uRk+fURm/foptnQok+lTnU8C11XiirK6U5/l1+784SywyoDQwonViM7\nV0+/wK5VqO1iDjDqCbv+oJJiXh6aPldRTnciO8F1Px1/5S5Y90zSNusLVk4MR4BNGFfVl6nyKyiK\ncurj9k8fj26XlEzloJxSqMtFURSljqCCriiKUkdQQVcURakjqKAriqLUEVTQFUVR6ggq6IqiKHUE\nFXRFUZQ6ggq6oihKHcErNUVF5HoRSRCR1a6fUgo4KoqiKFVJuStFPWqKnoPNe/6niPxgjNlYrOmX\nxpj/q4I+KoqiKBWgIhZ6QU1RY0wO4K4pqiiKopxCeKumKMDFIjIMWwzjPmPMnuINPGuKAmkisuUE\n++smHEg8yWNPBbT/NYv2v2bR/leO1mXt8FZyrh+BL4wx2SJyK/ARcHbxRsaYd4B3KnsxEYktKx9w\nbUD7X7No/2sW7X/VURGXS7k1RY0xScaYbNfb9+D/2zu/ECvKMIz/HrIsDVrtQkoDN5LCpFK6MIqI\nClKTtYsuNoSUhG4CLYJo8arLKPp3kV1kaSEWbVaLUGQmdKWVFba5iophhqZQWhSU0tPF922d3D17\nNtrdme/w/mA4883MgWfed+Y5zDvf9x3Gd9LfIAiCYAijMfTPgDmSOiVdAHQDfY0HSLqsodkFDIyd\nxCAIgmA0jNV/iq6W1AWcBX4EVo6jZhiDsk3FhP5qCf3VEvrHicr+UzQIgiAYW2KkaBAEQZsQhh4E\nQdAmFGforaYhqBuSrpC0Q9JeSd9IWpO3T5e0TdKB/Dmtaq3NkHSepC8lbc3tTkm7cg7ezC/La4uk\nDkm9kvZJGpB0U2HxfyRfO/2SNku6sM45kPSKpBOS+hu2DRtvJV7I57FH0oLqlP+tdTj9T+XrZ4+k\ndyR1NOzryfr3S7qrGtWJogy9YRqCxcBc4D5Jc6tV1ZKzwKO25wILgYey5seB7bbnANtzu66s4d89\nl54EnrV9FfATsKoSVaPneeAD29cA15POpYj4S5oJrAZutD2P1DGhm3rnYAOw6JxtzeK9GJiTlweB\ndROkcSQ2MFT/NmCe7etIgyd7APK93A1cm7/zYvapSijK0ClwGgLbx2x/kdd/IZnJTJLujfmwjcA9\n1SgcGUmzgLtJ4wuQJNKgsd58SG21A0i6BLgVWA9g+w/bpygk/plJwEWSJgFTgGPUOAe2PyH1dmuk\nWbyXAa85sRPoOKcb9IQznH7bH9o+m5s7SeNxIOl/w/bvtg8DB0k+VQmlGfpw0xDMrEjLf0bSbGA+\nsAuYYftY3nUcmFGRrFY8BzwG/JnblwKnGi7uuuegEzgJvJrLRi9Lmkoh8bf9PfA0cIRk5KeB3ZSV\nA2ge7xLv6QeA9/N6rfSXZujFIuli4G3gYds/N+5z6jtau/6jkpYCJ2zvrlrL/2ASsABYZ3s+8Cvn\nlFfqGn+AXGteRvphuhyYytByQFHUOd6tkLSWVEbdVLWW4SjN0FtOQ1BHJJ1PMvNNtrfkzT8MPlrm\nzxNV6RuBm4EuSd+Sylu3k+rRHfnxH+qfg6PAUdu7cruXZPAlxB/gTuCw7ZO2zwBbSHkpKQfQPN7F\n3NOSVgJLgeX+ZwBPrfSXZugtpyGoG7nmvB4YsP1Mw64+YEVeXwG8N9HaWmG7x/Ys27NJsf7Y9nJg\nB3BvPqyW2gexfRz4TtLVedMdwF4KiH/mCLBQ0pR8LQ3qLyYHmWbx7gPuz71dFgKnG0oztUHSIlLp\nscv2bw27+oBuSZMldZJe7n5ahUYAbBe1AEtIb5kPAWur1jMKvbeQHi/3AF/lZQmpFr0dOAB8BEyv\nWmuL87gN2JrXryRdtAeBt4DJVetrof0G4POcg3eBaSXFH3gC2Af0A68Dk+ucA2Azqd5/hvSEtKpZ\nvAGReq4dAr4m9eapo/6DpFr54D38UsPxa7P+/cDiKrXH0P8gCII2obSSSxAEQdCEMPQgCII2IQw9\nCIKgTQhDD4IgaBPC0IMgCNqEMPQgCII2IQw9CIKgTfgLgTJv5tJ14NsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMpoqE6OYjBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQMhU27R5tS5",
        "colab_type": "text"
      },
      "source": [
        "####Run on Seq\n",
        "\n",
        "Erstellen eines Videos auf Testdaten aus Stuttgart um zu Präsentieren wie es aussieht."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQyJ3Snf5oM_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "a586f87e-30f3-4f81-c7ca-fc84b00bd07c"
      },
      "source": [
        "\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "network = DeepLabV3(\"eval_seq\", project_dir=datasetPath).cuda()\n",
        "#network.load_state_dict(torch.load(datasetPath+\"/pretrained_models/model_13_2_2_2_epoch_580.pth\"))\n",
        "#network.load_state_dict(torch.load(\"/content/drive/My Drive/U_Net_Datasets/training_logs/model_5/checkpoints/model_5_epoch_92.pth\"))\n",
        "network.load_state_dict(\n",
        "    torch.load(\"/content/drive/My Drive/U_Net_Datasets/training_logs/model_50_b3_w2/checkpoints/model_50_b3_w2_epoch_128.pth\"))\n",
        "for sequence in [\"00\", \"01\", \"02\"]:\n",
        "    print (sequence)\n",
        "\n",
        "    val_dataset = DatasetSeq(cityscapes_data_path, cityscapes_meta_path, sequence=sequence)\n",
        "\n",
        "    num_val_batches = int(len(val_dataset)/batch_size)\n",
        "    print (\"num_val_batches:\", num_val_batches)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                             batch_size=batch_size, shuffle=False,\n",
        "                                             num_workers=1)\n",
        "\n",
        "    network.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n",
        "    unsorted_img_ids = []\n",
        "    for step, (imgs, img_ids) in enumerate(val_loader):\n",
        "        with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n",
        "            imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n",
        "\n",
        "            outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n",
        "\n",
        "            ####################################################################\n",
        "            # save data for visualization:\n",
        "            ####################################################################\n",
        "            outputs = outputs.data.cpu().numpy() # (shape: (batch_size, num_classes, img_h, img_w))\n",
        "            pred_label_imgs = np.argmax(outputs, axis=1) # (shape: (batch_size, img_h, img_w))\n",
        "            pred_label_imgs = pred_label_imgs.astype(np.uint8)\n",
        "\n",
        "            for i in range(pred_label_imgs.shape[0]):\n",
        "                pred_label_img = pred_label_imgs[i] # (shape: (img_h, img_w))\n",
        "                img_id = img_ids[i]\n",
        "                img = imgs[i] # (shape: (3, img_h, img_w))\n",
        "\n",
        "                img = img.data.cpu().numpy()\n",
        "                img = np.transpose(img, (1, 2, 0)) # (shape: (img_h, img_w, 3))\n",
        "                img = img*np.array([0.229, 0.224, 0.225])\n",
        "                img = img + np.array([0.485, 0.456, 0.406])\n",
        "                img = img*255.0\n",
        "                img = img.astype(np.uint8)\n",
        "\n",
        "                pred_label_img_color = label_img_to_color(pred_label_img)\n",
        "                overlayed_img = 0.35*img + 0.65*pred_label_img_color\n",
        "                overlayed_img = overlayed_img.astype(np.uint8)\n",
        "\n",
        "                img_h = overlayed_img.shape[0]\n",
        "                img_w = overlayed_img.shape[1]\n",
        "\n",
        "                cv2.imwrite(network.model_dir + \"/\" + img_id + \".png\", img)\n",
        "                cv2.imwrite(network.model_dir + \"/\" + img_id + \"_pred.png\", pred_label_img_color)\n",
        "                cv2.imwrite(network.model_dir + \"/\" + img_id + \"_overlayed.png\", overlayed_img)\n",
        "\n",
        "                unsorted_img_ids.append(img_id)\n",
        "\n",
        "    ############################################################################\n",
        "    # create visualization video:\n",
        "    ############################################################################\n",
        "    out = cv2.VideoWriter(\"%s/stuttgart_%s_combined.avi\" % (network.model_dir, sequence), cv2.VideoWriter_fourcc(*\"MJPG\"), 20, (2*img_w, 2*img_h))\n",
        "    sorted_img_ids = sorted(unsorted_img_ids)\n",
        "    for img_id in sorted_img_ids:\n",
        "        img = cv2.imread(network.model_dir + \"/\" + img_id + \".png\", -1)\n",
        "        pred_img = cv2.imread(network.model_dir + \"/\" + img_id + \"_pred.png\", -1)\n",
        "        overlayed_img = cv2.imread(network.model_dir + \"/\" + img_id + \"_overlayed.png\", -1)\n",
        "\n",
        "        combined_img = np.zeros((2*img_h, 2*img_w, 3), dtype=np.uint8)\n",
        "\n",
        "        combined_img[0:img_h, 0:img_w] = img\n",
        "        combined_img[0:img_h, img_w:(2*img_w)] = pred_img\n",
        "        combined_img[img_h:(2*img_h), (int(img_w/2)):(img_w + int(img_w/2))] = overlayed_img\n",
        "\n",
        "        out.write(combined_img)\n",
        "\n",
        "    out.release()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pretrained resnet, 18\n",
            "pretrained resnet, 50\n",
            "00\n",
            "num_val_batches: 299\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}