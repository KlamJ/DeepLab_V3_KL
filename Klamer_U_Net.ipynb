{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Klamer_U_Net.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "w6POq8z_-aSd",
        "HJLURZEO5PaY",
        "J9e91NXYz-mt"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KlamJ/DeepLab_V3_KL/blob/master/Klamer_U_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ-vxFBI_msg",
        "colab_type": "text"
      },
      "source": [
        "![Bild](https://drive.google.com/uc?id=18KhcCqqeHGeL3Fmsliv2gV2vBcFIy2cP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uwQDXN_yiEb",
        "colab_type": "text"
      },
      "source": [
        "# Cityscapes Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MQ5IWWiqbtn",
        "colab_type": "text"
      },
      "source": [
        "Der Cityscapes Datensatz konzentriert sich auf die semantische Segmentierung von städtischen Straßenszenen. Im Folgenden wird mithilfe dieses Datensatzes das DCNN DeepLabV3 Trainiert und evaluiert.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WJm0rnDLUdE",
        "colab_type": "text"
      },
      "source": [
        "##TO DO \n",
        " \n",
        " - Beschreibung der AUfgabe und Motivation\n",
        " - Beschreibung des DeepLab\n",
        " - Beschreibung der Datensatzes \n",
        " - Beschreibung der einzelen Codeblöcke \n",
        "  - https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
        "  - beschreiben warum adam !!!\n",
        " - Code Blöcke entzerren und verkleinern\n",
        " - modele Trainieren mit verschiedenen Volumen \n",
        "  - mit verschiedenen Netzten ? (Zeit) \n",
        " - Präsentation Anlegen vor Weihnachten fertig machen \n",
        " - Präsentation warsch. am 15.01.20\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VRxG_c94loZ",
        "colab_type": "text"
      },
      "source": [
        "##Präsentation\n",
        "\n",
        "- 15min \n",
        "- 15 slides max \n",
        "- einführung hälfte technik deeplab \n",
        "- ergebnisse andere hälfte \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBxQIChQvWdc",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Training mehr als eine instanz lang \n",
        "https://medium.com/@prajwal.prashanth22/google-colab-drive-as-persistent-storage-for-long-training-runs-cb82bc1d5b71\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Icq80SfN7rYR",
        "colab_type": "code",
        "outputId": "47ad8c55-1001-4f96-c876-90de97b35028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "## Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#https://github.com/KlamJ/deeplabv3\n",
        "#https://drive.google.com/open?id=17mKymLGO6mQxSnbHLsg7TwzGgoQK1MpN"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2INXlplU73ZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#einmalige ausführung\n",
        "#import zipfile\n",
        "#path_to_zip_file = #'/content/drive/My Drive/U_Net_Datasets/gtFine_trainvaltest.zip'\n",
        "#directory_to_extract_to = '/content/drive/My Drive/U_Net_Datasets' \n",
        "#with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "#    zip_ref.extractall(directory_to_extract_to)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feyLL9_FEAdP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "e7be48ae-d655-4ff0-99da-2d6b7d8d996a"
      },
      "source": [
        "#' ' means CPU whereas '/device:G:0' means GPU\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rsm8T6EEEvK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "fd8316a1-f87d-4e9a-a9ac-0bea5cc5caeb"
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 24.7 GB  | Proc size: 2.6 GB\n",
            "GPU RAM Free: 15375MB | Used: 905MB | Util   6% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI97pt3KEFh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#if not the only user and the gpu Util is >0 do following \n",
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbd8dQr-FKh3",
        "colab_type": "text"
      },
      "source": [
        "Resnet18\n",
        "\n",
        "model 2: \n",
        "  - batch 3 \n",
        "  - worker 4\n",
        "\n",
        "model 3: \n",
        "  - batch 30\n",
        "  - worker 4\n",
        "\n",
        "model 5\n",
        " - batch 3\n",
        " - worker 2 \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9mzp7ni0Cdp",
        "colab_type": "text"
      },
      "source": [
        "#DeepLab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d_Gwc6F2t3x",
        "colab_type": "text"
      },
      "source": [
        "##The DeepLab V3 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H992mJv6K9I",
        "colab_type": "text"
      },
      "source": [
        "Importieren der Benötigten Bibliotheken"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcPu5-b0iIrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import sys\n",
        "import cv2\n",
        "datasetPath = \"/content/drive/My Drive/U_Net_Datasets\"\n",
        "cityscapes_data_path=\"/content/drive/My Drive/U_Net_Datasets\"\n",
        "cityscapes_meta_path=\"/content/drive/My Drive/U_Net_Datasets/meta\"\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXKu9xxU0i60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "sys.path.append(datasetPath)\n",
        "from datasets import DatasetVal # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\n",
        "\n",
        "from datasets import DatasetTrain, DatasetVal # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\n",
        "\n",
        "\n",
        "sys.path.append(datasetPath)\n",
        "from datasets import DatasetSeq # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\n",
        "\n",
        "sys.path.append(datasetPath+\"/model\")\n",
        "from deeplabv3 import DeepLabV3\n",
        "\n",
        "sys.path.append(datasetPath+\"/utils\")\n",
        "from utils import label_img_to_color\n",
        "\n",
        "sys.path.append(datasetPath+\"/utils\")\n",
        "from utils import add_weight_decay\n",
        "\n",
        "sys.path.append(datasetPath)\n",
        "from datasets import DatasetThnSeq # (this needs to be imported before torch, because cv2 needs to be imported before torch for some reason)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raYB_3GT5LP1",
        "colab_type": "text"
      },
      "source": [
        "###Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5WnI-Qz6OQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "from collections import namedtuple\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "\n",
        "train_dirs = [\"jena/\", \"zurich/\", \"weimar/\", \"ulm/\", \"tubingen/\", \"stuttgart/\",\n",
        "              \"strasbourg/\", \"monchengladbach/\", \"krefeld/\", \"hanover/\",\n",
        "             \"hamburg/\", \"erfurt/\", \"dusseldorf/\", \"darmstadt/\", \"cologne/\",\n",
        "             \"bremen/\", \"bochum/\", \"aachen/\"]\n",
        "val_dirs = [\"frankfurt/\", \"munster/\", \"lindau/\"]\n",
        "test_dirs = [\"berlin\", \"bielefeld\", \"bonn\", \"leverkusen\", \"mainz\", \"munich\"]\n",
        "\n",
        "class DatasetTrain(torch.utils.data.Dataset):\n",
        "    def __init__(self, cityscapes_data_path, cityscapes_meta_path):\n",
        "        self.img_dir = cityscapes_data_path + \"/leftImg8bit/train/\"\n",
        "        self.label_dir = cityscapes_meta_path + \"/label_imgs/\"\n",
        "\n",
        "        self.img_h = 1024\n",
        "        self.img_w = 2048\n",
        "\n",
        "        self.new_img_h = 512\n",
        "        self.new_img_w = 1024\n",
        "\n",
        "        self.examples = []\n",
        "        for train_dir in train_dirs:\n",
        "            train_img_dir_path = self.img_dir + train_dir\n",
        "\n",
        "            file_names = os.listdir(train_img_dir_path)\n",
        "            for file_name in file_names:\n",
        "                img_id = file_name.split(\"_leftImg8bit.png\")[0]\n",
        "\n",
        "                img_path = train_img_dir_path + file_name\n",
        "\n",
        "                label_img_path = self.label_dir + img_id + \".png\"\n",
        "\n",
        "                example = {}\n",
        "                example[\"img_path\"] = img_path\n",
        "                example[\"label_img_path\"] = label_img_path\n",
        "                example[\"img_id\"] = img_id\n",
        "                self.examples.append(example)\n",
        "\n",
        "        self.num_examples = len(self.examples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = self.examples[index]\n",
        "\n",
        "        img_path = example[\"img_path\"]\n",
        "        img = cv2.imread(img_path, -1) # (shape: (1024, 2048, 3))\n",
        "        # resize img without interpolation (want the image to still match\n",
        "        # label_img, which we resize below):\n",
        "        img = cv2.resize(img, (self.new_img_w, self.new_img_h),\n",
        "                         interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024, 3))\n",
        "\n",
        "        label_img_path = example[\"label_img_path\"]\n",
        "        label_img = cv2.imread(label_img_path, -1) # (shape: (1024, 2048))\n",
        "        # resize label_img without interpolation (want the resulting image to\n",
        "        # still only contain pixel values corresponding to an object class):\n",
        "        label_img = cv2.resize(label_img, (self.new_img_w, self.new_img_h),\n",
        "                               interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024))\n",
        "\n",
        "        # flip the img and the label with 0.5 probability:\n",
        "        flip = np.random.randint(low=0, high=2)\n",
        "        if flip == 1:\n",
        "            img = cv2.flip(img, 1)\n",
        "            label_img = cv2.flip(label_img, 1)\n",
        "\n",
        "        ########################################################################\n",
        "        # randomly scale the img and the label:\n",
        "        ########################################################################\n",
        "        scale = np.random.uniform(low=0.7, high=2.0)\n",
        "        new_img_h = int(scale*self.new_img_h)\n",
        "        new_img_w = int(scale*self.new_img_w)\n",
        "\n",
        "        # resize img without interpolation (want the image to still match\n",
        "        # label_img, which we resize below):\n",
        "        img = cv2.resize(img, (new_img_w, new_img_h),\n",
        "                         interpolation=cv2.INTER_NEAREST) # (shape: (new_img_h, new_img_w, 3))\n",
        "\n",
        "        # resize label_img without interpolation (want the resulting image to\n",
        "        # still only contain pixel values corresponding to an object class):\n",
        "        label_img = cv2.resize(label_img, (new_img_w, new_img_h),\n",
        "                               interpolation=cv2.INTER_NEAREST) # (shape: (new_img_h, new_img_w))\n",
        "        ########################################################################\n",
        "\n",
        "         # # # # # # # debug visualization START\n",
        "        #print (scale)\n",
        "        #print (new_img_h)\n",
        "        #print (new_img_w)\n",
        "        \n",
        "        #cv2.imshow(\"test\", img)\n",
        "        #cv2.waitKey(0)\n",
        "        \n",
        "        #cv2.imshow(\"test\", label_img)\n",
        "        #cv2.waitKey(0)\n",
        "         # # # # # # # debug visualization END\n",
        "\n",
        "        ########################################################################\n",
        "        # select a 256x256 random crop from the img and label:\n",
        "        ########################################################################\n",
        "        start_x = np.random.randint(low=0, high=(new_img_w - 256))\n",
        "        end_x = start_x + 256\n",
        "        start_y = np.random.randint(low=0, high=(new_img_h - 256))\n",
        "        end_y = start_y + 256\n",
        "\n",
        "        img = img[start_y:end_y, start_x:end_x] # (shape: (256, 256, 3))\n",
        "        label_img = label_img[start_y:end_y, start_x:end_x] # (shape: (256, 256))\n",
        "        ########################################################################\n",
        "\n",
        "        # # # # # # # # debug visualization START\n",
        "        # print (img.shape)\n",
        "        # print (label_img.shape)\n",
        "        #\n",
        "        # cv2.imshow(\"test\", img)\n",
        "        # cv2.waitKey(0)\n",
        "        #\n",
        "        # cv2.imshow(\"test\", label_img)\n",
        "        # cv2.waitKey(0)\n",
        "        # # # # # # # # debug visualization END\n",
        "\n",
        "        # normalize the img (with the mean and std for the pretrained ResNet):\n",
        "        img = img/255.0\n",
        "        img = img - np.array([0.485, 0.456, 0.406])\n",
        "        img = img/np.array([0.229, 0.224, 0.225]) # (shape: (256, 256, 3))\n",
        "        img = np.transpose(img, (2, 0, 1)) # (shape: (3, 256, 256))\n",
        "        img = img.astype(np.float32)\n",
        "\n",
        "        # convert numpy -> torch:\n",
        "        img = torch.from_numpy(img) # (shape: (3, 256, 256))\n",
        "        label_img = torch.from_numpy(label_img) # (shape: (256, 256))\n",
        "\n",
        "        return (img, label_img)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_examples\n",
        "\n",
        "class DatasetVal(torch.utils.data.Dataset):\n",
        "    def __init__(self, cityscapes_data_path, cityscapes_meta_path):\n",
        "        self.img_dir = cityscapes_data_path + \"/leftImg8bit/val/\"\n",
        "        self.label_dir = cityscapes_meta_path + \"/label_imgs/\"\n",
        "\n",
        "        self.img_h = 1024\n",
        "        self.img_w = 2048\n",
        "\n",
        "        self.new_img_h = 512\n",
        "        self.new_img_w = 1024\n",
        "\n",
        "        self.examples = []\n",
        "        for val_dir in val_dirs:\n",
        "            val_img_dir_path = self.img_dir + val_dir\n",
        "\n",
        "            file_names = os.listdir(val_img_dir_path)\n",
        "            for file_name in file_names:\n",
        "                img_id = file_name.split(\"_leftImg8bit.png\")[0]\n",
        "\n",
        "                img_path = val_img_dir_path + file_name\n",
        "\n",
        "                label_img_path = self.label_dir + img_id + \".png\"\n",
        "                label_img = cv2.imread(label_img_path, -1) # (shape: (1024, 2048))\n",
        "\n",
        "                example = {}\n",
        "                example[\"img_path\"] = img_path\n",
        "                example[\"label_img_path\"] = label_img_path\n",
        "                example[\"img_id\"] = img_id\n",
        "                self.examples.append(example)\n",
        "\n",
        "        self.num_examples = len(self.examples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = self.examples[index]\n",
        "\n",
        "        img_id = example[\"img_id\"]\n",
        "\n",
        "        img_path = example[\"img_path\"]\n",
        "        img = cv2.imread(img_path, -1) # (shape: (1024, 2048, 3))\n",
        "        # resize img without interpolation (want the image to still match\n",
        "        # label_img, which we resize below):\n",
        "        img = cv2.resize(img, (self.new_img_w, self.new_img_h),\n",
        "                         interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024, 3))\n",
        "\n",
        "        label_img_path = example[\"label_img_path\"]\n",
        "        label_img = cv2.imread(label_img_path, -1) # (shape: (1024, 2048))\n",
        "        # resize label_img without interpolation (want the resulting image to\n",
        "        # still only contain pixel values corresponding to an object class):\n",
        "        label_img = cv2.resize(label_img, (self.new_img_w, self.new_img_h),\n",
        "                               interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024))\n",
        "\n",
        "        # # # # # # # # debug visualization START\n",
        "        # cv2.imshow(\"test\", img)\n",
        "        # cv2.waitKey(0)\n",
        "        #\n",
        "        # cv2.imshow(\"test\", label_img)\n",
        "        # cv2.waitKey(0)\n",
        "        # # # # # # # # debug visualization END\n",
        "\n",
        "        # normalize the img (with the mean and std for the pretrained ResNet):\n",
        "        img = img/255.0\n",
        "        img = img - np.array([0.485, 0.456, 0.406])\n",
        "        img = img/np.array([0.229, 0.224, 0.225]) # (shape: (512, 1024, 3))\n",
        "        img = np.transpose(img, (2, 0, 1)) # (shape: (3, 512, 1024))\n",
        "        img = img.astype(np.float32)\n",
        "\n",
        "        # convert numpy -> torch:\n",
        "        img = torch.from_numpy(img) # (shape: (3, 512, 1024))\n",
        "        label_img = torch.from_numpy(label_img) # (shape: (512, 1024))\n",
        "\n",
        "        return (img, label_img, img_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_examples\n",
        "\n",
        "class DatasetSeq(torch.utils.data.Dataset):\n",
        "    def __init__(self, cityscapes_data_path, cityscapes_meta_path, sequence):\n",
        "        self.img_dir = cityscapes_data_path + \"/leftImg8bit/demoVideo/stuttgart_\" + sequence + \"/\"\n",
        "\n",
        "        self.img_h = 1024\n",
        "        self.img_w = 2048\n",
        "\n",
        "        self.new_img_h = 512\n",
        "        self.new_img_w = 1024\n",
        "\n",
        "        self.examples = []\n",
        "\n",
        "        file_names = os.listdir(self.img_dir)\n",
        "        for file_name in file_names:\n",
        "            img_id = file_name.split(\"_leftImg8bit.png\")[0]\n",
        "\n",
        "            img_path = self.img_dir + file_name\n",
        "\n",
        "            example = {}\n",
        "            example[\"img_path\"] = img_path\n",
        "            example[\"img_id\"] = img_id\n",
        "            self.examples.append(example)\n",
        "\n",
        "        self.num_examples = len(self.examples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = self.examples[index]\n",
        "\n",
        "        img_id = example[\"img_id\"]\n",
        "\n",
        "        img_path = example[\"img_path\"]\n",
        "        img = cv2.imread(img_path, -1) # (shape: (1024, 2048, 3))\n",
        "        # resize img without interpolation:\n",
        "        img = cv2.resize(img, (self.new_img_w, self.new_img_h),\n",
        "                         interpolation=cv2.INTER_NEAREST) # (shape: (512, 1024, 3))\n",
        "\n",
        "        # normalize the img (with the mean and std for the pretrained ResNet):\n",
        "        img = img/255.0\n",
        "        img = img - np.array([0.485, 0.456, 0.406])\n",
        "        img = img/np.array([0.229, 0.224, 0.225]) # (shape: (512, 1024, 3))\n",
        "        img = np.transpose(img, (2, 0, 1)) # (shape: (3, 512, 1024))\n",
        "        img = img.astype(np.float32)\n",
        "\n",
        "        # convert numpy -> torch:\n",
        "        img = torch.from_numpy(img) # (shape: (3, 512, 1024))\n",
        "\n",
        "        return (img, img_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_examples\n",
        "\n",
        "class DatasetThnSeq(torch.utils.data.Dataset):\n",
        "    def __init__(self, thn_data_path):\n",
        "        self.img_dir = thn_data_path + \"/\"\n",
        "\n",
        "        self.examples = []\n",
        "\n",
        "        file_names = os.listdir(self.img_dir)\n",
        "        for file_name in file_names:\n",
        "            img_id = file_name.split(\".png\")[0]\n",
        "\n",
        "            img_path = self.img_dir + file_name\n",
        "\n",
        "            example = {}\n",
        "            example[\"img_path\"] = img_path\n",
        "            example[\"img_id\"] = img_id\n",
        "            self.examples.append(example)\n",
        "\n",
        "        self.num_examples = len(self.examples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = self.examples[index]\n",
        "\n",
        "        img_id = example[\"img_id\"]\n",
        "\n",
        "        img_path = example[\"img_path\"]\n",
        "        img = cv2.imread(img_path, -1) # (shape: (512, 1024, 3))\n",
        "\n",
        "        # normalize the img (with mean and std for the pretrained ResNet):\n",
        "        img = img/255.0\n",
        "        img = img - np.array([0.485, 0.456, 0.406])\n",
        "        img = img/np.array([0.229, 0.224, 0.225]) # (shape: (512, 1024, 3))\n",
        "        img = np.transpose(img, (2, 0, 1)) # (shape: (3, 512, 1024))\n",
        "        img = img.astype(np.float32)\n",
        "\n",
        "        # convert numpy -> torch:\n",
        "        img = torch.from_numpy(img) # (shape: (3, 512, 1024))\n",
        "\n",
        "        return (img, img_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_examples\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6POq8z_-aSd",
        "colab_type": "text"
      },
      "source": [
        "#### Einmalige ausführung zum erzeugen der Label und Klassengewichte\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-AVpbPI5W_m",
        "colab_type": "code",
        "outputId": "8bd10a22-baff-4882-ddf9-3fd94c637f98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "source": [
        "\n",
        "\n",
        "# (NOTE! this is taken from the official Cityscapes scripts:)\n",
        "Label = namedtuple( 'Label' , [\n",
        "\n",
        "    'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n",
        "                    # We use them to uniquely name a class\n",
        "\n",
        "    'id'          , # An integer ID that is associated with this label.\n",
        "                    # The IDs are used to represent the label in ground truth images\n",
        "                    # An ID of -1 means that this label does not have an ID and thus\n",
        "                    # is ignored when creating ground truth images (e.g. license plate).\n",
        "                    # Do not modify these IDs, since exactly these IDs are expected by the\n",
        "                    # evaluation server.\n",
        "\n",
        "    'trainId'     , # Feel free to modify these IDs as suitable for your method. Then create\n",
        "                    # ground truth images with train IDs, using the tools provided in the\n",
        "                    # 'preparation' folder. However, make sure to validate or submit results\n",
        "                    # to our evaluation server using the regular IDs above!\n",
        "                    # For trainIds, multiple labels might have the same ID. Then, these labels\n",
        "                    # are mapped to the same class in the ground truth images. For the inverse\n",
        "                    # mapping, we use the label that is defined first in the list below.\n",
        "                    # For example, mapping all void-type classes to the same ID in training,\n",
        "                    # might make sense for some approaches.\n",
        "                    # Max value is 255!\n",
        "\n",
        "    'category'    , # The name of the category that this label belongs to\n",
        "\n",
        "    'categoryId'  , # The ID of this category. Used to create ground truth images\n",
        "                    # on category level.\n",
        "\n",
        "    'hasInstances', # Whether this label distinguishes between single instances or not\n",
        "\n",
        "    'ignoreInEval', # Whether pixels having this class as ground truth label are ignored\n",
        "                    # during evaluations or not\n",
        "\n",
        "    'color'       , # The color of this label\n",
        "    ] )\n",
        "\n",
        "# (NOTE! this is taken from the official Cityscapes scripts:)\n",
        "labels = [\n",
        "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
        "    Label(  'unlabeled'            ,  0 ,      19 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'ego vehicle'          ,  1 ,      19 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'rectification border' ,  2 ,      19 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'out of roi'           ,  3 ,      19 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'static'               ,  4 ,      19 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'dynamic'              ,  5 ,      19 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
        "    Label(  'ground'               ,  6 ,      19 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
        "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
        "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
        "    Label(  'parking'              ,  9 ,      19 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
        "    Label(  'rail track'           , 10 ,      19 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
        "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
        "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
        "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
        "    Label(  'guard rail'           , 14 ,      19 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
        "    Label(  'bridge'               , 15 ,      19 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
        "    Label(  'tunnel'               , 16 ,      19 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
        "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
        "    Label(  'polegroup'            , 18 ,      19 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
        "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
        "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
        "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
        "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
        "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
        "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
        "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
        "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
        "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
        "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
        "    Label(  'caravan'              , 29 ,      19 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
        "    Label(  'trailer'              , 30 ,      19 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
        "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
        "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
        "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
        "    Label(  'license plate'        , -1 ,       19 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
        "]\n",
        "\n",
        "# create a function which maps id to trainId:\n",
        "id_to_trainId = {label.id: label.trainId for label in labels}\n",
        "id_to_trainId_map_func = np.vectorize(id_to_trainId.get)\n",
        "\n",
        "train_dirs = [\"jena/\", \"zurich/\", \"weimar/\", \"ulm/\", \"tubingen/\", \"stuttgart/\",\n",
        "              \"strasbourg/\", \"monchengladbach/\", \"krefeld/\", \"hanover/\",\n",
        "              \"hamburg/\", \"erfurt/\", \"dusseldorf/\", \"darmstadt/\", \"cologne/\",\n",
        "              \"bremen/\", \"bochum/\", \"aachen/\"]\n",
        "val_dirs = [\"frankfurt/\", \"munster/\", \"lindau/\"]\n",
        "test_dirs = [\"berlin\", \"bielefeld\", \"bonn\", \"leverkusen\", \"mainz\", \"munich\"]\n",
        "\n",
        "#cityscapes_data_path = \"/root/deeplabv3/data/cityscapes\"\n",
        "#cityscapes_meta_path = \"/root/deeplabv3/data/cityscapes/meta\"\n",
        "\n",
        "cityscapes_data_path = \"/content/drive/My Drive/U_Net_Datasets\"\n",
        "cityscapes_meta_path = \"/content/drive/My Drive/U_Net_Datasets/meta\"\n",
        "\n",
        "if not os.path.exists(cityscapes_meta_path):\n",
        "    os.makedirs(cityscapes_meta_path)\n",
        "if not os.path.exists(cityscapes_meta_path + \"/label_imgs\"):\n",
        "    os.makedirs(cityscapes_meta_path + \"/label_imgs\")\n",
        "\n",
        "################################################################################\n",
        "# convert all labels to label imgs with trainId pixel values (and save to disk):\n",
        "################################################################################\n",
        "train_label_img_paths = []\n",
        "\n",
        "img_dir = cityscapes_data_path + \"/leftImg8bit/train/\"\n",
        "label_dir = cityscapes_data_path + \"/gtFine/train/\"\n",
        "for train_dir in train_dirs:\n",
        "    print (train_dir)\n",
        "\n",
        "    train_img_dir_path = img_dir + train_dir\n",
        "    train_label_dir_path = label_dir + train_dir\n",
        "\n",
        "    file_names = os.listdir(train_img_dir_path)\n",
        "    for file_name in file_names:\n",
        "        img_id = file_name.split(\"_leftImg8bit.png\")[0]\n",
        "\n",
        "        gtFine_img_path = train_label_dir_path + img_id + \"_gtFine_labelIds.png\"\n",
        "        gtFine_img = cv2.imread(gtFine_img_path, -1) # (shape: (1024, 2048))\n",
        "\n",
        "        # convert gtFine_img from id to trainId pixel values:\n",
        "        label_img = id_to_trainId_map_func(gtFine_img)#(shape: (1024, 2048))\n",
        "        label_img = label_img.astype(np.uint8)\n",
        "\n",
        "        cv2.imwrite(cityscapes_meta_path + \"/label_imgs/\" + img_id + \".png\", label_img)\n",
        "        train_label_img_paths.append(cityscapes_meta_path + \"/label_imgs/\" + img_id + \".png\")\n",
        "\n",
        "img_dir = cityscapes_data_path + \"/leftImg8bit/val/\"\n",
        "label_dir = cityscapes_data_path + \"/gtFine/val/\"\n",
        "for val_dir in val_dirs:\n",
        "    print (val_dir)\n",
        "\n",
        "    val_img_dir_path = img_dir + val_dir\n",
        "    val_label_dir_path = label_dir + val_dir\n",
        "\n",
        "    file_names = os.listdir(val_img_dir_path)\n",
        "    for file_name in file_names:\n",
        "        img_id = file_name.split(\"_leftImg8bit.png\")[0]\n",
        "\n",
        "        gtFine_img_path = val_label_dir_path + img_id + \"_gtFine_labelIds.png\"\n",
        "        gtFine_img = cv2.imread(gtFine_img_path, -1) # (shape: (1024, 2048))\n",
        "\n",
        "        # convert gtFine_img from id to trainId pixel values:\n",
        "        label_img = id_to_trainId_map_func(gtFine_img) # (shape: (1024, 2048))\n",
        "        label_img = label_img.astype(np.uint8)\n",
        "\n",
        "        cv2.imwrite(cityscapes_meta_path + \"/label_imgs/\" + img_id + \".png\", label_img)\n",
        "\n",
        "################################################################################\n",
        "# compute the class weigths:\n",
        "################################################################################\n",
        "print (\"computing class weights\")\n",
        "\n",
        "num_classes = 20\n",
        "\n",
        "trainId_to_count = {}\n",
        "for trainId in range(num_classes):\n",
        "    trainId_to_count[trainId] = 0\n",
        "\n",
        "# get the total number of pixels in all train label_imgs that are of each object class:\n",
        "for step, label_img_path in enumerate(train_label_img_paths):\n",
        "    if step % 100 == 0:\n",
        "        print (step)\n",
        "\n",
        "    label_img = cv2.imread(label_img_path, -1)\n",
        "\n",
        "    for trainId in range(num_classes):\n",
        "        # count how many pixels in label_img which are of object class trainId:\n",
        "        trainId_mask = np.equal(label_img, trainId)\n",
        "        trainId_count = np.sum(trainId_mask)\n",
        "\n",
        "        # add to the total count:\n",
        "        trainId_to_count[trainId] += trainId_count\n",
        "\n",
        "# compute the class weights according to the ENet paper:\n",
        "class_weights = []\n",
        "total_count = sum(trainId_to_count.values())\n",
        "for trainId, count in trainId_to_count.items():\n",
        "    trainId_prob = float(count)/float(total_count)\n",
        "    trainId_weight = 1/np.log(1.02 + trainId_prob)\n",
        "    class_weights.append(trainId_weight)\n",
        "\n",
        "print (class_weights)\n",
        "\n",
        "with open(cityscapes_meta_path + \"/class_weights.pkl\", \"wb\") as file:\n",
        "    pickle.dump(class_weights, file, protocol=2) # (protocol=2 is needed to be able to open this file with python2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jena/\n",
            "zurich/\n",
            "weimar/\n",
            "ulm/\n",
            "tubingen/\n",
            "stuttgart/\n",
            "strasbourg/\n",
            "monchengladbach/\n",
            "krefeld/\n",
            "hanover/\n",
            "hamburg/\n",
            "erfurt/\n",
            "dusseldorf/\n",
            "darmstadt/\n",
            "cologne/\n",
            "bremen/\n",
            "bochum/\n",
            "aachen/\n",
            "frankfurt/\n",
            "munster/\n",
            "lindau/\n",
            "computing class weights\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n",
            "[3.362088928135997, 14.031521298730318, 4.986657918172686, 39.254403222891234, 36.5125971773311, 32.89620795239199, 46.286660134462245, 40.69042748040039, 6.698241903441155, 33.55545414377673, 18.487832644189325, 32.97431249303082, 47.676506488107115, 12.70028597336979, 45.20542136324199, 45.78372411642551, 45.825290445040096, 48.40614734589367, 42.75592219573717, 7.912219457368151]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJLURZEO5PaY",
        "colab_type": "text"
      },
      "source": [
        "####Random Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHSYi2oj5fxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this file contains code snippets which I have found (more or less) useful at\n",
        "# some point during the project. Probably nothing interesting to see here.\n",
        "\n",
        "import pickle\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "model_id = \"13_2_2_2\"\n",
        "\n",
        "with open(\"/home/fregu856/exjobb/training_logs/multitask/model_\" + model_id + \"/epoch_losses_train.pkl\", \"rb\") as file:\n",
        "    train_loss = pickle.load(file)\n",
        "\n",
        "with open(\"/home/fregu856/exjobb/training_logs/multitask/model_\" + model_id + \"/epoch_losses_val.pkl\", \"rb\") as file:\n",
        "    val_loss = pickle.load(file)\n",
        "\n",
        "print (\"train loss min:\", np.argmin(np.array(train_loss)), np.min(np.array(train_loss)))\n",
        "\n",
        "print (\"val loss min:\", np.argmin(np.array(val_loss)), np.min(np.array(val_loss)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtNUqHto5R65",
        "colab_type": "text"
      },
      "source": [
        "####Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l14z8QR5Ovn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def add_weight_decay(net, l2_value, skip_list=()):\n",
        "    # https://raberrytv.wordpress.com/2017/10/29/pytorch-weight-decay-made-easy/\n",
        "\n",
        "    decay, no_decay = [], []\n",
        "    for name, param in net.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue # frozen weights\n",
        "        if len(param.shape) == 1 or name.endswith(\".bias\") or name in skip_list:\n",
        "            no_decay.append(param)\n",
        "        else:\n",
        "            decay.append(param)\n",
        "\n",
        "    return [{'params': no_decay, 'weight_decay': 0.0}, {'params': decay, 'weight_decay': l2_value}]\n",
        "\n",
        "# function for colorizing a label image:\n",
        "def label_img_to_color(img):\n",
        "    label_to_color = {\n",
        "        0: [128, 64,128],\n",
        "        1: [244, 35,232],\n",
        "        2: [ 70, 70, 70],\n",
        "        3: [102,102,156],\n",
        "        4: [190,153,153],\n",
        "        5: [153,153,153],\n",
        "        6: [250,170, 30],\n",
        "        7: [220,220,  0],\n",
        "        8: [107,142, 35],\n",
        "        9: [152,251,152],\n",
        "        10: [ 70,130,180],\n",
        "        11: [220, 20, 60],\n",
        "        12: [255,  0,  0],\n",
        "        13: [  0,  0,142],\n",
        "        14: [  0,  0, 70],\n",
        "        15: [  0, 60,100],\n",
        "        16: [  0, 80,100],\n",
        "        17: [  0,  0,230],\n",
        "        18: [119, 11, 32],\n",
        "        19: [81,  0, 81]\n",
        "        }\n",
        "\n",
        "    img_height, img_width = img.shape\n",
        "\n",
        "    img_color = np.zeros((img_height, img_width, 3))\n",
        "    for row in range(img_height):\n",
        "        for col in range(img_width):\n",
        "            label = img[row, col]\n",
        "\n",
        "            img_color[row, col] = np.array(label_to_color[label])\n",
        "\n",
        "    return img_color"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfcowAUH32SX",
        "colab_type": "text"
      },
      "source": [
        "###Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwBiOz6m39og",
        "colab_type": "text"
      },
      "source": [
        "####ASPP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8lPxBUz2251",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ASPP, self).__init__()\n",
        "\n",
        "        self.conv_1x1_1 = nn.Conv2d(512, 256, kernel_size=1)\n",
        "        self.bn_conv_1x1_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_1 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=6, dilation=6)\n",
        "        self.bn_conv_3x3_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_2 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=12, dilation=12)\n",
        "        self.bn_conv_3x3_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_3 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=18, dilation=18)\n",
        "        self.bn_conv_3x3_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.conv_1x1_2 = nn.Conv2d(512, 256, kernel_size=1)\n",
        "        self.bn_conv_1x1_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_1x1_3 = nn.Conv2d(1280, 256, kernel_size=1) # (1280 = 5*256)\n",
        "        self.bn_conv_1x1_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_1x1_4 = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, feature_map):\n",
        "        # (feature_map has shape (batch_size, 512, h/16, w/16)) (assuming self.resnet is ResNet18_OS16 or ResNet34_OS16. If self.resnet instead is ResNet18_OS8 or ResNet34_OS8, it will be (batch_size, 512, h/8, w/8))\n",
        "\n",
        "        feature_map_h = feature_map.size()[2] # (== h/16)\n",
        "        feature_map_w = feature_map.size()[3] # (== w/16)\n",
        "\n",
        "        out_1x1 = F.relu(self.bn_conv_1x1_1(self.conv_1x1_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out_3x3_1 = F.relu(self.bn_conv_3x3_1(self.conv_3x3_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out_3x3_2 = F.relu(self.bn_conv_3x3_2(self.conv_3x3_2(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out_3x3_3 = F.relu(self.bn_conv_3x3_3(self.conv_3x3_3(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "\n",
        "        out_img = self.avg_pool(feature_map) # (shape: (batch_size, 512, 1, 1))\n",
        "        out_img = F.relu(self.bn_conv_1x1_2(self.conv_1x1_2(out_img))) # (shape: (batch_size, 256, 1, 1))\n",
        "        out_img = F.upsample(out_img, size=(feature_map_h, feature_map_w), mode=\"bilinear\") # (shape: (batch_size, 256, h/16, w/16))\n",
        "\n",
        "        out = torch.cat([out_1x1, out_3x3_1, out_3x3_2, out_3x3_3, out_img], 1) # (shape: (batch_size, 1280, h/16, w/16))\n",
        "        out = F.relu(self.bn_conv_1x1_3(self.conv_1x1_3(out))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out = self.conv_1x1_4(out) # (shape: (batch_size, num_classes, h/16, w/16))\n",
        "\n",
        "        return out\n",
        "\n",
        "class ASPP_Bottleneck(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ASPP_Bottleneck, self).__init__()\n",
        "\n",
        "        self.conv_1x1_1 = nn.Conv2d(4*512, 256, kernel_size=1)\n",
        "        self.bn_conv_1x1_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_1 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=6, dilation=6)\n",
        "        self.bn_conv_3x3_1 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_2 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=12, dilation=12)\n",
        "        self.bn_conv_3x3_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_3x3_3 = nn.Conv2d(4*512, 256, kernel_size=3, stride=1, padding=18, dilation=18)\n",
        "        self.bn_conv_3x3_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.conv_1x1_2 = nn.Conv2d(4*512, 256, kernel_size=1)\n",
        "        self.bn_conv_1x1_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_1x1_3 = nn.Conv2d(1280, 256, kernel_size=1) # (1280 = 5*256)\n",
        "        self.bn_conv_1x1_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_1x1_4 = nn.Conv2d(256, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, feature_map):\n",
        "        # (feature_map has shape (batch_size, 4*512, h/16, w/16))\n",
        "\n",
        "        feature_map_h = feature_map.size()[2] # (== h/16)\n",
        "        feature_map_w = feature_map.size()[3] # (== w/16)\n",
        "\n",
        "        out_1x1 = F.relu(self.bn_conv_1x1_1(self.conv_1x1_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out_3x3_1 = F.relu(self.bn_conv_3x3_1(self.conv_3x3_1(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out_3x3_2 = F.relu(self.bn_conv_3x3_2(self.conv_3x3_2(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out_3x3_3 = F.relu(self.bn_conv_3x3_3(self.conv_3x3_3(feature_map))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "\n",
        "        out_img = self.avg_pool(feature_map) # (shape: (batch_size, 512, 1, 1))\n",
        "        out_img = F.relu(self.bn_conv_1x1_2(self.conv_1x1_2(out_img))) # (shape: (batch_size, 256, 1, 1))\n",
        "        out_img = F.upsample(out_img, size=(feature_map_h, feature_map_w), mode=\"bilinear\") # (shape: (batch_size, 256, h/16, w/16))\n",
        "\n",
        "        out = torch.cat([out_1x1, out_3x3_1, out_3x3_2, out_3x3_3, out_img], 1) # (shape: (batch_size, 1280, h/16, w/16))\n",
        "        out = F.relu(self.bn_conv_1x1_3(self.conv_1x1_3(out))) # (shape: (batch_size, 256, h/16, w/16))\n",
        "        out = self.conv_1x1_4(out) # (shape: (batch_size, num_classes, h/16, w/16))\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO6CrH4C4MqC",
        "colab_type": "text"
      },
      "source": [
        "####ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSFBJVdj4BJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NOTE! OS: output stride, the ratio of input image resolution to final output resolution (OS16: output size is (img_h/16, img_w/16)) (OS8: output size is (img_h/8, img_w/8))\n",
        "\n",
        "preTrainedModelsPath = datasetPath + \"/pretrained_models\"\n",
        "\n",
        "def make_layer(block, in_channels, channels, num_blocks, stride=1, dilation=1):\n",
        "    strides = [stride] + [1]*(num_blocks - 1) # (stride == 2, num_blocks == 4 --> strides == [2, 1, 1, 1])\n",
        "\n",
        "    blocks = []\n",
        "    for stride in strides:\n",
        "        blocks.append(block(in_channels=in_channels, channels=channels, stride=stride, dilation=dilation))\n",
        "        in_channels = block.expansion*channels\n",
        "\n",
        "    layer = nn.Sequential(*blocks) # (*blocks: call with unpacked list entires as arguments)\n",
        "\n",
        "    return layer\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, channels, stride=1, dilation=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        out_channels = self.expansion*channels\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=3, stride=stride, padding=dilation, dilation=dilation, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=dilation, dilation=dilation, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "\n",
        "        if (stride != 1) or (in_channels != out_channels):\n",
        "            conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            bn = nn.BatchNorm2d(out_channels)\n",
        "            self.downsample = nn.Sequential(conv, bn)\n",
        "        else:\n",
        "            self.downsample = nn.Sequential()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (x has shape: (batch_size, in_channels, h, w))\n",
        "\n",
        "        out = F.relu(self.bn1(self.conv1(x))) # (shape: (batch_size, channels, h, w) if stride == 1, (batch_size, channels, h/2, w/2) if stride == 2)\n",
        "        out = self.bn2(self.conv2(out)) # (shape: (batch_size, channels, h, w) if stride == 1, (batch_size, channels, h/2, w/2) if stride == 2)\n",
        "\n",
        "        out = out + self.downsample(x) # (shape: (batch_size, channels, h, w) if stride == 1, (batch_size, channels, h/2, w/2) if stride == 2)\n",
        "\n",
        "        out = F.relu(out) # (shape: (batch_size, channels, h, w) if stride == 1, (batch_size, channels, h/2, w/2) if stride == 2)\n",
        "\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channels, channels, stride=1, dilation=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "\n",
        "        out_channels = self.expansion*channels\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=stride, padding=dilation, dilation=dilation, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(channels, out_channels, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        if (stride != 1) or (in_channels != out_channels):\n",
        "            conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            bn = nn.BatchNorm2d(out_channels)\n",
        "            self.downsample = nn.Sequential(conv, bn)\n",
        "        else:\n",
        "            self.downsample = nn.Sequential()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (x has shape: (batch_size, in_channels, h, w))\n",
        "\n",
        "        out = F.relu(self.bn1(self.conv1(x))) # (shape: (batch_size, channels, h, w))\n",
        "        out = F.relu(self.bn2(self.conv2(out))) # (shape: (batch_size, channels, h, w) if stride == 1, (batch_size, channels, h/2, w/2) if stride == 2)\n",
        "        out = self.bn3(self.conv3(out)) # (shape: (batch_size, out_channels, h, w) if stride == 1, (batch_size, out_channels, h/2, w/2) if stride == 2)\n",
        "\n",
        "        out = out + self.downsample(x) # (shape: (batch_size, out_channels, h, w) if stride == 1, (batch_size, out_channels, h/2, w/2) if stride == 2)\n",
        "\n",
        "        out = F.relu(out) # (shape: (batch_size, out_channels, h, w) if stride == 1, (batch_size, out_channels, h/2, w/2) if stride == 2)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet_Bottleneck_OS16(nn.Module):\n",
        "    def __init__(self, num_layers):\n",
        "        super(ResNet_Bottleneck_OS16, self).__init__()\n",
        "\n",
        "        if num_layers == 50:\n",
        "            resnet = models.resnet50()\n",
        "            # load pretrained model:\n",
        "            resnet.load_state_dict(torch.load(preTrainedModelsPath+\"/resnet/resnet50-19c8e357.pth\"))\n",
        "            # remove fully connected layer, avg pool and layer5:\n",
        "            self.resnet = nn.Sequential(*list(resnet.children())[:-3])\n",
        "\n",
        "            print (\"pretrained resnet, 50\")\n",
        "        elif num_layers == 101:\n",
        "            resnet = models.resnet101()\n",
        "            # load pretrained model:\n",
        "            resnet.load_state_dict(torch.load(preTrainedModelsPath+\"/resnet/resnet101-5d3b4d8f.pth\"))\n",
        "            # remove fully connected layer, avg pool and layer5:\n",
        "            self.resnet = nn.Sequential(*list(resnet.children())[:-3])\n",
        "\n",
        "            print (\"pretrained resnet, 101\")\n",
        "        elif num_layers == 152:\n",
        "            resnet = models.resnet152()\n",
        "            # load pretrained model:\n",
        "            resnet.load_state_dict(torch.load(preTrainedModelsPath+\"/resnet/resnet152-b121ed2d.pth\"))\n",
        "            # remove fully connected layer, avg pool and layer5:\n",
        "            self.resnet = nn.Sequential(*list(resnet.children())[:-3])\n",
        "\n",
        "            print (\"pretrained resnet, 152\")\n",
        "        else:\n",
        "            raise Exception(\"num_layers must be in {50, 101, 152}!\")\n",
        "\n",
        "        self.layer5 = make_layer(Bottleneck, in_channels=4*256, channels=512, num_blocks=3, stride=1, dilation=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (x has shape (batch_size, 3, h, w))\n",
        "\n",
        "        # pass x through (parts of) the pretrained ResNet:\n",
        "        c4 = self.resnet(x) # (shape: (batch_size, 4*256, h/16, w/16)) (it's called c4 since 16 == 2^4)\n",
        "\n",
        "        output = self.layer5(c4) # (shape: (batch_size, 4*512, h/16, w/16))\n",
        "\n",
        "        return output\n",
        "\n",
        "class ResNet_BasicBlock_OS16(nn.Module):\n",
        "    def __init__(self, num_layers):\n",
        "        super(ResNet_BasicBlock_OS16, self).__init__()\n",
        "\n",
        "        if num_layers == 18:\n",
        "            resnet = models.resnet18()\n",
        "            # load pretrained model:\n",
        "            \n",
        "            \n",
        "            resnet.load_state_dict(torch.load(preTrainedModelsPath+\"/resnet/resnet18-5c106cde.pth\"))\n",
        "            # remove fully connected layer, avg pool and layer5:\n",
        "            \n",
        "            self.resnet = nn.Sequential(*list(resnet.children())[:-3])\n",
        "\n",
        "            num_blocks = 2\n",
        "            print (\"pretrained resnet, 18\")\n",
        "        elif num_layers == 34:\n",
        "            resnet = models.resnet34()\n",
        "            # load pretrained model:\n",
        "            resnet.load_state_dict(torch.load(preTrainedModelsPath+\"/resnet/resnet34-333f7ec4.pth\"))\n",
        "            # remove fully connected layer, avg pool and layer5:\n",
        "            self.resnet = nn.Sequential(*list(resnet.children())[:-3])\n",
        "\n",
        "            num_blocks = 3\n",
        "            print (\"pretrained resnet, 34\")\n",
        "        else:\n",
        "            raise Exception(\"num_layers must be in {18, 34}!\")\n",
        "\n",
        "        self.layer5 = make_layer(BasicBlock, in_channels=256, channels=512, num_blocks=num_blocks, stride=1, dilation=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (x has shape (batch_size, 3, h, w))\n",
        "\n",
        "        # pass x through (parts of) the pretrained ResNet:\n",
        "        c4 = self.resnet(x) # (shape: (batch_size, 256, h/16, w/16)) (it's called c4 since 16 == 2^4)\n",
        "\n",
        "        output = self.layer5(c4) # (shape: (batch_size, 512, h/16, w/16))\n",
        "\n",
        "        return output\n",
        "\n",
        "class ResNet_BasicBlock_OS8(nn.Module):\n",
        "    def __init__(self, num_layers):\n",
        "        super(ResNet_BasicBlock_OS8, self).__init__()\n",
        "\n",
        "        if num_layers == 18:\n",
        "            resnet = models.resnet18()\n",
        "            # load pretrained model:            \n",
        "            resnet.load_state_dict(torch.load(preTrainedModelsPath+\"/resnet/resnet18-5c106cde.pth\"))       \n",
        "\n",
        "            # remove fully connected layer, avg pool, layer4 and layer5:\n",
        "\n",
        "            \n",
        "            self.resnet = nn.Sequential(*list(resnet.children())[:-4])\n",
        "\n",
        "            num_blocks_layer_4 = 2\n",
        "            num_blocks_layer_5 = 2\n",
        "            print (\"pretrained resnet, 18\")\n",
        "        elif num_layers == 34:\n",
        "            resnet = models.resnet34()\n",
        "            # load pretrained model:\n",
        "            resnet.load_state_dict(torch.load(preTrainedModelsPath+\"/resnet/resnet34-333f7ec4.pth\"))\n",
        "            # remove fully connected layer, avg pool, layer4 and layer5:\n",
        "            self.resnet = nn.Sequential(*list(resnet.children())[:-4])\n",
        "\n",
        "            num_blocks_layer_4 = 6\n",
        "            num_blocks_layer_5 = 3\n",
        "            print (\"pretrained resnet, 34\")\n",
        "        else:\n",
        "            raise Exception(\"num_layers must be in {18, 34}!\")\n",
        "\n",
        "        self.layer4 = make_layer(BasicBlock, in_channels=128, channels=256, num_blocks=num_blocks_layer_4, stride=1, dilation=2)\n",
        "\n",
        "        self.layer5 = make_layer(BasicBlock, in_channels=256, channels=512, num_blocks=num_blocks_layer_5, stride=1, dilation=4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (x has shape (batch_size, 3, h, w))\n",
        "\n",
        "        # pass x through (parts of) the pretrained ResNet:\n",
        "        c3 = self.resnet(x) # (shape: (batch_size, 128, h/8, w/8)) (it's called c3 since 8 == 2^3)\n",
        "\n",
        "        output = self.layer4(c3) # (shape: (batch_size, 256, h/8, w/8))\n",
        "        output = self.layer5(output) # (shape: (batch_size, 512, h/8, w/8))\n",
        "\n",
        "        return output\n",
        "\n",
        "def ResNet18_OS16():\n",
        "    return ResNet_BasicBlock_OS16(num_layers=18)\n",
        "\n",
        "def ResNet34_OS16():\n",
        "    return ResNet_BasicBlock_OS16(num_layers=34)\n",
        "\n",
        "def ResNet50_OS16():\n",
        "    return ResNet_Bottleneck_OS16(num_layers=50)\n",
        "\n",
        "def ResNet101_OS16():\n",
        "    return ResNet_Bottleneck_OS16(num_layers=101)\n",
        "\n",
        "def ResNet152_OS16():\n",
        "    return ResNet_Bottleneck_OS16(num_layers=152)\n",
        "\n",
        "def ResNet18_OS8():\n",
        "    return ResNet_BasicBlock_OS8(num_layers=18)\n",
        "\n",
        "def ResNet34_OS8():\n",
        "    return ResNet_BasicBlock_OS8(num_layers=34)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l35iRa74B86",
        "colab_type": "text"
      },
      "source": [
        "####DeepLab V3\n",
        "\n",
        "Erstellen des DeepLab V3 \n",
        "  - hier kann man einstellen welches ResNet verwendet wird\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjlFHBL20GZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from resnet import ResNet18_OS16, ResNet34_OS16, ResNet50_OS16, ResNet101_OS16, ResNet152_OS16, ResNet18_OS8, ResNet34_OS8\n",
        "#from aspp import ASPP, ASPP_Bottleneck\n",
        "\n",
        "class DeepLabV3(nn.Module):\n",
        "    def __init__(self, model_id, project_dir):\n",
        "        super(DeepLabV3, self).__init__()\n",
        "\n",
        "        self.num_classes = 20\n",
        "\n",
        "        self.model_id = model_id\n",
        "        self.project_dir = project_dir\n",
        "        self.create_model_dirs()\n",
        "\n",
        "        self.resnet = ResNet18_OS8() # NOTE! specify the type of ResNet here\n",
        "        #self.resnet = ResNet50_OS16() # NOTE! specify the type of ResNet here\n",
        "        self.aspp = ASPP(num_classes=self.num_classes) # NOTE! if you use ResNet50-152,\n",
        "        #set self.aspp = ASPP_Bottleneck(num_classes=self.num_classes) #instead\n",
        "        #self.aspp = ASPP_Bottleneck(num_classes=self.num_classes)\n",
        "    def forward(self, x):\n",
        "        # (x has shape (batch_size, 3, h, w))\n",
        "\n",
        "        h = x.size()[2]\n",
        "        w = x.size()[3]\n",
        "\n",
        "        feature_map = self.resnet(x) # (shape: (batch_size, 512, h/16, w/16)) (assuming self.resnet \n",
        "        #is ResNet18_OS16 or ResNet34_OS16. If self.resnet is ResNet18_OS8 or ResNet34_OS8,\n",
        "        # it will be (batch_size, 512, h/8, w/8). If self.resnet is ResNet50-152, it will be (batch_size, 4*512, h/16, w/16))\n",
        "\n",
        "        output = self.aspp(feature_map) # (shape: (batch_size, num_classes, h/16, w/16))\n",
        "\n",
        "        output = F.upsample(output, size=(h, w), mode=\"bilinear\") # (shape: (batch_size, num_classes, h, w))\n",
        "\n",
        "        return output\n",
        "\n",
        "    def create_model_dirs(self):\n",
        "        self.logs_dir = self.project_dir + \"/training_logs\"\n",
        "        self.model_dir = self.logs_dir + \"/model_%s\" % self.model_id\n",
        "        self.checkpoints_dir = self.model_dir + \"/checkpoints\"\n",
        "        if not os.path.exists(self.logs_dir):\n",
        "            os.makedirs(self.logs_dir)\n",
        "        if not os.path.exists(self.model_dir):\n",
        "            os.makedirs(self.model_dir)\n",
        "            os.makedirs(self.checkpoints_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZmj3OkL6brT",
        "colab_type": "text"
      },
      "source": [
        "###Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDSBaVoj6dpp",
        "colab_type": "code",
        "outputId": "a64f6091-579c-4e25-dc5c-4aa8ec4d7c69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "# NOTE! NOTE! change this to not overwrite all log data when you train the model:\n",
        "model_id = \"5\"\n",
        "\n",
        "num_epochs = 1000\n",
        "batch_size = 3\n",
        "learning_rate = 0.0001\n",
        "\n",
        "#network = DeepLabV3(model_id, project_dir=\"/root/deeplabv3\").cuda()\n",
        "network = DeepLabV3(model_id, project_dir=datasetPath).cuda()\n",
        "train_dataset = DatasetTrain(cityscapes_data_path=\"/content/drive/My Drive/U_Net_Datasets\",\n",
        "                             cityscapes_meta_path=\"/content/drive/My Drive/U_Net_Datasets/meta\")\n",
        "val_dataset = DatasetVal(cityscapes_data_path=\"/content/drive/My Drive/U_Net_Datasets\",\n",
        "                         cityscapes_meta_path=\"/content/drive/My Drive/U_Net_Datasets/meta\")\n",
        "\n",
        "num_train_batches = int(len(train_dataset)/batch_size)\n",
        "num_val_batches = int(len(val_dataset)/batch_size)\n",
        "print (\"num_train_batches:\", num_train_batches)\n",
        "print (\"num_val_batches:\", num_val_batches)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size, shuffle=True,\n",
        "                                           num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                         batch_size=batch_size, shuffle=False,\n",
        "                                         num_workers=2)\n",
        "\n",
        "params = add_weight_decay(network, l2_value=0.0001)\n",
        "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
        "\n",
        "with open(\"/content/drive/My Drive/U_Net_Datasets/meta/class_weights.pkl\", \"rb\") as file: # (needed for python3)\n",
        "    class_weights = np.array(pickle.load(file))\n",
        "class_weights = torch.from_numpy(class_weights)\n",
        "class_weights = Variable(class_weights.type(torch.FloatTensor)).cuda()\n",
        "\n",
        "# loss function\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "epoch_losses_train = []\n",
        "epoch_losses_val = []\n",
        "for epoch in range(num_epochs):\n",
        "    print (\"###########################\")\n",
        "    print (\"######## NEW EPOCH ########\")\n",
        "    print (\"###########################\")\n",
        "    print (\"epoch: %d/%d\" % (epoch+1, num_epochs))\n",
        "\n",
        "    ############################################################################\n",
        "    # train:\n",
        "    ############################################################################\n",
        "    network.train() # (set in training mode, this affects BatchNorm and dropout)\n",
        "    batch_losses = []\n",
        "    for step, (imgs, label_imgs) in enumerate(train_loader):\n",
        "        #current_time = time.time()\n",
        "\n",
        "        imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n",
        "        label_imgs = Variable(label_imgs.type(torch.LongTensor)).cuda() # (shape: (batch_size, img_h, img_w))\n",
        "\n",
        "        outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n",
        "\n",
        "        # compute the loss:\n",
        "        loss = loss_fn(outputs, label_imgs)\n",
        "        loss_value = loss.data.cpu().numpy()\n",
        "        batch_losses.append(loss_value)\n",
        "\n",
        "        # optimization step:\n",
        "        optimizer.zero_grad() # (reset gradients)\n",
        "        loss.backward() # (compute gradients)\n",
        "        optimizer.step() # (perform optimization step)\n",
        "\n",
        "        #print (time.time() - current_time)\n",
        "\n",
        "    epoch_loss = np.mean(batch_losses)\n",
        "    epoch_losses_train.append(epoch_loss)\n",
        "    with open(\"%s/epoch_losses_train.pkl\" % network.model_dir, \"wb\") as file:\n",
        "        pickle.dump(epoch_losses_train, file)\n",
        "    print (\"train loss: %g\" % epoch_loss)\n",
        "    plt.figure(1)\n",
        "    plt.plot(epoch_losses_train, \"k^\")\n",
        "    plt.plot(epoch_losses_train, \"k\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.title(\"train loss per epoch\")\n",
        "    plt.savefig(\"%s/epoch_losses_train.png\" % network.model_dir)\n",
        "    plt.close(1)\n",
        "\n",
        "    print (\"####\")\n",
        "\n",
        "    ############################################################################\n",
        "    # val:\n",
        "    ############################################################################\n",
        "    network.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n",
        "    batch_losses = []\n",
        "    for step, (imgs, label_imgs, img_ids) in enumerate(val_loader):\n",
        "        with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n",
        "            imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n",
        "            label_imgs = Variable(label_imgs.type(torch.LongTensor)).cuda() # (shape: (batch_size, img_h, img_w))\n",
        "\n",
        "            outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n",
        "\n",
        "            # compute the loss:\n",
        "            loss = loss_fn(outputs, label_imgs)\n",
        "            loss_value = loss.data.cpu().numpy()\n",
        "            batch_losses.append(loss_value)\n",
        "\n",
        "    epoch_loss = np.mean(batch_losses)\n",
        "    epoch_losses_val.append(epoch_loss)\n",
        "    with open(\"%s/epoch_losses_val.pkl\" % network.model_dir, \"wb\") as file:\n",
        "        pickle.dump(epoch_losses_val, file)\n",
        "    print (\"val loss: %g\" % epoch_loss)\n",
        "    plt.figure(1)\n",
        "    plt.plot(epoch_losses_val, \"k^\")\n",
        "    plt.plot(epoch_losses_val, \"k\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.title(\"val loss per epoch\")\n",
        "    plt.savefig(\"%s/epoch_losses_val.png\" % network.model_dir)\n",
        "    plt.close(1)\n",
        "\n",
        "    # save the model weights to disk:\n",
        "    checkpoint_path = network.checkpoints_dir + \"/model_\" + model_id +\"_epoch_\" + str(epoch+1) + \".pth\"\n",
        "    torch.save(network.state_dict(), checkpoint_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pretrained resnet, 18\n",
            "num_train_batches: 991\n",
            "num_val_batches: 166\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 1/1000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2404: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 1.71566\n",
            "####\n",
            "val loss: 1.28551\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 2/1000\n",
            "train loss: 1.33474\n",
            "####\n",
            "val loss: 1.15877\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 3/1000\n",
            "train loss: 1.18687\n",
            "####\n",
            "val loss: 1.17286\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 4/1000\n",
            "train loss: 1.12538\n",
            "####\n",
            "val loss: 0.959327\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 5/1000\n",
            "train loss: 1.04912\n",
            "####\n",
            "val loss: 0.954953\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 6/1000\n",
            "train loss: 1.0255\n",
            "####\n",
            "val loss: 0.931524\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 7/1000\n",
            "train loss: 0.988906\n",
            "####\n",
            "val loss: 1.04643\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 8/1000\n",
            "train loss: 0.968043\n",
            "####\n",
            "val loss: 0.857911\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 9/1000\n",
            "train loss: 0.925999\n",
            "####\n",
            "val loss: 0.839865\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 10/1000\n",
            "train loss: 0.892962\n",
            "####\n",
            "val loss: 0.958511\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 11/1000\n",
            "train loss: 0.878194\n",
            "####\n",
            "val loss: 0.772173\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 12/1000\n",
            "train loss: 0.892676\n",
            "####\n",
            "val loss: 0.77452\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 13/1000\n",
            "train loss: 0.88536\n",
            "####\n",
            "val loss: 0.809101\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 14/1000\n",
            "train loss: 0.88334\n",
            "####\n",
            "val loss: 0.799234\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 15/1000\n",
            "train loss: 0.846447\n",
            "####\n",
            "val loss: 0.754723\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 16/1000\n",
            "train loss: 0.845235\n",
            "####\n",
            "val loss: 0.76175\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 17/1000\n",
            "train loss: 0.813823\n",
            "####\n",
            "val loss: 0.779446\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 18/1000\n",
            "train loss: 0.831728\n",
            "####\n",
            "val loss: 0.761042\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 19/1000\n",
            "train loss: 0.821838\n",
            "####\n",
            "val loss: 0.766232\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 20/1000\n",
            "train loss: 0.786221\n",
            "####\n",
            "val loss: 0.734486\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 21/1000\n",
            "train loss: 0.821138\n",
            "####\n",
            "val loss: 0.754037\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 22/1000\n",
            "train loss: 0.805262\n",
            "####\n",
            "val loss: 0.705937\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 23/1000\n",
            "train loss: 0.77948\n",
            "####\n",
            "val loss: 0.787906\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 24/1000\n",
            "train loss: 0.786325\n",
            "####\n",
            "val loss: 0.764995\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 25/1000\n",
            "train loss: 0.753593\n",
            "####\n",
            "val loss: 0.760987\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 26/1000\n",
            "train loss: 0.793109\n",
            "####\n",
            "val loss: 0.774265\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 27/1000\n",
            "train loss: 0.754502\n",
            "####\n",
            "val loss: 0.700833\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 28/1000\n",
            "train loss: 0.749984\n",
            "####\n",
            "val loss: 0.717664\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 29/1000\n",
            "train loss: 0.760234\n",
            "####\n",
            "val loss: 0.747271\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 30/1000\n",
            "train loss: 0.731958\n",
            "####\n",
            "val loss: 0.804621\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 31/1000\n",
            "train loss: 0.759049\n",
            "####\n",
            "val loss: 0.701573\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 32/1000\n",
            "train loss: 0.739601\n",
            "####\n",
            "val loss: 0.707144\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 33/1000\n",
            "train loss: 0.742906\n",
            "####\n",
            "val loss: 0.711912\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 34/1000\n",
            "train loss: 0.755379\n",
            "####\n",
            "val loss: 0.681996\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 35/1000\n",
            "train loss: 0.732557\n",
            "####\n",
            "val loss: 0.709347\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 36/1000\n",
            "train loss: 0.719606\n",
            "####\n",
            "val loss: 0.703882\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 37/1000\n",
            "train loss: 0.729999\n",
            "####\n",
            "val loss: 0.68445\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 38/1000\n",
            "train loss: 0.69874\n",
            "####\n",
            "val loss: 0.696958\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 39/1000\n",
            "train loss: 0.718789\n",
            "####\n",
            "val loss: 0.691597\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 40/1000\n",
            "train loss: 0.695767\n",
            "####\n",
            "val loss: 0.664548\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 41/1000\n",
            "train loss: 0.735807\n",
            "####\n",
            "val loss: 0.674044\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 42/1000\n",
            "train loss: 0.717692\n",
            "####\n",
            "val loss: 0.689207\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 43/1000\n",
            "train loss: 0.701422\n",
            "####\n",
            "val loss: 0.65024\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 44/1000\n",
            "train loss: 0.687598\n",
            "####\n",
            "val loss: 0.688667\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 45/1000\n",
            "train loss: 0.724074\n",
            "####\n",
            "val loss: 0.674225\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 46/1000\n",
            "train loss: 0.710331\n",
            "####\n",
            "val loss: 0.714989\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 47/1000\n",
            "train loss: 0.70117\n",
            "####\n",
            "val loss: 0.674995\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 48/1000\n",
            "train loss: 0.700614\n",
            "####\n",
            "val loss: 0.694076\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 49/1000\n",
            "train loss: 0.693369\n",
            "####\n",
            "val loss: 0.649252\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 50/1000\n",
            "train loss: 0.705156\n",
            "####\n",
            "val loss: 0.666214\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 51/1000\n",
            "train loss: 0.687096\n",
            "####\n",
            "val loss: 0.651839\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 52/1000\n",
            "train loss: 0.697569\n",
            "####\n",
            "val loss: 0.654726\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 53/1000\n",
            "train loss: 0.680616\n",
            "####\n",
            "val loss: 0.681329\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 54/1000\n",
            "train loss: 0.683423\n",
            "####\n",
            "val loss: 0.725613\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 55/1000\n",
            "train loss: 0.698523\n",
            "####\n",
            "val loss: 0.67288\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 56/1000\n",
            "train loss: 0.669199\n",
            "####\n",
            "val loss: 0.673152\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 57/1000\n",
            "train loss: 0.702548\n",
            "####\n",
            "val loss: 0.69209\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 58/1000\n",
            "train loss: 0.663823\n",
            "####\n",
            "val loss: 0.630746\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 59/1000\n",
            "train loss: 0.672464\n",
            "####\n",
            "val loss: 0.722704\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 60/1000\n",
            "train loss: 0.681095\n",
            "####\n",
            "val loss: 0.649465\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 61/1000\n",
            "train loss: 0.671824\n",
            "####\n",
            "val loss: 0.648272\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 62/1000\n",
            "train loss: 0.668526\n",
            "####\n",
            "val loss: 0.68453\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 63/1000\n",
            "train loss: 0.660785\n",
            "####\n",
            "val loss: 0.645854\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 64/1000\n",
            "train loss: 0.665651\n",
            "####\n",
            "val loss: 0.619457\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 65/1000\n",
            "train loss: 0.660806\n",
            "####\n",
            "val loss: 0.652411\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 66/1000\n",
            "train loss: 0.675629\n",
            "####\n",
            "val loss: 0.641825\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 67/1000\n",
            "train loss: 0.659099\n",
            "####\n",
            "val loss: 0.645898\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 68/1000\n",
            "train loss: 0.661729\n",
            "####\n",
            "val loss: 0.654561\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 69/1000\n",
            "train loss: 0.682242\n",
            "####\n",
            "val loss: 0.652763\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 70/1000\n",
            "train loss: 0.658476\n",
            "####\n",
            "val loss: 0.635429\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 71/1000\n",
            "train loss: 0.656254\n",
            "####\n",
            "val loss: 0.654201\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 72/1000\n",
            "train loss: 0.675585\n",
            "####\n",
            "val loss: 0.663511\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 73/1000\n",
            "train loss: 0.672718\n",
            "####\n",
            "val loss: 0.644366\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 74/1000\n",
            "train loss: 0.673083\n",
            "####\n",
            "val loss: 0.672179\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 75/1000\n",
            "train loss: 0.665288\n",
            "####\n",
            "val loss: 0.659531\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 76/1000\n",
            "train loss: 0.656303\n",
            "####\n",
            "val loss: 0.64869\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 77/1000\n",
            "train loss: 0.658585\n",
            "####\n",
            "val loss: 0.677516\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 78/1000\n",
            "train loss: 0.637445\n",
            "####\n",
            "val loss: 0.665743\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 79/1000\n",
            "train loss: 0.637875\n",
            "####\n",
            "val loss: 0.63143\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 80/1000\n",
            "train loss: 0.648955\n",
            "####\n",
            "val loss: 0.686944\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 81/1000\n",
            "train loss: 0.640288\n",
            "####\n",
            "val loss: 0.640511\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 82/1000\n",
            "train loss: 0.642326\n",
            "####\n",
            "val loss: 0.655672\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 83/1000\n",
            "train loss: 0.656749\n",
            "####\n",
            "val loss: 0.630606\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 84/1000\n",
            "train loss: 0.643661\n",
            "####\n",
            "val loss: 0.644394\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 85/1000\n",
            "train loss: 0.623739\n",
            "####\n",
            "val loss: 0.637112\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 86/1000\n",
            "train loss: 0.624221\n",
            "####\n",
            "val loss: 0.607721\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 87/1000\n",
            "train loss: 0.643154\n",
            "####\n",
            "val loss: 0.621109\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 88/1000\n",
            "train loss: 0.646104\n",
            "####\n",
            "val loss: 0.618736\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 89/1000\n",
            "train loss: 0.634005\n",
            "####\n",
            "val loss: 0.63241\n",
            "###########################\n",
            "######## NEW EPOCH ########\n",
            "###########################\n",
            "epoch: 90/1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LnKitgx4dCi",
        "colab_type": "text"
      },
      "source": [
        "###Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIIqzmN-4jqR",
        "colab_type": "text"
      },
      "source": [
        "####Eval on Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPKGInk04i9Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "outputId": "fbf2b874-3f9b-4c40-e324-8ce0751ece88"
      },
      "source": [
        "\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "network = DeepLabV3(\"eval_val\", project_dir=datasetPath).cuda()\n",
        "network.load_state_dict(torch.load(datasetPath+\"/pretrained_models/model_13_2_2_2_epoch_580.pth\"))\n",
        "\n",
        "val_dataset = DatasetVal(cityscapes_data_path, cityscapes_meta_path)\n",
        "\n",
        "num_val_batches = int(len(val_dataset)/batch_size)\n",
        "print (\"num_val_batches:\", num_val_batches)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                         batch_size=batch_size, shuffle=False,\n",
        "                                         num_workers=1)\n",
        "\n",
        "with open(cityscapes_meta_path+\"/class_weights.pkl\", \"rb\") as file: # (needed for python3)\n",
        "    class_weights = np.array(pickle.load(file))\n",
        "class_weights = torch.from_numpy(class_weights)\n",
        "class_weights = Variable(class_weights.type(torch.FloatTensor)).cuda()\n",
        "\n",
        "# loss function\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "network.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n",
        "batch_losses = []\n",
        "for step, (imgs, label_imgs, img_ids) in enumerate(val_loader):\n",
        "    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n",
        "        imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n",
        "        label_imgs = Variable(label_imgs.type(torch.LongTensor)).cuda() # (shape: (batch_size, img_h, img_w))\n",
        "\n",
        "        outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n",
        "\n",
        "        # compute the loss:\n",
        "        loss = loss_fn(outputs, label_imgs)\n",
        "        loss_value = loss.data.cpu().numpy()\n",
        "        batch_losses.append(loss_value)\n",
        "\n",
        "        ########################################################################\n",
        "        # save data for visualization:\n",
        "        ########################################################################\n",
        "        outputs = outputs.data.cpu().numpy() # (shape: (batch_size, num_classes, img_h, img_w))\n",
        "        pred_label_imgs = np.argmax(outputs, axis=1) # (shape: (batch_size, img_h, img_w))\n",
        "        pred_label_imgs = pred_label_imgs.astype(np.uint8)\n",
        "\n",
        "        for i in range(pred_label_imgs.shape[0]):\n",
        "            if i == 0:\n",
        "                pred_label_img = pred_label_imgs[i] # (shape: (img_h, img_w))\n",
        "                img_id = img_ids[i]\n",
        "                img = imgs[i] # (shape: (3, img_h, img_w))\n",
        "\n",
        "                img = img.data.cpu().numpy()\n",
        "                img = np.transpose(img, (1, 2, 0)) # (shape: (img_h, img_w, 3))\n",
        "                img = img*np.array([0.229, 0.224, 0.225])\n",
        "                img = img + np.array([0.485, 0.456, 0.406])\n",
        "                img = img*255.0\n",
        "                img = img.astype(np.uint8)\n",
        "\n",
        "                pred_label_img_color = label_img_to_color(pred_label_img)\n",
        "                overlayed_img = 0.35*img + 0.65*pred_label_img_color\n",
        "                overlayed_img = overlayed_img.astype(np.uint8)\n",
        "\n",
        "                cv2.imwrite(network.model_dir + \"/\" + img_id + \"_overlayed.png\", overlayed_img)\n",
        "\n",
        "val_loss = np.mean(batch_losses)\n",
        "print (\"val loss: %g\" % val_loss)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pretrained resnet, 50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-aaa2e103334c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepLabV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eval_val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasetPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetPath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/pretrained_models/model_13_2_2_2_epoch_580.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetVal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcityscapes_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcityscapes_meta_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 839\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DeepLabV3:\n\tMissing key(s) in state_dict: \"resnet.resnet.4.0.conv3.weight\", \"resnet.resnet.4.0.bn3.weight\", \"resnet.resnet.4.0.bn3.bias\", \"resnet.resnet.4.0.bn3.running_mean\", \"resnet.resnet.4.0.bn3.running_var\", \"resnet.resnet.4.0.downsample.0.weight\", \"resnet.resnet.4.0.downsample.1.weight\", \"resnet.resnet.4.0.downsample.1.bias\", \"resnet.resnet.4.0.downsample.1.running_mean\", \"resnet.resnet.4.0.downsample.1.running_var\", \"resnet.resnet.4.1.conv3.weight\", \"resnet.resnet.4.1.bn3.weight\", \"resnet.resnet.4.1.bn3.bias\", \"resnet.resnet.4.1.bn3.running_mean\", \"resnet.resnet.4.1.bn3.running_var\", \"resnet.resnet.4.2.conv1.weight\", \"resnet.resnet.4.2.bn1.weight\", \"resnet.resnet.4.2.bn1.bias\", \"resnet.resnet.4.2.bn1.running_mean\", \"resnet.resnet.4.2.bn1.running_var\", \"resnet.resnet.4.2.conv2.weight\", \"resnet.resnet.4.2.bn2.weight\", \"resnet.resnet.4.2.bn2.bias\", \"resnet.resnet.4.2.bn2.running_mean\", \"resnet.resnet.4.2.bn2.running_var\", \"resnet.resnet.4.2.conv3.weight\", \"resnet.resnet.4.2.bn3.weight\", \"resnet.resnet.4.2.bn3.bias\", \"resnet.resnet.4.2.bn3.running_mean\", \"resnet.resnet.4.2.bn3.running_var\", \"resnet.resnet.5.0.conv3.weight\", \"resnet.resnet.5.0.bn3.weight\", \"resnet.resnet.5.0.bn3.bias\", \"resnet.resnet.5.0.bn3.running_mean\", \"resnet.resnet.5.0.bn3.running_var\", \"resnet.resnet.5.1.conv3.weight\", \"resnet.resnet.5.1.bn3.weight\", \"resnet.resnet.5.1.bn3.bias\", \"resnet.resnet.5.1.bn3.running_mean\", \"resnet.resnet.5.1.bn3.running_var\", \"resnet.resnet.5.2.conv1.weight\", \"resnet.resnet.5.2.bn...\n\tUnexpected key(s) in state_dict: \"resnet.layer4.0.conv1.weight\", \"resnet.layer4.0.bn1.weight\", \"resnet.layer4.0.bn1.bias\", \"resnet.layer4.0.bn1.running_mean\", \"resnet.layer4.0.bn1.running_var\", \"resnet.layer4.0.conv2.weight\", \"resnet.layer4.0.bn2.weight\", \"resnet.layer4.0.bn2.bias\", \"resnet.layer4.0.bn2.running_mean\", \"resnet.layer4.0.bn2.running_var\", \"resnet.layer4.0.downsample.0.weight\", \"resnet.layer4.0.downsample.1.weight\", \"resnet.layer4.0.downsample.1.bias\", \"resnet.layer4.0.downsample.1.running_mean\", \"resnet.layer4.0.downsample.1.running_var\", \"resnet.layer4.1.conv1.weight\", \"resnet.layer4.1.bn1.weight\", \"resnet.layer4.1.bn1.bias\", \"resnet.layer4.1.bn1.running_mean\", \"resnet.layer4.1.bn1.running_var\", \"resnet.layer4.1.conv2.weight\", \"resnet.layer4.1.bn2.weight\", \"resnet.layer4.1.bn2.bias\", \"resnet.layer4.1.bn2.running_mean\", \"resnet.layer4.1.bn2.running_var\". \n\tsize mismatch for resnet.resnet.4.0.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n\tsize mismatch for resnet.resnet.4.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for resnet.resnet.5.0.conv1.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for resnet.resnet.5.0.downsample.0.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for resnet.resnet.5.0.downsample.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.resnet.5.0.downsample.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.resnet.5.0.downsample.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.resnet.5.0.downsample.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.resnet.5.1.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for resnet.layer5.0.conv1.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 1, 1]).\n\tsize mismatch for resnet.layer5.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for resnet.layer5.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer5.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer5.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer5.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer5.1.conv1.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 2048, 1, 1]).\n\tsize mismatch for aspp.conv_1x1_1.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 2048, 1, 1]).\n\tsize mismatch for aspp.conv_3x3_1.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 2048, 3, 3]).\n\tsize mismatch for aspp.conv_3x3_2.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 2048, 3, 3]).\n\tsize mismatch for aspp.conv_3x3_3.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 2048, 3, 3]).\n\tsize mismatch for aspp.conv_1x1_2.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 2048, 1, 1])."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWkMA7Qv4uww",
        "colab_type": "text"
      },
      "source": [
        "####Eval for Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "512GO4Vh4yl9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "outputId": "3ca71468-22e4-454a-ed19-dafc4907aae0"
      },
      "source": [
        "\n",
        "\n",
        "trainId_to_id = {\n",
        "    0: 7,\n",
        "    1: 8,\n",
        "    2: 11,\n",
        "    3: 12,\n",
        "    4: 13,\n",
        "    5: 17,\n",
        "    6: 19,\n",
        "    7: 20,\n",
        "    8: 21,\n",
        "    9: 22,\n",
        "    10: 23,\n",
        "    11: 24,\n",
        "    12: 25,\n",
        "    13: 26,\n",
        "    14: 27,\n",
        "    15: 28,\n",
        "    16: 31,\n",
        "    17: 32,\n",
        "    18: 33,\n",
        "    19: 0\n",
        "}\n",
        "trainId_to_id_map_func = np.vectorize(trainId_to_id.get)\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "network = DeepLabV3(\"eval_val_for_metrics\", project_dir=datasetPath).cuda()\n",
        "network.load_state_dict(torch.load(datasetPath+\"/pretrained_models/model_13_2_2_2_epoch_580.pth\"))\n",
        "\n",
        "val_dataset = DatasetVal(cityscapes_data_path,cityscapes_meta_path)\n",
        "\n",
        "num_val_batches = int(len(val_dataset)/batch_size)\n",
        "print (\"num_val_batches:\", num_val_batches)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                         batch_size=batch_size, shuffle=False,\n",
        "                                         num_workers=1)\n",
        "\n",
        "with open(cityscapes_meta_path+\"/class_weights.pkl\", \"rb\") as file: # (needed for python3)\n",
        "    class_weights = np.array(pickle.load(file))\n",
        "class_weights = torch.from_numpy(class_weights)\n",
        "class_weights = Variable(class_weights.type(torch.FloatTensor)).cuda()\n",
        "\n",
        "# loss function\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "network.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n",
        "batch_losses = []\n",
        "for step, (imgs, label_imgs, img_ids) in enumerate(val_loader):\n",
        "    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n",
        "        imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n",
        "        label_imgs = Variable(label_imgs.type(torch.LongTensor)).cuda() # (shape: (batch_size, img_h, img_w))\n",
        "\n",
        "        outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n",
        "\n",
        "        # compute the loss:\n",
        "        loss = loss_fn(outputs, label_imgs)\n",
        "        loss_value = loss.data.cpu().numpy()\n",
        "        batch_losses.append(loss_value)\n",
        "\n",
        "        ########################################################################\n",
        "        # save data for visualization:\n",
        "        ########################################################################\n",
        "        outputs = F.upsample(outputs, size=(1024, 2048), mode=\"bilinear\") # (shape: (batch_size, num_classes, 1024, 2048))\n",
        "\n",
        "        outputs = outputs.data.cpu().numpy() # (shape: (batch_size, num_classes, 1024, 2048))\n",
        "        pred_label_imgs = np.argmax(outputs, axis=1) # (shape: (batch_size, 1024, 2048))\n",
        "        pred_label_imgs = pred_label_imgs.astype(np.uint8)\n",
        "\n",
        "        for i in range(pred_label_imgs.shape[0]):\n",
        "            pred_label_img = pred_label_imgs[i] # (shape: (1024, 2048))\n",
        "            img_id = img_ids[i]\n",
        "\n",
        "            # convert pred_label_img from trainId to id pixel values:\n",
        "            pred_label_img = trainId_to_id_map_func(pred_label_img) # (shape: (1024, 2048))\n",
        "            pred_label_img = pred_label_img.astype(np.uint8)\n",
        "\n",
        "            cv2.imwrite(network.model_dir + \"/\" + img_id + \"_pred_label_img.png\", pred_label_img)\n",
        "\n",
        "val_loss = np.mean(batch_losses)\n",
        "print (\"val loss: %g\" % val_loss)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pretrained resnet, 50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-6a5daee58da3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepLabV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eval_val_for_metrics\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasetPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetPath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/pretrained_models/model_13_2_2_2_epoch_580.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetVal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcityscapes_data_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcityscapes_meta_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 839\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DeepLabV3:\n\tMissing key(s) in state_dict: \"resnet.resnet.4.0.conv3.weight\", \"resnet.resnet.4.0.bn3.weight\", \"resnet.resnet.4.0.bn3.bias\", \"resnet.resnet.4.0.bn3.running_mean\", \"resnet.resnet.4.0.bn3.running_var\", \"resnet.resnet.4.0.downsample.0.weight\", \"resnet.resnet.4.0.downsample.1.weight\", \"resnet.resnet.4.0.downsample.1.bias\", \"resnet.resnet.4.0.downsample.1.running_mean\", \"resnet.resnet.4.0.downsample.1.running_var\", \"resnet.resnet.4.1.conv3.weight\", \"resnet.resnet.4.1.bn3.weight\", \"resnet.resnet.4.1.bn3.bias\", \"resnet.resnet.4.1.bn3.running_mean\", \"resnet.resnet.4.1.bn3.running_var\", \"resnet.resnet.4.2.conv1.weight\", \"resnet.resnet.4.2.bn1.weight\", \"resnet.resnet.4.2.bn1.bias\", \"resnet.resnet.4.2.bn1.running_mean\", \"resnet.resnet.4.2.bn1.running_var\", \"resnet.resnet.4.2.conv2.weight\", \"resnet.resnet.4.2.bn2.weight\", \"resnet.resnet.4.2.bn2.bias\", \"resnet.resnet.4.2.bn2.running_mean\", \"resnet.resnet.4.2.bn2.running_var\", \"resnet.resnet.4.2.conv3.weight\", \"resnet.resnet.4.2.bn3.weight\", \"resnet.resnet.4.2.bn3.bias\", \"resnet.resnet.4.2.bn3.running_mean\", \"resnet.resnet.4.2.bn3.running_var\", \"resnet.resnet.5.0.conv3.weight\", \"resnet.resnet.5.0.bn3.weight\", \"resnet.resnet.5.0.bn3.bias\", \"resnet.resnet.5.0.bn3.running_mean\", \"resnet.resnet.5.0.bn3.running_var\", \"resnet.resnet.5.1.conv3.weight\", \"resnet.resnet.5.1.bn3.weight\", \"resnet.resnet.5.1.bn3.bias\", \"resnet.resnet.5.1.bn3.running_mean\", \"resnet.resnet.5.1.bn3.running_var\", \"resnet.resnet.5.2.conv1.weight\", \"resnet.resnet.5.2.bn...\n\tUnexpected key(s) in state_dict: \"resnet.layer4.0.conv1.weight\", \"resnet.layer4.0.bn1.weight\", \"resnet.layer4.0.bn1.bias\", \"resnet.layer4.0.bn1.running_mean\", \"resnet.layer4.0.bn1.running_var\", \"resnet.layer4.0.conv2.weight\", \"resnet.layer4.0.bn2.weight\", \"resnet.layer4.0.bn2.bias\", \"resnet.layer4.0.bn2.running_mean\", \"resnet.layer4.0.bn2.running_var\", \"resnet.layer4.0.downsample.0.weight\", \"resnet.layer4.0.downsample.1.weight\", \"resnet.layer4.0.downsample.1.bias\", \"resnet.layer4.0.downsample.1.running_mean\", \"resnet.layer4.0.downsample.1.running_var\", \"resnet.layer4.1.conv1.weight\", \"resnet.layer4.1.bn1.weight\", \"resnet.layer4.1.bn1.bias\", \"resnet.layer4.1.bn1.running_mean\", \"resnet.layer4.1.bn1.running_var\", \"resnet.layer4.1.conv2.weight\", \"resnet.layer4.1.bn2.weight\", \"resnet.layer4.1.bn2.bias\", \"resnet.layer4.1.bn2.running_mean\", \"resnet.layer4.1.bn2.running_var\". \n\tsize mismatch for resnet.resnet.4.0.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n\tsize mismatch for resnet.resnet.4.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for resnet.resnet.5.0.conv1.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for resnet.resnet.5.0.downsample.0.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for resnet.resnet.5.0.downsample.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.resnet.5.0.downsample.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.resnet.5.0.downsample.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.resnet.5.0.downsample.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.resnet.5.1.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for resnet.layer5.0.conv1.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 1, 1]).\n\tsize mismatch for resnet.layer5.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for resnet.layer5.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer5.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer5.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer5.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer5.1.conv1.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 2048, 1, 1]).\n\tsize mismatch for aspp.conv_1x1_1.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 2048, 1, 1]).\n\tsize mismatch for aspp.conv_3x3_1.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 2048, 3, 3]).\n\tsize mismatch for aspp.conv_3x3_2.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 2048, 3, 3]).\n\tsize mismatch for aspp.conv_3x3_3.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 2048, 3, 3]).\n\tsize mismatch for aspp.conv_1x1_2.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 2048, 1, 1])."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM3GWpVd5o5w",
        "colab_type": "text"
      },
      "source": [
        "###Visualisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQMhU27R5tS5",
        "colab_type": "text"
      },
      "source": [
        "####Run on Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQyJ3Snf5oM_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "outputId": "cee4efe1-f3cf-422f-fa86-7e112f0c85c7"
      },
      "source": [
        "\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "network = DeepLabV3(\"eval_seq\", project_dir=datasetPath).cuda()\n",
        "network.load_state_dict(torch.load(datasetPath+\"/pretrained_models/model_13_2_2_2_epoch_580.pth\"))\n",
        "\n",
        "for sequence in [\"00\", \"01\", \"02\"]:\n",
        "    print (sequence)\n",
        "\n",
        "    val_dataset = DatasetSeq(cityscapes_data_path, cityscapes_meta_path, sequence=sequence)\n",
        "\n",
        "    num_val_batches = int(len(val_dataset)/batch_size)\n",
        "    print (\"num_val_batches:\", num_val_batches)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                             batch_size=batch_size, shuffle=False,\n",
        "                                             num_workers=1)\n",
        "\n",
        "    network.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n",
        "    unsorted_img_ids = []\n",
        "    for step, (imgs, img_ids) in enumerate(val_loader):\n",
        "        with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n",
        "            imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n",
        "\n",
        "            outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n",
        "\n",
        "            ####################################################################\n",
        "            # save data for visualization:\n",
        "            ####################################################################\n",
        "            outputs = outputs.data.cpu().numpy() # (shape: (batch_size, num_classes, img_h, img_w))\n",
        "            pred_label_imgs = np.argmax(outputs, axis=1) # (shape: (batch_size, img_h, img_w))\n",
        "            pred_label_imgs = pred_label_imgs.astype(np.uint8)\n",
        "\n",
        "            for i in range(pred_label_imgs.shape[0]):\n",
        "                pred_label_img = pred_label_imgs[i] # (shape: (img_h, img_w))\n",
        "                img_id = img_ids[i]\n",
        "                img = imgs[i] # (shape: (3, img_h, img_w))\n",
        "\n",
        "                img = img.data.cpu().numpy()\n",
        "                img = np.transpose(img, (1, 2, 0)) # (shape: (img_h, img_w, 3))\n",
        "                img = img*np.array([0.229, 0.224, 0.225])\n",
        "                img = img + np.array([0.485, 0.456, 0.406])\n",
        "                img = img*255.0\n",
        "                img = img.astype(np.uint8)\n",
        "\n",
        "                pred_label_img_color = label_img_to_color(pred_label_img)\n",
        "                overlayed_img = 0.35*img + 0.65*pred_label_img_color\n",
        "                overlayed_img = overlayed_img.astype(np.uint8)\n",
        "\n",
        "                img_h = overlayed_img.shape[0]\n",
        "                img_w = overlayed_img.shape[1]\n",
        "\n",
        "                cv2.imwrite(network.model_dir + \"/\" + img_id + \".png\", img)\n",
        "                cv2.imwrite(network.model_dir + \"/\" + img_id + \"_pred.png\", pred_label_img_color)\n",
        "                cv2.imwrite(network.model_dir + \"/\" + img_id + \"_overlayed.png\", overlayed_img)\n",
        "\n",
        "                unsorted_img_ids.append(img_id)\n",
        "\n",
        "    ############################################################################\n",
        "    # create visualization video:\n",
        "    ############################################################################\n",
        "    out = cv2.VideoWriter(\"%s/stuttgart_%s_combined.avi\" % (network.model_dir, sequence), cv2.VideoWriter_fourcc(*\"MJPG\"), 20, (2*img_w, 2*img_h))\n",
        "    sorted_img_ids = sorted(unsorted_img_ids)\n",
        "    for img_id in sorted_img_ids:\n",
        "        img = cv2.imread(network.model_dir + \"/\" + img_id + \".png\", -1)\n",
        "        pred_img = cv2.imread(network.model_dir + \"/\" + img_id + \"_pred.png\", -1)\n",
        "        overlayed_img = cv2.imread(network.model_dir + \"/\" + img_id + \"_overlayed.png\", -1)\n",
        "\n",
        "        combined_img = np.zeros((2*img_h, 2*img_w, 3), dtype=np.uint8)\n",
        "\n",
        "        combined_img[0:img_h, 0:img_w] = img\n",
        "        combined_img[0:img_h, img_w:(2*img_w)] = pred_img\n",
        "        combined_img[img_h:(2*img_h), (int(img_w/2)):(img_w + int(img_w/2))] = overlayed_img\n",
        "\n",
        "        out.write(combined_img)\n",
        "\n",
        "    out.release()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pretrained resnet, 50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-a9936bb68a98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepLabV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eval_seq\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasetPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetPath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/pretrained_models/model_13_2_2_2_epoch_580.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"00\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"01\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"02\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 839\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DeepLabV3:\n\tMissing key(s) in state_dict: \"resnet.resnet.4.0.conv3.weight\", \"resnet.resnet.4.0.bn3.weight\", \"resnet.resnet.4.0.bn3.bias\", \"resnet.resnet.4.0.bn3.running_mean\", \"resnet.resnet.4.0.bn3.running_var\", \"resnet.resnet.4.0.downsample.0.weight\", \"resnet.resnet.4.0.downsample.1.weight\", \"resnet.resnet.4.0.downsample.1.bias\", \"resnet.resnet.4.0.downsample.1.running_mean\", \"resnet.resnet.4.0.downsample.1.running_var\", \"resnet.resnet.4.1.conv3.weight\", \"resnet.resnet.4.1.bn3.weight\", \"resnet.resnet.4.1.bn3.bias\", \"resnet.resnet.4.1.bn3.running_mean\", \"resnet.resnet.4.1.bn3.running_var\", \"resnet.resnet.4.2.conv1.weight\", \"resnet.resnet.4.2.bn1.weight\", \"resnet.resnet.4.2.bn1.bias\", \"resnet.resnet.4.2.bn1.running_mean\", \"resnet.resnet.4.2.bn1.running_var\", \"resnet.resnet.4.2.conv2.weight\", \"resnet.resnet.4.2.bn2.weight\", \"resnet.resnet.4.2.bn2.bias\", \"resnet.resnet.4.2.bn2.running_mean\", \"resnet.resnet.4.2.bn2.running_var\", \"resnet.resnet.4.2.conv3.weight\", \"resnet.resnet.4.2.bn3.weight\", \"resnet.resnet.4.2.bn3.bias\", \"resnet.resnet.4.2.bn3.running_mean\", \"resnet.resnet.4.2.bn3.running_var\", \"resnet.resnet.5.0.conv3.weight\", \"resnet.resnet.5.0.bn3.weight\", \"resnet.resnet.5.0.bn3.bias\", \"resnet.resnet.5.0.bn3.running_mean\", \"resnet.resnet.5.0.bn3.running_var\", \"resnet.resnet.5.1.conv3.weight\", \"resnet.resnet.5.1.bn3.weight\", \"resnet.resnet.5.1.bn3.bias\", \"resnet.resnet.5.1.bn3.running_mean\", \"resnet.resnet.5.1.bn3.running_var\", \"resnet.resnet.5.2.conv1.weight\", \"resnet.resnet.5.2.bn...\n\tUnexpected key(s) in state_dict: \"resnet.layer4.0.conv1.weight\", \"resnet.layer4.0.bn1.weight\", \"resnet.layer4.0.bn1.bias\", \"resnet.layer4.0.bn1.running_mean\", \"resnet.layer4.0.bn1.running_var\", \"resnet.layer4.0.conv2.weight\", \"resnet.layer4.0.bn2.weight\", \"resnet.layer4.0.bn2.bias\", \"resnet.layer4.0.bn2.running_mean\", \"resnet.layer4.0.bn2.running_var\", \"resnet.layer4.0.downsample.0.weight\", \"resnet.layer4.0.downsample.1.weight\", \"resnet.layer4.0.downsample.1.bias\", \"resnet.layer4.0.downsample.1.running_mean\", \"resnet.layer4.0.downsample.1.running_var\", \"resnet.layer4.1.conv1.weight\", \"resnet.layer4.1.bn1.weight\", \"resnet.layer4.1.bn1.bias\", \"resnet.layer4.1.bn1.running_mean\", \"resnet.layer4.1.bn1.running_var\", \"resnet.layer4.1.conv2.weight\", \"resnet.layer4.1.bn2.weight\", \"resnet.layer4.1.bn2.bias\", \"resnet.layer4.1.bn2.running_mean\", \"resnet.layer4.1.bn2.running_var\". \n\tsize mismatch for resnet.resnet.4.0.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1]).\n\tsize mismatch for resnet.resnet.4.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 256, 1, 1]).\n\tsize mismatch for resnet.resnet.5.0.conv1.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 256, 1, 1]).\n\tsize mismatch for resnet.resnet.5.0.downsample.0.weight: copying a param with shape torch.Size([128, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([512, 256, 1, 1]).\n\tsize mismatch for resnet.resnet.5.0.downsample.1.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.resnet.5.0.downsample.1.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.resnet.5.0.downsample.1.running_mean: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.resnet.5.0.downsample.1.running_var: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for resnet.resnet.5.1.conv1.weight: copying a param with shape torch.Size([128, 128, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 512, 1, 1]).\n\tsize mismatch for resnet.layer5.0.conv1.weight: copying a param with shape torch.Size([512, 256, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 1024, 1, 1]).\n\tsize mismatch for resnet.layer5.0.downsample.0.weight: copying a param with shape torch.Size([512, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([2048, 1024, 1, 1]).\n\tsize mismatch for resnet.layer5.0.downsample.1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer5.0.downsample.1.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer5.0.downsample.1.running_mean: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer5.0.downsample.1.running_var: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([2048]).\n\tsize mismatch for resnet.layer5.1.conv1.weight: copying a param with shape torch.Size([512, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([512, 2048, 1, 1]).\n\tsize mismatch for aspp.conv_1x1_1.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 2048, 1, 1]).\n\tsize mismatch for aspp.conv_3x3_1.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 2048, 3, 3]).\n\tsize mismatch for aspp.conv_3x3_2.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 2048, 3, 3]).\n\tsize mismatch for aspp.conv_3x3_3.weight: copying a param with shape torch.Size([256, 512, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 2048, 3, 3]).\n\tsize mismatch for aspp.conv_1x1_2.weight: copying a param with shape torch.Size([256, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 2048, 1, 1])."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNyZXuEo5vcD",
        "colab_type": "text"
      },
      "source": [
        "####Run on THn seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bd_bZrT5v2K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "d951a628-0b58-444b-d864-e4345927ecaf"
      },
      "source": [
        "batch_size = 2\n",
        "\n",
        "network = DeepLabV3(\"eval_seq_thn\", project_dir=datasetPath).cuda()\n",
        "network.load_state_dict(torch.load(datasetPath+\"/pretrained_models/model_13_2_2_2_epoch_580.pth\"))\n",
        "\n",
        "val_dataset = DatasetThnSeq(thn_data_path=datasetPath+\"/data/thn\")\n",
        "\n",
        "num_val_batches = int(len(val_dataset)/batch_size)\n",
        "print (\"num_val_batches:\", num_val_batches)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                         batch_size=batch_size, shuffle=False,\n",
        "                                         num_workers=1)\n",
        "\n",
        "network.eval() # (set in evaluation mode, this affects BatchNorm and dropout)\n",
        "unsorted_img_ids = []\n",
        "for step, (imgs, img_ids) in enumerate(val_loader):\n",
        "    with torch.no_grad(): # (corresponds to setting volatile=True in all variables, this is done during inference to reduce memory consumption)\n",
        "        imgs = Variable(imgs).cuda() # (shape: (batch_size, 3, img_h, img_w))\n",
        "\n",
        "        outputs = network(imgs) # (shape: (batch_size, num_classes, img_h, img_w))\n",
        "\n",
        "        ########################################################################\n",
        "        # save data for visualization:\n",
        "        ########################################################################\n",
        "        outputs = outputs.data.cpu().numpy() # (shape: (batch_size, num_classes, img_h, img_w))\n",
        "        pred_label_imgs = np.argmax(outputs, axis=1) # (shape: (batch_size, img_h, img_w))\n",
        "        pred_label_imgs = pred_label_imgs.astype(np.uint8)\n",
        "\n",
        "        for i in range(pred_label_imgs.shape[0]):\n",
        "            pred_label_img = pred_label_imgs[i] # (shape: (img_h, img_w))\n",
        "            img_id = img_ids[i]\n",
        "            img = imgs[i] # (shape: (3, img_h, img_w))\n",
        "\n",
        "            img = img.data.cpu().numpy()\n",
        "            img = np.transpose(img, (1, 2, 0)) # (shape: (img_h, img_w, 3))\n",
        "            img = img*np.array([0.229, 0.224, 0.225])\n",
        "            img = img + np.array([0.485, 0.456, 0.406])\n",
        "            img = img*255.0\n",
        "            img = img.astype(np.uint8)\n",
        "\n",
        "            pred_label_img_color = label_img_to_color(pred_label_img)\n",
        "            overlayed_img = 0.35*img + 0.65*pred_label_img_color\n",
        "            overlayed_img = overlayed_img.astype(np.uint8)\n",
        "\n",
        "            img_h = overlayed_img.shape[0]\n",
        "            img_w = overlayed_img.shape[1]\n",
        "\n",
        "            # TODO! do this using network.model_dir instead\n",
        "            cv2.imwrite(network.model_dir + \"/\" + img_id + \".png\", img)\n",
        "            cv2.imwrite(network.model_dir + \"/\" + img_id + \"_pred.png\", pred_label_img_color)\n",
        "            cv2.imwrite(network.model_dir + \"/\" + img_id + \"_overlayed.png\", overlayed_img)\n",
        "\n",
        "            unsorted_img_ids.append(img_id)\n",
        "\n",
        "################################################################################\n",
        "# create visualization video:\n",
        "################################################################################\n",
        "out = cv2.VideoWriter(\"%s/thn_combined.avi\" % network.model_dir, cv2.VideoWriter_fourcc(*\"MJPG\"), 12, (2*img_w, 2*img_h))\n",
        "sorted_img_ids = sorted(unsorted_img_ids)\n",
        "for img_id in sorted_img_ids:\n",
        "    img = cv2.imread(network.model_dir + \"/\" + img_id + \".png\", -1)\n",
        "    pred_img = cv2.imread(network.model_dir + \"/\" + img_id + \"_pred.png\", -1)\n",
        "    overlayed_img = cv2.imread(network.model_dir + \"/\" + img_id + \"_overlayed.png\", -1)\n",
        "\n",
        "    combined_img = np.zeros((2*img_h, 2*img_w, 3), dtype=np.uint8)\n",
        "\n",
        "    combined_img[0:img_h, 0:img_w] = img\n",
        "    combined_img[0:img_h, img_w:(2*img_w)] = pred_img\n",
        "    combined_img[img_h:(2*img_h), (int(img_w/2)):(img_w + int(img_w/2))] = overlayed_img\n",
        "\n",
        "    out.write(combined_img)\n",
        "\n",
        "out.release()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pretrained resnet, 18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-e2362df1f1de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasetPath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/pretrained_models/model_13_2_2_2_epoch_580.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetThnSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthn_data_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasetPath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/data/thn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnum_val_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-bb7b5cd1cc0f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, thn_data_path)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mfile_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mimg_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/U_Net_Datasets/data/thn/'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9e91NXYz-mt",
        "colab_type": "text"
      },
      "source": [
        "#U-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdMb4P-JBs4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hioxs_xWBAKD",
        "colab_type": "text"
      },
      "source": [
        "###Blabla Über das Datenset \n",
        "wer warum woraus wozu usw\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iAit3H43tqd",
        "colab_type": "text"
      },
      "source": [
        "### Benötigte Bibliotheken Importieren\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuMdGpfm6n97",
        "colab_type": "code",
        "outputId": "5d97010d-ac09-4ae9-cf64-c048a3646fbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZAcRajt3rl8",
        "colab_type": "code",
        "outputId": "682c75e1-9139-48c3-a257-6ff3198fcf21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "\n",
        "# Quelle https://www.kaggle.com/keegil/keras-u-net-starter-lb-0-277\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from itertools import chain\n",
        "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
        "from skimage.transform import resize\n",
        "from skimage.morphology import label\n",
        "\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Dropout, Lambda\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Set some parameters\n",
        "IMG_WIDTH = 128\n",
        "#IMG_WIDTH = 2048\n",
        "IMG_HEIGHT = 128\n",
        "#IMG_HEIGHT= 1024\n",
        "IMG_CHANNELS = 3 \n",
        "\n",
        "\n",
        "TRAIN_PATH = '/content/drive/My Drive/U_Net_Datasets/gtFine_trainvaltest/gtFine_copy'\n",
        "TEST_PATH = '/content/drive/My Drive/U_Net_Datasets/gtFine_trainvaltest/gtFine_test'\n",
        "\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='skimage')\n",
        "seed = 42\n",
        "random.seed = seed\n",
        "np.random.seed = seed"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AYBFOThXx6_",
        "colab_type": "text"
      },
      "source": [
        "## Cityscapes Dataset gtFine_trainvaltest einmalig abändern \n",
        "Abändern der Datenstruktur und der Namensgebung\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4wbRr6dXxPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code der Einmalig ausgeführt wird um Dataset Anzupassen \n",
        "\n",
        "\n",
        "\n",
        "#import zipfile\n",
        "#with zipfile.ZipFile(\"/content/drive/My Drive/U_Net_Datasets/gtFine_trainvaltest.zip\",\"r\") as zip_ref:\n",
        "#    zip_ref.extractall(\"/content/drive/My Drive/U_Net_Datasets/gtFine_trainvaltest\")\n",
        "\n",
        "#import os\n",
        "#import shutil\n",
        "#filePath = '/content/drive/My Drive/U_Net_Datasets/gtFine_trainvaltest/gtFine/train'\n",
        "\n",
        "#ldseg= np.array(os.listdir(filePath))\n",
        "#ldseg\n",
        "\n",
        "#for i in ldseg:\n",
        " #print(np.array(os.listdir('/content/drive/My Drive/U_Net_Datasets/gtFine_trainvaltest/gtFine/train/'+i))) #test\n",
        " #source = np.array(os.listdir(filePath +'/'+ i))\n",
        " #print(\"NEXT\")\n",
        " #index = 0\n",
        " #for a in source:\n",
        "  #Umbenenen \n",
        "  #print (a) \n",
        "  #src = filePath + '/' + i + '/' +a\n",
        "  #dst_Items = src.split('_')\n",
        "  #indexStr =  '_'+str(index) + '_'\n",
        "  \n",
        "  #dst = dst_Items[0] + '_'\n",
        "  #dst = dst +dst_Items[1] + '_'\n",
        "  #dst = dst +dst_Items[2] + '_'\n",
        "  #dst = dst +dst_Items[3] \n",
        "  ##dst = dst +src[4] + '_'  \n",
        "  #dst = dst + indexStr \n",
        "  #dst = dst + dst_Items[6] + '_'\n",
        "  #dst = dst + dst_Items[7] \n",
        "  #index += 1\n",
        "  #os.rename(str(src),dst)\n",
        "\n",
        "\n",
        "#Testing \n",
        " #print(dst) \n",
        " #print(src)\n",
        "#len(dst)\n",
        "#/content/drive/My Drive/U_Net_Datasets/gtFine_trainvaltest/gtFine/train/jena/jena_000020_000019_gtFine_labelIds.png\n",
        "#/content/drive/My Drive/U_Net_Datasets/gtFine_trainvaltest/gtFine/train/jena/jena/_0_gtFine_labelIds.png\n",
        " \n",
        "#src_folder = os.listdir('/content/drive/My Drive/U_Net_Datasets/gtFine_trainvaltest/gtFine/train/')\n",
        "#for folder in src_folder:\n",
        "#  print(src_folder)\n",
        "#  src_file = os.listdir('/content/drive/My Drive/U_Net_Datasets/gtFine_trainvaltest/gtFine/train/'+ folder)\n",
        "#  for file in src_file:\n",
        "#    full_file_name = os.path.join('/content/drive/My Drive/U_Net_Datasets/gtFine_trainvaltest/gtFine/train/'+ folder, file)\n",
        "#    if os.path.isfile(full_file_name):\n",
        "#      #print(full_file_name)\n",
        "#      shutil.copy(full_file_name, '/content/drive/My Drive/U_Net_Datasets/gtFine_trainvaltest/gtFine_copy')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSr4yztNgwl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#src_folder = os.listdir('/content/drive/My Drive/U_Net_Datasets/gtFine_trainvaltest/gtFine/test/')\n",
        "#for folder in src_folder:\n",
        "#  print(src_folder)\n",
        "#  src_file = os.listdir('/content/drive/My Drive/U_Net_Datasets/gtFine_trainvaltest/gtFine/test/'+ folder)\n",
        "#  for file in src_file:\n",
        "#    full_file_name = os.path.join('/content/drive/My Drive/U_Net_Datasets/gtFine_trainvaltest/gtFine/test/'+ folder, file)\n",
        "#    if os.path.isfile(full_file_name):\n",
        "#      #print(full_file_name)\n",
        "#      shutil.copy(full_file_name, '/content/drive/My Drive/U_Net_Datasets/gtFine_trainvaltest/gtFine_test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qju_F0y4SHK",
        "colab_type": "text"
      },
      "source": [
        "#### Trainings und Test ID´s erstellen\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5urkadSX4rml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get train and test IDs\n",
        "train_ids = next(os.walk(TRAIN_PATH))[2]\n",
        "test_ids = next(os.walk(TEST_PATH))[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHobIUUw4a4k",
        "colab_type": "text"
      },
      "source": [
        "### Daten holen\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40WdRYrwlAmC",
        "colab_type": "code",
        "outputId": "e10bbc67-f3fa-4890-d0dd-651274fcda00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "X_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
        "Y_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
        "X_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f586692874bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_HEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_WIDTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_CHANNELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_HEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_WIDTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_HEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_WIDTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_CHANNELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7Ryql6w4fIX",
        "colab_type": "code",
        "outputId": "ed408b31-4a53-4d0b-b467-20d733af4f7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        }
      },
      "source": [
        "# Get and resize train images and masks\n",
        "print('Getting and resizing train images and masks ... ')\n",
        "sys.stdout.flush()\n",
        "for n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):    \n",
        "    path = TRAIN_PATH +'/'+ id_ \n",
        "    imgList = np.array(os.listdir(TRAIN_PATH))\n",
        "    for image in imgList:\n",
        "      if np.isin('.png',image):\n",
        "        #img = imread(path)[:,:,IMG_CHANNELS]\n",
        "        img = np.array(imageio.imread(path), dtype=np.uint8)\n",
        "        img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
        "        X_train[n] = img\n",
        "        mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
        "        for mask_file in next(os.walk(path + '/masks/'))[2]: \n",
        "          mask_ = imread(path + '/masks/' + mask_file)\n",
        "          mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True), axis=-1)\n",
        "          mask = np.maximum(mask, mask_)\n",
        "          Y_train[n] = mask\n",
        "print('resize train images and masks Done!')\n",
        "\n",
        "#image = np.array(imageio.imread(image_path), dtype=np.uint8)\n",
        "#mask = np.array(imageio.imread(mask_path), dtype=np.uint8)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting and resizing train images and masks ... \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|▍         | 487/11900 [02:03<47:59,  3.96it/s]ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-3-fc710ed5a32a>\", line 9, in <module>\n",
            "    if np.isin('.png',image):\n",
            "  File \"<__array_function__ internals>\", line 6, in isin\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py\", line 696, in isin\n",
            "    invert=invert).reshape(element.shape)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 1452, in getframeinfo\n",
            "    lines, lnum = findsource(frame)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 170, in findsource\n",
            "    file = getsourcefile(object) or getfile(object)\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
            "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
            "  File \"/usr/lib/python3.6/inspect.py\", line 742, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 395, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 429, in _joinrealpath\n",
            "    if not islink(newpath):\n",
            "  File \"/usr/lib/python3.6/posixpath.py\", line 171, in islink\n",
            "    st = os.lstat(path)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vC9yblclpL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sizes_test = [[],[]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W97-xPWNUVEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get and resize test images\n",
        "\n",
        "\n",
        "print('Getting and resizing test images ... ')\n",
        "sys.stdout.flush()\n",
        "for n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n",
        "    path = TEST_PATH + '/'+ id_\n",
        "    #imgList = np.array(os.listdir(path))\n",
        "    for image in imgList:\n",
        "      if np.isin('.png',image):               \n",
        "        #image = str('/' + image)\n",
        "        img = imread(path)[:,:,:IMG_CHANNELS]\n",
        "        sizes_test.append([img.shape[0], img.shape[1]])\n",
        "        img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
        "        X_test[n] = img\n",
        "print('resize test images and masks Done!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRVX3JW849-G",
        "colab_type": "code",
        "outputId": "ce84e3fb-a8f3-4b10-c7be-54e44fad5a46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        }
      },
      "source": [
        "# Check if training data looks all right\n",
        "ix = random.randint(0, len(train_ids))\n",
        "imshow(X_train[ix])\n",
        "plt.show()\n",
        "imshow(np.squeeze(Y_train[ix]))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR4AAAEYCAYAAACKkJnLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR4AAAEYCAYAAACKkJnLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUf433KO5VCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This implementation is most definitely\n",
        "# not correct due to the very large \n",
        "# discrepancy between the results reported\n",
        "# here and the LB results.\n",
        "# It also seems to just increase over \n",
        "# time no matter what when you train ...\n",
        "# Define IoU metric\n",
        "def mean_iou(y_true, y_pred):\n",
        "    prec = []\n",
        "    for t in np.arange(0.5, 1.0, 0.05):\n",
        "        y_pred_ = tf.to_int32(y_pred > t)\n",
        "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
        "        K.get_session().run(tf.local_variables_initializer())\n",
        "        with tf.control_dependencies([up_opt]):\n",
        "            score = tf.identity(score)\n",
        "        prec.append(score)\n",
        "    return K.mean(K.stack(prec), axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUwsyONF5ip7",
        "colab_type": "text"
      },
      "source": [
        "###Erstellen eines neuronalen Netzes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtF6BQfc5oSo",
        "colab_type": "code",
        "outputId": "4856d518-838d-46e8-dfcd-863d3f1f1cdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Build U-Net model\n",
        "inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "s = Lambda(lambda x: x / 255) (inputs)\n",
        "\n",
        "c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\n",
        "c1 = Dropout(0.1) (c1)\n",
        "c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\n",
        "p1 = MaxPooling2D((2, 2)) (c1)\n",
        "\n",
        "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\n",
        "c2 = Dropout(0.1) (c2)\n",
        "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\n",
        "p2 = MaxPooling2D((2, 2)) (c2)\n",
        "\n",
        "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\n",
        "c3 = Dropout(0.2) (c3)\n",
        "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\n",
        "p3 = MaxPooling2D((2, 2)) (c3)\n",
        "\n",
        "c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\n",
        "c4 = Dropout(0.2) (c4)\n",
        "c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\n",
        "p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
        "\n",
        "c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\n",
        "c5 = Dropout(0.3) (c5)\n",
        "c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n",
        "\n",
        "u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\n",
        "u6 = concatenate([u6, c4])\n",
        "c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\n",
        "c6 = Dropout(0.2) (c6)\n",
        "c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n",
        "\n",
        "u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
        "u7 = concatenate([u7, c3])\n",
        "c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\n",
        "c7 = Dropout(0.2) (c7)\n",
        "c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n",
        "\n",
        "u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n",
        "u8 = concatenate([u8, c2])\n",
        "c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\n",
        "c8 = Dropout(0.1) (c8)\n",
        "c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n",
        "\n",
        "u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n",
        "u9 = concatenate([u9, c1], axis=3)\n",
        "c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\n",
        "c9 = Dropout(0.1) (c9)\n",
        "c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n",
        "\n",
        "outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
        "\n",
        "model = Model(inputs=[inputs], outputs=[outputs])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[mean_iou])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-eac6d04dc953>:4: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-eac6d04dc953>:4: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/metrics_impl.py:1178: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/metrics_impl.py:1178: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 128, 128, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 128, 128, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 128, 128, 16) 448         lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 128, 128, 16) 0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 128, 128, 16) 2320        dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 16)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 64, 64, 32)   4640        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 64, 64, 32)   0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 64, 64, 32)   9248        dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 32)   0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 64)   18496       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32, 32, 64)   0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 64)   36928       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 64)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 128)  73856       max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 16, 16, 128)  0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 128)  147584      dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 128)    0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 8, 8, 256)    295168      max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 8, 8, 256)    0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 8, 8, 256)    590080      dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTrans (None, 16, 16, 128)  131200      conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 16, 16, 256)  0           conv2d_transpose_1[0][0]         \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 128)  295040      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 16, 16, 128)  0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 128)  147584      dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 32, 32, 64)   32832       conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 128)  0           conv2d_transpose_2[0][0]         \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 64)   73792       concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 32, 32, 64)   0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 64)   36928       dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 64, 64, 32)   8224        conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 64, 64, 64)   0           conv2d_transpose_3[0][0]         \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 64, 64, 32)   18464       concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 64, 64, 32)   0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 64, 64, 32)   9248        dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTrans (None, 128, 128, 16) 2064        conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 128, 128, 32) 0           conv2d_transpose_4[0][0]         \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 128, 128, 16) 4624        concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 128, 128, 16) 0           conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 128, 128, 16) 2320        dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 128, 128, 1)  17          conv2d_18[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,941,105\n",
            "Trainable params: 1,941,105\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUI6ZUcS5un8",
        "colab_type": "code",
        "outputId": "ba29042d-d061-40b7-e440-f83f572abda7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "source": [
        "# Fit model\n",
        "earlystopper = EarlyStopping(patience=5, verbose=1)\n",
        "checkpointer = ModelCheckpoint('model-dsbowl2018-1.h5', verbose=1, save_best_only=True)\n",
        "results = model.fit(X_train, Y_train, validation_split=0.1, batch_size=16, epochs=50, \n",
        "                    callbacks=[earlystopper, checkpointer])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10710 samples, validate on 1190 samples\n",
            "Epoch 1/50\n",
            "10710/10710 [==============================] - 102s 10ms/step - loss: 0.0054 - mean_iou: 0.5030 - val_loss: 1.4268e-07 - val_mean_iou: 0.5000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00000, saving model to model-dsbowl2018-1.h5\n",
            "Epoch 2/50\n",
            "10710/10710 [==============================] - 92s 9ms/step - loss: 6.0527e-07 - mean_iou: 0.5000 - val_loss: 1.1895e-07 - val_mean_iou: 0.5000\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to model-dsbowl2018-1.h5\n",
            "Epoch 3/50\n",
            "10710/10710 [==============================] - 93s 9ms/step - loss: 3.4636e-07 - mean_iou: 0.5000 - val_loss: 1.0753e-07 - val_mean_iou: 0.5000\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to model-dsbowl2018-1.h5\n",
            "Epoch 4/50\n",
            "  816/10710 [=>............................] - ETA: 1:23 - loss: 2.2950e-07 - mean_iou: 0.5000"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-de1200ac47b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpointer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model-dsbowl2018-1.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m results = model.fit(X_train, Y_train, validation_split=0.1, batch_size=16, epochs=50, \n\u001b[0;32m----> 4\u001b[0;31m                     callbacks=[earlystopper, checkpointer])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNwsYKm75znZ",
        "colab_type": "code",
        "outputId": "135f31c9-e8c9-4743-d637-17b82bd6d2cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# Predict on train, val and test\n",
        "model = load_model('model-dsbowl2018-1.h5', custom_objects={'mean_iou': mean_iou})\n",
        "preds_train = model.predict(X_train[:int(X_train.shape[0]*0.9)], verbose=1)\n",
        "preds_val = model.predict(X_train[int(X_train.shape[0]*0.9):], verbose=1)\n",
        "preds_test = model.predict(X_test, verbose=1)\n",
        "\n",
        "# Threshold predictions\n",
        "preds_train_t = (preds_train > 0.5).astype(np.uint8)\n",
        "preds_val_t = (preds_val > 0.5).astype(np.uint8)\n",
        "preds_test_t = (preds_test > 0.5).astype(np.uint8)\n",
        "\n",
        "# Create list of upsampled test masks\n",
        "preds_test_upsampled = []\n",
        "for i in range(len(preds_test)):\n",
        "    preds_test_upsampled.append(resize(np.squeeze(preds_test[i]),\n",
        "                                       (sizes_test[i][0], sizes_test[i][1]),\n",
        "                                       mode='constant', preserve_range=True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10710/10710 [==============================] - 20s 2ms/step\n",
            "1190/1190 [==============================] - 2s 2ms/step\n",
            "6100/6100 [==============================] - 10s 2ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-3044efddbdc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     preds_test_upsampled.append(resize(np.squeeze(preds_test[i]),\n\u001b[0;32m---> 15\u001b[0;31m                                        \u001b[0;34m(\u001b[0m\u001b[0msizes_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                                        mode='constant', preserve_range=True))\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7Atj3J-51Tx",
        "colab_type": "code",
        "outputId": "c04d774d-3981-4723-eaac-a33ce2c90435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        }
      },
      "source": [
        "# Perform a sanity check on some random training samples\n",
        "ix = random.randint(0, len(preds_train_t))\n",
        "imshow(X_train[ix])\n",
        "plt.show()\n",
        "imshow(np.squeeze(Y_train[ix]))\n",
        "plt.show()\n",
        "imshow(np.squeeze(preds_train_t[ix]))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR4AAAEYCAYAAACKkJnLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAOh0lEQVR4nO3df6ydBX3H8fdnrejEzbaaNLXFUWOj\ncWYqaRwE/zCiEZwRlhiHcbFuLM0SN5lbojD/WPxjycyMiolza/AHWwjIkI2GZDqsbO4fOosYBCpS\nZUqbQln44bIlDuZ3f5yneIR7uXDOvd9z7uH9Sk7uOc/59c1T7jvP85xzeVJVSFKnX5j1AJKefQyP\npHaGR1I7wyOpneGR1M7wSGq3ZuFJcm6Su5IcSXLJWr2PpPUna/E9niQbgO8BbwGOAt8E3l1Vd676\nm0lad9Zqi+f1wJGq+kFV/S9wNXD+Gr2XpHVm4xq97nbg3rHbR4FfX+7BSfz6tLSAqipLLV+r8Kwo\nyV5g76zeX9LsrFV4jgGnjd3eMSx7XFXtA/aBWzzSs81aHeP5JrAryc4kpwAXAvvX6L0krTNrssVT\nVY8l+QPgq8AG4PNVdcdavJek9WdNPk5/xkO4qyUtpOUOLvvNZUntDI+kdoZHUjvDI6md4ZHUzvBI\namd4JLUzPJLaGR5J7QyPpHaGR1I7wyOpneGR1M7wSGpneCS1MzyS2hkeSe0Mj6R2hkdSO8MjqZ3h\nkdTO8EhqZ3gktTM8ktoZHkntDI+kdoZHUjvDI6md4ZHUzvBIajdxeJKcluSmJHcmuSPJxcPyLUlu\nTHL38HPz6o0raRGkqiZ7YrIN2FZV30ryS8AtwAXA+4AHq+ovklwCbK6qD6/wWpMNIWmuVVWWWj7x\nFk9VHa+qbw3X/ws4DGwHzgeuGB52BaMYSdLjNq7GiyQ5HXgdcBDYWlXHh7vuA7Yu85y9wN7VeH9J\n68vEu1qPv0DyAuBfgT+vquuSPFxVm8buf6iqnvI4j7ta0mJa9V0tgCTPAb4MXFlV1w2L7x+O/5w8\nDnRimveQtHim+VQrwOeAw1X1ibG79gN7hut7gOsnH0/SIprmU603AP8GfAf46bD4Txkd57kGeCnw\nQ+BdVfXgCq/lrpa0gJbb1Zr6GM9qMDzSYlqTYzySNAnDI6md4ZHUzvBIamd4JLUzPJLaGR5J7QyP\npHaGR1I7wyOpneGR1M7wSGpneCS1MzyS2hkeSe0Mj6R2hkdSO8MjqZ3hkdTO8EhqZ3gktTM8ktoZ\nHkntDI+kdoZHUjvDI6md4ZHUzvBIamd4JLWbOjxJNiS5NckNw+2dSQ4mOZLkS0lOmX5MSYtkNbZ4\nLgYOj93+GPDJqno58BBw0Sq8h6QFMlV4kuwAfgO4fLgd4E3AtcNDrgAumOY9JC2eabd4PgV8CPjp\ncPtFwMNV9dhw+yiwfaknJtmb5FCSQ1POIGmdmTg8Sd4OnKiqWyZ5flXtq6rdVbV70hkkrU8bp3ju\n2cA7krwNeB7wy8BlwKYkG4etnh3AsenHlLRIJt7iqapLq2pHVZ0OXAh8vareA9wEvHN42B7g+qmn\nlLRQ1uJ7PB8G/jjJEUbHfD63Bu8haR1LVc16BpLMfghJq66qstRyv7ksqZ3hkdTO8EhqZ3gktTM8\nktoZHkntDI+kdoZHUjvDI6md4ZHUzvBIamd4JLUzPJLaGR5J7QyPpHaGR1I7wyOpneGR1M7wSGpn\neCS1MzyS2hkeSe0Mj6R2hkdSO8MjqZ3hkdTO8EhqZ3gktTM8ktoZHkntpgpPkk1Jrk3y3SSHk5yV\nZEuSG5PcPfzcvFrDSloM027xXAZ8papeCbwGOAxcAhyoql3AgeG2JD0uVTXZE5MXAt8GXlZjL5Lk\nLuCNVXU8yTbgX6rqFSu81mRDSJprVZWllk+zxbMTeAD4QpJbk1ye5FRga1UdHx5zH7B1qScn2Zvk\nUJJDU8wgaR2aZotnN3AzcHZVHUxyGfBj4A+ratPY4x6qqqc8zuMWj7SY1mKL5yhwtKoODrevBc4A\n7h92sRh+npjiPSQtoInDU1X3AfcmOXn85hzgTmA/sGdYtge4fqoJJS2ciXe1AJK8FrgcOAX4AfA7\njGJ2DfBS4IfAu6rqwRVex10taQEtt6s1VXhWi+GRFtNaHOORpIkYHkntDI+kdoZHUjvDI6md4ZHU\nzvBIamd4JLUzPJLaGR5J7QyPpHaGR1I7wyOpneGR1M7wSGpneCS1MzyS2hkeSe0Mj6R2hkdSO8Mj\nqZ3hkdTO8EhqZ3gktTM8ktoZHkntDI+kdoZHUjvDI6ndVOFJ8sEkdyS5PclVSZ6XZGeSg0mOJPlS\nklNWa1hJi2Hi8CTZDnwA2F1VrwY2ABcCHwM+WVUvBx4CLlqNQSUtjml3tTYCv5hkI/B84DjwJuDa\n4f4rgAumfA9JC2bi8FTVMeDjwI8YBecR4Bbg4ap6bHjYUWD7Us9PsjfJoSSHJp1B0vo0za7WZuB8\nYCfwEuBU4Nyn+/yq2ldVu6tq96QzSFqfptnVejNwT1U9UFWPAtcBZwObhl0vgB3AsSlnlLRgpgnP\nj4Azkzw/SYBzgDuBm4B3Do/ZA1w/3YiSFk2qavInJx8Ffgt4DLgV+D1Gx3SuBrYMy367qn6ywutM\nPoSkuVVVWWr5VOFZLYZHWkzLhcdvLktqZ3gktTM8ktoZHkntDI+kdoZHUjvDI6md4ZHUzvBIamd4\nJLUzPJLaGR5J7QyPpHaGR1I7wyOpneGR1M7wSGpneCS1MzyS2hkeSe0Mj6R2hkdSO8MjqZ3hkdTO\n8EhqZ3gktTM8ktoZHkntDI+kdoZHUrsVw5Pk80lOJLl9bNmWJDcmuXv4uXlYniSfTnIkyW1JzljL\n4SWtT09ni+eLwLlPWHYJcKCqdgEHhtsA5wG7hste4LOrM6akRbJieKrqG8CDT1h8PnDFcP0K4IKx\n5X9bIzcDm5JsW61hJS2GSY/xbK2q48P1+4Ctw/XtwL1jjzs6LHuSJHuTHEpyaMIZJK1TG6d9gaqq\nJDXB8/YB+wAmeb6k9WvSLZ77T+5CDT9PDMuPAaeNPW7HsEySHjdpePYDe4bre4Drx5a/d/h060zg\nkbFdMkkaqaqnvABXAceBRxkds7kIeBGjT7PuBr4GbBkeG+AzwPeB7wC7V3r94XnlxYuXxbss9zuf\n4Rd/pjzGIy2mqspSy/3msqR2hkdSO8MjqZ3hkdTO8EhqZ3gktTM8ktoZHkntDI+kdoZHUjvDI6md\n4ZHUzvBIamd4JLUzPJLaGR5J7QyPpHaGR1I7wyOpneGR1M7wSGpneCS1MzyS2hkeSe0Mj6R2hkdS\nO8MjqZ3hkdTO8Ehqt2J4knw+yYkkt48t+8sk301yW5J/SLJp7L5LkxxJcleSt67V4JLWr6ezxfNF\n4NwnLLsReHVV/RrwPeBSgCSvAi4EfnV4zl8l2bBq00paCCuGp6q+ATz4hGX/XFWPDTdvBnYM188H\nrq6qn1TVPcAR4PWrOK+kBbAax3h+F/in4fp24N6x+44Oy54kyd4kh5IcWoUZJK0jG6d5cpKPAI8B\nVz7T51bVPmDf8Do1zRyS1peJw5PkfcDbgXOq6mQ4jgGnjT1sx7BMkh430a5WknOBDwHvqKr/Gbtr\nP3Bhkucm2QnsAv59+jElLZIVt3iSXAW8EXhxkqPAnzH6FOu5wI1JAG6uqt+vqjuSXAPcyWgX7P1V\n9X9rNbyk9Sk/20ua4RAe45EWUlVlqeV+c1lSO8MjqZ3hkdTO8EhqZ3gktTM8ktoZHkntDI+kdoZH\nUrup/jp9Ff0n8N/Dz3n0YuZztnmdC+Z3tnmdC+Z3tknn+pXl7piLP5kASHKoqnbPeo6lzOts8zoX\nzO9s8zoXzO9sazGXu1qS2hkeSe3mKTz7Zj3AU5jX2eZ1Lpjf2eZ1Lpjf2VZ9rrk5xiPp2WOetngk\nPUvMRXiSnDucAPBIkktmOMdpSW5KcmeSO5JcPCzfkuTGJHcPPzfPaL4NSW5NcsNwe2eSg8N6+1KS\nU2Y016Yk1w4neTyc5Kx5WGdJPjj8O96e5Kokz5vVOlvmxJhLrqOMfHqY8bYkZ8xgtjU9aefMwzOc\n8O8zwHnAq4B3DycGnIXHgD+pqlcBZwLvH2a5BDhQVbuAA8PtWbgYODx2+2PAJ6vq5cBDwEUzmQou\nA75SVa8EXsNoxpmusyTbgQ8Au6vq1cAGRiebnNU6+yJPPjHmcuvoPEb/v/JdwF7gszOYbW1P2llV\nM70AZwFfHbt9KXDprOcaZrkeeAtwF7BtWLYNuGsGs+xg9B/nm4AbgDD6UtfGpdZj41wvBO5hOF44\ntnym64yfneNtC6Mvyt4AvHWW6ww4Hbh9pXUE/A3w7qUe1zXbE+77TeDK4frP/X4CXwXOeqbvN/Mt\nHp7BSQA7JTkdeB1wENhaVceHu+4Dts5gpE8xOrPHT4fbLwIerp+d0XVW620n8ADwhWE38PIkpzLj\ndVZVx4CPAz8CjgOPALcwH+vspOXW0bz9Tkx00s6nMg/hmTtJXgB8Gfijqvrx+H01ynzrR4FJ3g6c\nqKpbOt/3adoInAF8tqpex+hPX35ut2pG62wzo1Nq7wReApzKk3cn5sYs1tHTMc1JO5/KPIRnrk4C\nmOQ5jKJzZVVdNyy+P8m24f5twInmsc4G3pHkP4CrGe1uXQZsSnLy7+1mtd6OAker6uBw+1pGIZr1\nOnszcE9VPVBVjwLXMVqP87DOTlpuHc3F78TYSTvfM4QRVmm2eQjPN4Fdw6cNpzA6cLV/FoNkdJKw\nzwGHq+oTY3ftB/YM1/cwOvbTpqouraodVXU6o/Xz9ap6D3AT8M5ZzTXMdh9wb5JXDIvOYXRetZmu\nM0a7WGcmef7w73pyrpmvszHLraP9wHuHT7fOBB4Z2yVrseYn7ew6sLbCga23MTpy/n3gIzOc4w2M\nNndvA749XN7G6HjKAeBu4GvAlhnO+EbghuH6y4Z/9CPA3wPPndFMrwUODevtH4HN87DOgI8C3wVu\nB/6O0UkoZ7LOgKsYHWt6lNFW4kXLrSNGHxx8Zvh9+A6jT+a6ZzvC6FjOyd+Dvx57/EeG2e4Czpvk\nPf3msqR287CrJelZxvBIamd4JLUzPJLaGR5J7QyPpHaGR1I7wyOp3f8DlMNPQan/ceAAAAAASUVO\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR4AAAEYCAYAAACKkJnLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR4AAAEYCAYAAACKkJnLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvkKLjt152zQ",
        "colab_type": "code",
        "outputId": "0ebf31f5-2731-4fe1-8c2f-164d06ed1200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        }
      },
      "source": [
        "# Perform a sanity check on some random validation samples\n",
        "ix = random.randint(0, len(preds_val_t))\n",
        "imshow(X_train[int(X_train.shape[0]*0.9):][ix])\n",
        "plt.show()\n",
        "imshow(np.squeeze(Y_train[int(Y_train.shape[0]*0.9):][ix]))\n",
        "plt.show()\n",
        "imshow(np.squeeze(preds_val_t[ix]))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR4AAAEYCAYAAACKkJnLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR4AAAEYCAYAAACKkJnLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR4AAAEYCAYAAACKkJnLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW7OIWyj5412",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run-length encoding stolen from https://www.kaggle.com/rakhlin/fast-run-length-encoding-python\n",
        "def rle_encoding(x):\n",
        "    dots = np.where(x.T.flatten() == 1)[0]\n",
        "    run_lengths = []\n",
        "    prev = -2\n",
        "    for b in dots:\n",
        "        if (b>prev+1): run_lengths.extend((b + 1, 0))\n",
        "        run_lengths[-1] += 1\n",
        "        prev = b\n",
        "    return run_lengths\n",
        "\n",
        "def prob_to_rles(x, cutoff=0.5):\n",
        "    lab_img = label(x > cutoff)\n",
        "    for i in range(1, lab_img.max() + 1):\n",
        "        yield rle_encoding(lab_img == i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luhJjhrk584n",
        "colab_type": "code",
        "outputId": "c0021203-e738-429b-a200-8e95a21ae759",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "new_test_ids = []\n",
        "rles = []\n",
        "for n, id_ in enumerate(test_ids):\n",
        "    rle = list(prob_to_rles(preds_test_upsampled[n]))\n",
        "    rles.extend(rle)\n",
        "    new_test_ids.extend([id_] * len(rle))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-1ba4b212d187>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mrle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_to_rles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_test_upsampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mrles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnew_test_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OfPDfUJ5-AK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create submission DataFrame\n",
        "sub = pd.DataFrame()\n",
        "sub['ImageId'] = new_test_ids\n",
        "sub['EncodedPixels'] = pd.Series(rles).apply(lambda x: ' '.join(str(y) for y in x))\n",
        "sub.to_csv('sub-dsbowl2018-1.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}